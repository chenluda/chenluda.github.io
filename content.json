{"meta":{"title":"Glenn","subtitle":"","description":"","author":"Glenn","url":"https://chenluda.github.io","root":"/"},"pages":[{"title":"categories","date":"un55fin55","updated":"un55fin55","comments":true,"path":"categories/index.html","permalink":"https://chenluda.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"un55fin55","updated":"un55fin55","comments":true,"path":"tags/index.html","permalink":"https://chenluda.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"代码 | 获取 arXiv 论文作者单位","slug":"代码/代码-获取-arXiv-论文作者单位-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/代码/代码-获取-arXiv-论文作者单位-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E8%8E%B7%E5%8F%96-arXiv-%E8%AE%BA%E6%96%87%E4%BD%9C%E8%80%85%E5%8D%95%E4%BD%8D-Glenn/","excerpt":"","text":"前几天写了些代码，实现了每天定点向微信推送 arXiv 的最新文章，推送内容包括论文标题、论文网址、首次发布时间、摘要。现在需要提供作者单位，代码已集成在上一篇文章的代码中。 论文标题、论文网址、首次发布时间、摘要等内容都可以通过 arXiv 的接口直接获取。但它没有提供作者单位。 作者单位在论文中的位置随期刊或会议格式而变动，但通常都在第一页。 所以我们需要先将论文 pdf 的第一页解析为可识别的文本格式。 这里可以使用 pdfplumber 库。它可以按页处理 pdf ，获取页面文字，提取表格等操作。 获取到第一页的文本后，就可以识别机构名称了。 这里有三种思路： 一是设置正则化规则去匹配机构名称，这种方式运算量太大，程序很容易卡死，所以直接被淘汰； 二是使用命名实体识别的模型，我这里只使用了两个，一个是 spacy 的 en_core_web_sm，另一个是 huggingface spaces 的 dslim&#x2F;bert-base-NER。效果都不太好，大家可以尝试使用更具有针对性的 NER 模型。 三是使用 chatgpt，因为需要 api key，这种方式本来是不想用的，但无奈效果太好了。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121&#x27;&#x27;&#x27;Description: 获取 arXiv 论文作者单位Version: 1.0Author: GlennEmail: chenluda01@outlook.comDate: 2023-04-24 15:32:39FilePath: \\12-arxivdownload\\getAffiliation.pyCopyright (c) 2023 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import osimport refrom io import BytesIOimport openaiimport pdfplumberimport requestsimport spacyfrom transformers import pipeline# 设置 OpenAI 的 API keyos.environ[&#x27;OPENAI\\_API\\_KEY&#x27;] = &#x27;your\\_openai\\_api\\_key&#x27;openai.api\\_key = os.getenv(&#x27;OPENAI\\_API\\_KEY&#x27;)def get\\_affiliation\\_by\\_bert(text): &quot;&quot;&quot; 使用 bert 模型获取作者单位 &quot;&quot;&quot; # 初始化 BERT NER nlp = pipeline(&quot;ner&quot;, model=&quot;dslim/bert-base-NER&quot;, tokenizer=&quot;dslim/bert-base-NER&quot;) entities = nlp(text) institutions = set() # 提取组织实体 for i, entity in enumerate(entities): if entity[&#x27;entity&#x27;].startswith(&#x27;B-ORG&#x27;): org\\_name = entity[&#x27;word&#x27;] j = i + 1 while j &lt; len(entities) and entities[j][&#x27;entity&#x27;].startswith(&#x27;I-ORG&#x27;): org\\_name += &#x27;&#x27; + entities[j][&#x27;word&#x27;][2:] j += 1 institutions.add(org\\_name) author\\_affiliation = list(institutions) return author\\_affiliationdef get\\_affiliation\\_by\\_spacy(text): &quot;&quot;&quot; 使用 spacy 模型获取作者单位 &quot;&quot;&quot; # 加载预训练模型 nlp = spacy.load(&#x27;en\\_core\\_web\\_sm&#x27;) doc = nlp(text) institutions = set() # 提取组织实体 for ent in doc.ents: if ent.label\\_ == &quot;ORG&quot;: institutions.add(ent.text) author\\_affiliation = list(institutions) return author\\_affiliationdef get\\_affiliation\\_by\\_openai(text): &quot;&quot;&quot; 使用 OpenAI API 模型获取作者单位 &quot;&quot;&quot; prompt = &#x27;Extract the organization names from the given text: &#x27; + text # 调用 GPT-3.5 的接口 response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a very powerful named entity recognition model.&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125; ], ) output = response.choices[0].message.content.strip() institutions = [o.strip() for o in output.split(&quot;\\n&quot;)] author\\_affiliation = list(set(institutions)) return author\\_affiliationdef get\\_text\\_by\\_pdfplumber(pdf\\_url, methodType=&quot;openai&quot;): &quot;&quot;&quot; 使用 pdfplumber 提取 PDF 文本并获取作者单位 &quot;&quot;&quot; # 从 URL 获取 PDF 文件内容 response = requests.get(pdf\\_url) pdf\\_content = BytesIO(response.content) # 使用 pdfplumber 提取 PDF 文本 with pdfplumber.open(pdf\\_content) as pdf: first\\_page = pdf.pages[0] text = first\\_page.extract\\_text() # 根据方法类型选择实体识别方法 if methodType == &#x27;openai&#x27;: author\\_affiliation = get\\_affiliation\\_by\\_openai(text) if methodType == &#x27;spacy\\_model&#x27;: author\\_affiliation = get\\_affiliation\\_by\\_spacy(text) if methodType == &#x27;bert\\_model&#x27;: author\\_affiliation = get\\_affiliation\\_by\\_bert(text) return author\\_affiliationif \\_\\_name\\_\\_==&quot;\\_\\_main\\_\\_&quot;: pdf\\_url = &#x27;https://arxiv.org/pdf/2304.10864.pdf&#x27; methodType = &quot;bert\\_model&quot; author\\_affiliation = get\\_text\\_by\\_pdfplumber(pdf\\_url, methodType) print(&#x27;Author affiliation are: &#x27;, author\\_affiliation) 以这篇文章[1]为例： 题目：FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation单位：北京科技大学、中佛罗里达大学、伯明翰大学、理海大学论文网址：https://arxiv.org/abs/2304.10864论文代码：未公开首次发布时间：2023 年 4 月 21 日 金标准： Author affiliation are: [‘School of Automation and Electrical Engineering, University of Science and Technology Beijing‘, ‘Center for Research in Computer Vision, University of Central Florida’, ‘University of Birmingham‘, ‘Lehigh University‘] 使用 bert 模型获取作者单位： Author affiliation are: [‘Shan‘, ‘Originalage‘, ‘LichaS‘, ‘Jiang‘, ‘UniversityofCentralFida‘, ‘UniversityofScienceT‘] 使用 spacy 模型获取作者单位： Author affiliation are: [‘MIM‘, ‘Masked Image Modeling‘, ‘Original Image‘, ‘NLP‘, ‘MLM‘, ‘Fourier Spectrum‘, ‘Medical Image Segmentation‘, ‘MAE‘] 使用 OpenAI API 模型获取作者单位： Author affiliation are: [‘- University of Science and Technology Beijing‘, ‘- University of Birmingham‘, ‘- Center for Research in Computer Vision, University of Central Florida‘, ‘- FreMAE‘, ‘- Lehigh University‘] 参考 ^Wang, W. et al. FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation. arXiv preprint arXiv:2304.10864 (2023).","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"arXiv","slug":"arXiv","permalink":"https://chenluda.github.io/tags/arXiv/"},{"name":"Organization","slug":"Organization","permalink":"https://chenluda.github.io/tags/Organization/"}]},{"title":"代码 | 每天定点向微信推送 arXiv 最新文章","slug":"代码/代码-每天定点向微信推送-arXiv-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/代码/代码-每天定点向微信推送-arXiv-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E6%AF%8F%E5%A4%A9%E5%AE%9A%E7%82%B9%E5%90%91%E5%BE%AE%E4%BF%A1%E6%8E%A8%E9%80%81-arXiv-Glenn/","excerpt":"","text":"作为科研人，每天早起的第一件事， 就是打开 arXiv 查找自己 follow 的领域有没有最新的文章。 现在我想把这件事自动化： 类似部署自动打卡函数，在阿里云函数计算 FC 中设置一个带有定时触发器的云函数， 再将每天定点向微信推送 arXiv 最新文章的脚本程序部署上去， 实现： 根据关键词搜索在 arXiv 发布的文章； 判断文章的发布日期是否是前一天（或指定日期），如果是，则保留； 使用 Server 酱将前一天文章的标题、网址、摘要推送至微信。 效果展示： 这里我为了展示效果，将条件“前一天发布”改为了特定条件“2023 年 4 月 12 日发布”。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153&#x27;&#x27;&#x27;Description: 每天定点向微信推送 arXiv 最新文章Version: 1.0Author: GlennEmail: chenluda01@outlook.comDate: 2023-04-19 08:35:13FilePath: \\12-arxivdownload\\index.pyCopyright (c) 2023 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import datetimeimport osimport timefrom io import BytesIOimport openaiimport pdfplumberimport requestsdef get\\_author\\_affiliation(pdf\\_url): &quot;&quot;&quot; 从 PDF 文件中提取作者单位信息 &quot;&quot;&quot; response = requests.get(pdf\\_url) pdf\\_content = BytesIO(response.content) author\\_affiliation = [] with pdfplumber.open(pdf\\_content) as pdf: first\\_page = pdf.pages[0] text = first\\_page.extract\\_text() # 设置提示词 prompt = &#x27;Extract the organization names from the given text: &#x27; + text # 调用 GPT-3.5 的接口 response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a very powerful named entity recognition model.&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125; ], ) output = response.choices[0].message.content.strip() institutions = [o.strip() for o in output.split(&quot;\\n&quot;)] author\\_affiliation = list(set(institutions)) return author\\_affiliationdef get\\_yesterday(): &quot;&quot;&quot; 获取前一天的日期 &quot;&quot;&quot; today = datetime.datetime.now() yesterday = today - datetime.timedelta(days=1) return yesterday.strftime(&#x27;%Y-%m-%d&#x27;)def search\\_arxiv\\_papers(search\\_term, yester\\_date, max\\_results=10): &quot;&quot;&quot; 在 arxiv 按照关键词查找前一天的论文 &quot;&quot;&quot; papers = [] base\\_url = &#x27;http://export.arxiv.org/api/query?&#x27; search\\_query = f&#x27;search\\_query=all:&#123;search\\_term&#125;&amp;start=0&amp;max\\_results=&#123;max\\_results&#125;&amp;sortBy=submittedDate&amp;sortOrder=descending&#x27; response = requests.get(base\\_url + search\\_query) if response.status\\_code != 200: print(&quot;请求失败，请检查你的查询参数。&quot;) return feed = response.text entries = feed.split(&#x27;&lt;entry&gt;&#x27;)[1:] if not entries: print(&quot;没有找到与搜索词匹配的论文。&quot;) return for entry in entries: # 获取标题、摘要、链接、首次发布日期 title = entry.split(&#x27;&lt;title&gt;&#x27;)[1].split(&#x27;&lt;/title&gt;&#x27;)[0].strip() summary = entry.split(&#x27;&lt;summary&gt;&#x27;)[1].split(&#x27;&lt;/summary&gt;&#x27;)[0].strip() url = entry.split(&#x27;&lt;id&gt;&#x27;)[1].split(&#x27;&lt;/id&gt;&#x27;)[0].strip() pub\\_date = entry.split(&#x27;&lt;published&gt;&#x27;)[1].split(&#x27;&lt;/published&gt;&#x27;)[0] pub\\_date = datetime.datetime.strptime(pub\\_date, &quot;%Y-%m-%dT%H:%M:%SZ&quot;).strftime(&quot;%Y-%m-%d&quot;) # 获取 PDF 链接 pdf\\_url = entry.split(&#x27;&lt;link href=&quot;&#x27;)[1].split(&#x27;&quot; rel=&quot;alternate&#x27;)[0].strip() pdf\\_url = pdf\\_url.replace(&quot;abs&quot;, &quot;pdf&quot;) # 获取作者单位 author\\_affiliation = get\\_author\\_affiliation(pdf\\_url) if pub\\_date == yester\\_date: papers.append(&#123; &#x27;title&#x27;: title, &#x27;url&#x27;: url, &#x27;pub\\_date&#x27;: pub\\_date, &#x27;summary&#x27;: summary, &#x27;author\\_affiliation&#x27;: author\\_affiliation &#125;) return papersdef send\\_wechat\\_message(title, content, SERVERCHAN\\_API\\_KEY): &quot;&quot;&quot; 使用 Serve 酱向微信推送论文信息 &quot;&quot;&quot; url = f&#x27;https://sctapi.ftqq.com/&#123;SERVERCHAN\\_API\\_KEY&#125;.send&#x27; params = &#123; &#x27;title&#x27;: title, &#x27;desp&#x27;: content, &#125; requests.post(url, params=params)# def handler(event, context):if \\_\\_name\\_\\_ == &#x27;\\_\\_main\\_\\_&#x27;: # 设置 OPENAI 的 API\\_KEY os.environ[&#x27;OPENAI\\_API\\_KEY&#x27;] = &#x27;your\\_openai\\_api\\_key&#x27; openai.api\\_key = os.getenv(&#x27;OPENAI\\_API\\_KEY&#x27;) # 修改为自己 Serve 酱 API SERVERCHAN\\_API\\_KEY = &#x27;SCT206421TeQFPxkyqpZQFegFELJaKCW6d&#x27; # 关键词 search\\_term = &#x27;&quot;masked image model&quot;&#x27; # 获取的最大论文数 max\\_results = 10 # 获取前一天的日期 yester\\_date = get\\_yesterday() # 在 arxiv 按照关键词查找前一天的论文 papers = search\\_arxiv\\_papers(search\\_term, yester\\_date, max\\_results) for paper in papers: title = paper[&#x27;title&#x27;] url = paper[&#x27;url&#x27;] pub\\_date = paper[&#x27;pub\\_date&#x27;] summary = paper[&#x27;summary&#x27;] msg\\_title = f&#x27;标题：&#123;title&#125;&#x27; msg\\_url = f&#x27;论文网址：&#123;url&#125;&#x27; msg\\_pub\\_date = f&#x27;首次发布时间：&#123;pub\\_date&#125;&#x27; msg\\_summary = f&#x27;摘要：&#123;summary&#125;&#x27; msg\\_content = f&#x27;&#123;msg\\_title&#125;\\n\\n&#123;msg\\_url&#125;\\n\\n&#123;msg\\_pub\\_date&#125;\\n\\n&#123;msg\\_summary&#125;&#x27; send\\_wechat\\_message(title, msg\\_content, SERVERCHAN\\_API\\_KEY) # 为避免触发微信推送服务的限制，等待一段时间后再发送下一篇论文的信息 time.sleep(5) 使用 gpt3.5 识别论文的作者单位，如果不需要的话，可以注释掉相关代码。 其中，handler 函数中的 SERVERCHAN_API_KEY、search_term 、max_results 是根据需求修改的变量。 SERVERCHAN_API_KEY：自己 Serve 酱的 API（下方会解释）。 search_term：搜索论文的关键词，如果使用双引号将词包裹起来，表明论文中必须出现这个词，例如，’Masked Image Model’ 和 ‘“Masked Image Model”‘ 搜索结果不同，具体请查看 arxiv 的文献检索说明。 max_results：检索论文的最大数量。 如果想直接本地运行，则可以将 def handler(event, context): 改为 if __name__ &#x3D;&#x3D; ‘__main__‘: 。 1. Server 酱我们需要使用到Server 酱，这是一款从服务器、路由器等设备上推消息到手机的工具。我们需要使用她实现向微信推送消息的功能。 打开官网网址：https://sct.ftqq.com/； 微信扫码登陆后，进入 Key&amp;API 模块； 将 SendKey 复制替换代码中的 Your-Server-API。 2. 阿里云函数计算 FC函数计算（Function Compute）是一个事件驱动的全托管 Serverless 计算服务，无需管理服务器等基础设施，只需编写代码并上传，函数计算会准备好计算资源，并以弹性、可靠的方式运行代码。 打开官网网址：国内唯一入选Forrester领导者象限，登陆后，进入管理控制台； 进入服务及函数模块，点击创建服务按钮； 登陆后，进入管理控制台，再进入服务及函数模块，点击创建服务按钮 将弹窗中的名称和描述填完后，点击左下角确定按钮； 将弹窗中的名称和描述填完后，点击左下角确定按钮 进入函数管理模块，点击创建函数按钮； 进入函数管理模块，点击创建函数按钮 填写函数名称； 将上面给出的脚本代码保存为 index.py 放入一个名为 arxiv_push_code 的空文件夹中； 压缩该文件夹，将压缩包拖至代码包处； 点击页面下方创建按钮； 将 arxiv_push_code 文件夹中的 index.py 拖至上级文件夹 CODE 处； 将 arxiv_push_code 文件夹删除； 进入触发器管理模块，点击创建触发器按钮； 进入触发器管理模块，点击创建触发器按钮 在弹窗中填写相关内容，其中“指定时间”就是在设置每天几点向微信推送文章； 填写完后点击左下方确定按钮，完成触发器创建； 回到函数代码页面，点击部署代码按钮，出现部署成功提示后，表示部署成功； 点击测试函数，可以直接运行。","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"arXiv","slug":"arXiv","permalink":"https://chenluda.github.io/tags/arXiv/"},{"name":"WeChat","slug":"WeChat","permalink":"https://chenluda.github.io/tags/WeChat/"}]},{"title":"代码 | 将知乎专栏文章转换为 Markdown 文件保存到本地","slug":"代码/代码-将知乎专栏文章转换为-Markd-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/代码/代码-将知乎专栏文章转换为-Markd-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E5%B0%86%E7%9F%A5%E4%B9%8E%E4%B8%93%E6%A0%8F%E6%96%87%E7%AB%A0%E8%BD%AC%E6%8D%A2%E4%B8%BA-Markd-Glenn/","excerpt":"","text":"最近想构建一个本地知识库。 需要从知乎下载文章、专栏、回答，并以 Markdown 格式保存到本地。 自己写了个脚本， 可以判断给定 url 的类型，是文章、专栏还是回答，三种类型的处理方式不同； 然后将图片保存至本地，并将转换的 Markdown 中图片 url 更换为本地路径，使图片可以在本地显示。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205&#x27;&#x27;&#x27;Description: 将知乎专栏文章转换为 Markdown 文件保存到本地Version: 1.0Author: 陈路达Email: chenluda01@outlook.comDate: 2023-04-18 10:48:38FilePath: \\11-zhihuToMd\\main.pyCopyright (c) 2022 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import osimport reimport urllib.parseimport requestsfrom bs4 import BeautifulSoupfrom markdownify import markdownify as mddef download\\_image(url, save\\_path): &quot;&quot;&quot; 从指定url下载图片并保存到本地 &quot;&quot;&quot; if url.startswith(&quot;data:image/&quot;): # 如果链接以 &quot;data:&quot; 开头，则直接写入数据到文件 with open(save\\_path, &quot;wb&quot;) as f: f.write(url.split(&quot;,&quot;, 1)[1].encode(&quot;utf-8&quot;)) else: response = requests.get(url) with open(save\\_path, &quot;wb&quot;) as f: f.write(response.content)def get\\_valid\\_filename(s): &quot;&quot;&quot; 将字符串转换为有效的文件名 &quot;&quot;&quot; s = str(s).strip().replace(&#x27; &#x27;, &#x27;\\_&#x27;) return re.sub(r&#x27;(?u)[^-\\w\\_]&#x27;, &#x27;&#x27;, s)def judge\\_zhihu\\_type(url): &quot;&quot;&quot; 判断url类型 &quot;&quot;&quot; response = requests.get(url) soup = BeautifulSoup(response.content, &quot;html.parser&quot;) if soup.text.find(&quot;知乎专栏&quot;) != -1: # 如果是专栏，将所有文章放在一个以专栏标题命名的文件夹中 title = soup.text.split(&#x27;-&#x27;)[0].strip() folder\\_name = get\\_valid\\_filename(title) os.makedirs(folder\\_name, exist\\_ok=True) os.chdir(folder\\_name) parse\\_zhihu\\_column(url) elif url.find(&quot;answer&quot;) != -1: # 如果是回答 parse\\_zhihu\\_answer(url) else: # 如果是单篇文章 parse\\_zhihu\\_article(url)def save\\_and\\_transform(title\\_element, content\\_element, author, url): &quot;&quot;&quot; 转化并保存为 Markdown 格式文件 &quot;&quot;&quot; # 获取标题和内容 if title\\_element is not None: title = title\\_element.text.strip() else: title = &quot;Untitled&quot; # 防止文件名称太长，加载不出图像 markdown\\_title = get\\_valid\\_filename(title)[0:20] markdown\\_title = f&quot;&#123;markdown\\_title&#125;\\_&#123;author&#125;&quot; if content\\_element is not None: # 将 css 样式移除 for style\\_tag in content\\_element.find\\_all(&quot;style&quot;): style\\_tag.decompose() for img\\_lazy in content\\_element.find\\_all(&quot;img&quot;, class\\_=lambda x: &#x27;lazy&#x27; in x if x else True): img\\_lazy.decompose() # 处理回答中的图片 for img in content\\_element.find\\_all(&quot;img&quot;): # 将图片链接替换为本地路径 img\\_url = img.get(&quot;data-original&quot;, img.get(&quot;src&quot;, None)) if img\\_url is None: continue img\\_name = urllib.parse.quote(os.path.basename(img\\_url)) img\\_path = f&quot;&#123;markdown\\_title&#125;\\_files/&#123;img\\_name&#125;&quot; # 如果图片链接中 .jpg 后面还有字符串则直接截停 if img\\_path.find(&#x27;.jpg&#x27;) + 3 != len(img\\_path) - 1: img\\_path = img\\_path[0: img\\_path.find(&#x27;.jpg&#x27;) + 4] img[&quot;src&quot;] = img\\_path # 下载图片并保存到本地 os.makedirs(os.path.dirname(img\\_path), exist\\_ok=True) download\\_image(img\\_url, img\\_path) # 在图片后插入换行符 img.insert\\_after(&#x27;\\n\\n&#x27;) # 在 &lt;/figcaption&gt; 后面加上换行符 for figcaption in content\\_element.find\\_all(&quot;figcaption&quot;): figcaption.insert\\_after(&#x27;\\n\\n&#x27;) # 获取回答文本内容 content = content\\_element.decode\\_contents().strip() # 转换为 markdown content = md(content) else: content = &quot;&quot; # 转化为 Markdown 格式 if content: markdown = f&quot;# &#123;title&#125;\\n\\n \\*\\*Author:\\*\\* [&#123;author&#125;]\\n\\n \\*\\*Link:\\*\\* [&#123;url&#125;]\\n\\n&#123;content&#125;&quot; else: markdown = f&quot;# &#123;title&#125;\\n\\nContent is empty.&quot; # 保存 Markdown 文件 with open(f&quot;&#123;markdown\\_title&#125;.md&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(markdown)def parse\\_zhihu\\_article(url): &quot;&quot;&quot; 解析知乎文章并保存为Markdown格式文件 &quot;&quot;&quot; # 发送GET请求获取网页内容 response = requests.get(url) # 解析HTML soup = BeautifulSoup(response.content, &quot;html.parser&quot;) # 找到文章标题和内容所在的元素 title\\_element = soup.select\\_one(&quot;h1.Post-Title&quot;) content\\_element = soup.select\\_one(&quot;div.Post-RichText&quot;) author = soup.select\\_one(&#x27;div.AuthorInfo &gt; div.AuthorInfo-content &gt; div.AuthorInfo-head&#x27;).text.strip() # 解析知乎文章并保存为Markdown格式文件 save\\_and\\_transform(title\\_element, content\\_element, author, url)def parse\\_zhihu\\_answer(url): &quot;&quot;&quot; 解析知乎回答并保存为 Markdown 格式文件 &quot;&quot;&quot; # 发送 GET 请求获取网页内容 response = requests.get(url) # 解析 HTML soup = BeautifulSoup(response.content, &quot;html.parser&quot;) # 找到回答标题、内容、作者所在的元素 title\\_element = soup.select\\_one(&quot;h1.QuestionHeader-title&quot;) content\\_element = soup.select\\_one(&quot;div.RichContent-inner&quot;) author = soup.select\\_one(&#x27;div.AuthorInfo &gt; div.AuthorInfo-content &gt; div.AuthorInfo-head&#x27;).text.strip() # 解析知乎文章并保存为Markdown格式文件 save\\_and\\_transform(title\\_element, content\\_element, author, url)def parse\\_zhihu\\_column(url): &quot;&quot;&quot; 解析知乎专栏并获取所有文章链接 &quot;&quot;&quot; # 获取所有文章链接 items = [] url\\_template = &quot;https://zhuanlan.zhihu.com/p/&#123;id&#125;&quot; offset = 0 while True: api\\_url = f&quot;/api/v4/columns/&#123;url.split(&#x27;/&#x27;)[-1]&#125;/items?limit=10&amp;offset=&#123;offset&#125;&quot; response = requests.get(f&quot;https://www.zhihu.com&#123;api\\_url&#125;&quot;) data = response.json() items += data[&quot;data&quot;] if data[&quot;paging&quot;][&quot;is\\_end&quot;]: break offset += 10 article\\_links = [url\\_template.format(id=item[&quot;id&quot;]) for item in items] # 遍历所有文章链接，转换为Markdown并保存到本地 for article\\_link in article\\_links: parse\\_zhihu\\_article(article\\_link)if \\_\\_name\\_\\_==&quot;\\_\\_main\\_\\_&quot;: # 回答 # url = &quot;https://www.zhihu.com/question/593914819/answer/2971671307&quot; # 文章 # url = &quot;https://zhuanlan.zhihu.com/p/626703154&quot; # 专栏 url = &quot;https://www.zhihu.com/column/c\\_1620937636624703488&quot; judge\\_zhihu\\_type(url) 1. 先扔个回答的 urlMeta 发布图像分割论文 Segment Anything，将给 CV 研究带来什么影响？ - Glenn的回答 - 知乎 https://www.zhihu.com/question/593914819/answer/2971671307 将会生成一个以“问题标题 + ‘_‘ + 作者名称 + ‘_files’”命名的文件夹和以“问题标题 + ‘_‘ + 作者名称”命名的 .md 文件。 文件夹中保存了本回答的所有图片。 为防止问题标题太长，导致图片加载不出来，在第 20 个字符处做了截断。 2. 单个文章的 urlGlenn：MIM | HPM：引入蒸馏概念自动生成掩蔽策略 将会生成一个以“文章标题 + ‘_‘ + 作者名称 + ‘_files’”命名的文件夹和以“文章标题 + ‘_‘ + 作者名称”命名的 .md 文件。 3. 专栏文章 urlMIM 将会生成一个以“专栏标题”命名的文件夹，里面包含了所有专栏文章的图像文件夹和 .md 文件。","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"DataBase","slug":"DataBase","permalink":"https://chenluda.github.io/tags/DataBase/"},{"name":"Zhihu","slug":"Zhihu","permalink":"https://chenluda.github.io/tags/Zhihu/"},{"name":"Markdown","slug":"Markdown","permalink":"https://chenluda.github.io/tags/Markdown/"}]},{"title":"代码 | 从 OpenReview 获取顶会接收论文集并保存至本地数据库","slug":"代码/代码-从-OpenReview-获取顶-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/代码/代码-从-OpenReview-获取顶-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E4%BB%8E-OpenReview-%E8%8E%B7%E5%8F%96%E9%A1%B6-Glenn/","excerpt":"","text":"最近在调整自己的论文追踪网站，发现从 arXiv 的 Comments 扒论文的中稿会议或期刊效率太低，且准确率也不高，想着还不如直接爬取会议接受的论文。 AI 论文追踪 by GlennOpenReview 是一个旨在促进同行评审过程中透明度的平台。通常，OpenReview 公布接收结果的时间比会议官方网站要早。实际上，大部分论文推送网站也是从 OpenReview 上获取会议论文信息的。 OpenReviewGithub 上也有从该网站爬取会议论文集的项目，如 paper_downloader。但是，这个项目是通过直接请求页面元素并根据布局来查找相关论文信息的。由于不同会议在 OpenReview 上的页面布局各不相同，特别是在不同年份之间，这就导致了一个问题：代码变得极其冗余（因为每个会议在不同的年份都需要有自己独特的爬取代码）。 paper_downloader要知道 OpenReview 是有自己的官方 API 文档的 :) 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&#x27;&#x27;&#x27;Description: 从 openreview 获取被接收的顶会论文保存至数据库Version: 1.0Author: GlennEmail: chenluda01@outlook.comDate: 2023-05-04 13:45:40FilePath: \\12-arxivdownload\\main.pyCopyright (c) 2023 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import openreviewimport mysql.connectorimport timedef get\\_papers\\_from\\_openreview(conference\\_id): &quot;&quot;&quot; 从 openreview 获取被接收的顶会论文 &quot;&quot;&quot; def get\\_accepted\\_forum\\_ids(blind\\_notes): &quot;&quot;&quot; 从提交的论文中获取被接收的论文 &quot;&quot;&quot; forum\\_ids = set() for note in blind\\_notes: for reply in note.details[&quot;directReplies&quot;]: if reply[&quot;invitation&quot;].endswith(&quot;Decision&quot;) and &#x27;Accept&#x27; in reply[&quot;content&quot;][&#x27;decision&#x27;]: forum\\_ids.add(reply[&#x27;forum&#x27;]) return forum\\_ids def format\\_note(note, conference\\_name): &quot;&quot;&quot; 获取需要存储的论文信息 &quot;&quot;&quot; authors\\_string = &#x27;,&#x27;.join(note.content[&#x27;authors&#x27;]) tags\\_string = &#x27;,&#x27;.join(note.content[&#x27;keywords&#x27;]) localTime = time.localtime(note.pdate/1000) strTime = time.strftime(&#x27;%Y-%m-%d&#x27;, localTime) return &#123; &#x27;title&#x27;: note.content[&#x27;title&#x27;], &#x27;url&#x27;: &#x27;https://openreview.net/forum?id=&#x27; + note.forum, &#x27;pub\\_date&#x27;: strTime, &#x27;summary&#x27;: note.content[&#x27;abstract&#x27;], &#x27;authors&#x27;: authors\\_string, &#x27;tags&#x27;: tags\\_string, &#x27;read\\_num&#x27;: 0, &#x27;conference&#x27;: conference\\_name, &#x27;venue&#x27;: note.content[&#x27;venue&#x27;] &#125; # 获取该会议的所有提交论文 submissions = client.get\\_all\\_notes( invitation=conference\\_id + &#x27;/Conference/-/Blind\\_Submission&#x27;, details=&#x27;directReplies&#x27;) # 从提交的论文中获取被接收论文的 id accepted\\_forum\\_ids = get\\_accepted\\_forum\\_ids(submissions) # 通过 id 获取需要存储的论文信息 notes\\_list = [format\\_note(note, conference\\_name) for note in submissions if note.forum in accepted\\_forum\\_ids] return notes\\_listdef insert\\_papers\\_to\\_db(papers): &quot;&quot;&quot; 将获取的论文信息保存至数据库 &quot;&quot;&quot; connection = None try: connection = mysql.connector.connect( host=&#x27;your-host-address&#x27;, user=&#x27;your-username&#x27;, password=&#x27;your-password&#x27;, database=&#x27;your-database&#x27; ) cursor = connection.cursor() for paper in papers: query = &#x27;&#x27;&#x27; INSERT INTO card\\_data (title, abstracts, tags, authors, date, url, read\\_num, conference, venue) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) &#x27;&#x27;&#x27; cursor.execute(query, ( paper[&#x27;title&#x27;], paper[&#x27;summary&#x27;], paper[&#x27;tags&#x27;], paper[&#x27;authors&#x27;], paper[&#x27;pub\\_date&#x27;], paper[&#x27;url&#x27;], paper[&#x27;read\\_num&#x27;], paper[&#x27;conference&#x27;], paper[&#x27;venue&#x27;] )) connection.commit() except mysql.connector.Error as error: print(f&quot;Failed to insert paper: &#123;error&#125;&quot;) finally: if connection is not None and connection.is\\_connected(): cursor.close() connection.close()if \\_\\_name\\_\\_ == &#x27;\\_\\_main\\_\\_&#x27;: client = openreview.Client(baseurl=&#x27;https://api.openreview.net&#x27;) # 会议名称，建议先在官网上查看下收录会议该年的论文是否已经放出 conference\\_name = &#x27;ICLR&#x27; # 会议年份 conference\\_year = &#x27;2023&#x27; # conference\\_name = &#x27;NeurIPS&#x27; # conference\\_year = &#x27;2022&#x27; paper\\_list = get\\_papers\\_from\\_openreview( conference\\_name + &#x27;.cc/&#x27; + conference\\_year) pass if paper\\_list: insert\\_papers\\_to\\_db(paper\\_list) else: print(&quot;No papers found to insert into the database.&quot;) 注：在填写会议名称和会议年份时，建议先去网站上查看下该年该会议的论文是否已经公开。 本地数据库","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Conference paper","slug":"Conference-paper","permalink":"https://chenluda.github.io/tags/Conference-paper/"},{"name":"OpenReview","slug":"OpenReview","permalink":"https://chenluda.github.io/tags/OpenReview/"},{"name":"DataBase","slug":"DataBase","permalink":"https://chenluda.github.io/tags/DataBase/"}]},{"title":"MIM 的小总结","slug":"Masked_Image_Model/MIM-的小总结-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-的小总结-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-%E7%9A%84%E5%B0%8F%E6%80%BB%E7%BB%93-Glenn/","excerpt":"","text":"MIM 专栏： 距离放出的最新 Masked Image Model（MIM）文章（非特定领域）已经过去 8 天了。 趁这段时间，赶紧对 MIM 做下小总结。 MIM 作为上游任务，其目标应该是为下游任务（例如分类、分割、目标检测）提供一个有利的初始化。 直观上来看，当模型面临具有挑战性的重建任务时，它需要学会识别图像中更复杂、更抽象的特征，以便在缺失数据的情况下进行有效的预测。这意味着编码器需要捕捉更高层次的图像特征和结构信息。通过这种方式，编码器可以学到更丰富的特征表示，从而为下游任务提供更有利的初始化。 通常增加重建任务难度的方式有很多，例如： 改进掩蔽策略。可以通过改进掩蔽策略来增加重建任务的难度。例如，HPM[1] 主张掩蔽富含语义的前景对象，从而迫使模型学习更复杂的特征表示。 HPM 改变重建目标。 1）传统的像素级别重建任务可能会限制模型的表达能力。为了提高模型的性能，可以将重建目标替换为更高级的语义表示，如 MaskFeat[2] 使用 HOG 特征，BEiT-v2[3] 使用 CLIP 特征等。这将迫使模型学习更丰富的特征表示，从而有助于下游任务的性能； 2）深度监督是一种在建模过程中引入不同监督信号的方法。通过让模型关注不同尺度和抽象级别的重建任务，可以促使模型学习更丰富的特征表示。例如，LocalMIM[4] 对不同尺度的图像特征进行监督，DeepMIM[5] 在模型的不同层次引入监督信号，从而强制模型在多个抽象层次上学习有效的特征表示。 LocalMIM 引入额外的代理任务。CMAE[6] 除了重建任务以外，引入了一个对比学习的任务，迫使模型学习更具区分性和局部敏感性的特征表示。MixedAE[7] 引入了同源识别（homologous recognition） 的辅助代理任务，不仅通过明确要求每个 patch 识别同源 patches 来缓解互信息的增加，而且还可以执行 object-aware 的自监督预训练，以获得更好的下游密集感知性能。 MixedAE但当重建任务变得过度困难时，也会导致一系列问题： 如果过度掩蔽前景对象，模型可能会遇到错位（misalignment）问题，即模型在重建图像时，可能无法正确对齐原始图像中的局部结构和细节。这可能会导致生成的图像与原始图像的语义有所偏差，从而影响模型在下游任务中的性能。 MIM 的错位（misalignment）问题 针对复杂的重建目标，可能导致模型过于关注重建任务中的细节，而忽略了下游任务中更为重要的全局信息，从而影响模型性能。 随着重建任务变得越来越困难，模型可能需要更多的计算资源和时间才能收敛。这可能会导致训练成本上升，对硬件资源要求更高，并且需要更多的数据来避免过拟合。 为了解决这些问题，我们可以将思路转变如下： 如何量化重建任务的难度？HPM 使用重建损失作为重建任务难度的指标，而 MixedAE 使用模型输入和重建目标之间的互信息作为量化标准。 如何平衡重建任务的难度和下游任务的需求？可以通过在训练过程中逐渐增加重建任务的难度，或者在训练过程中引入多个不同难度的重建任务，以逐步提高模型的表征能力。 如何选择合适的重建目标？这涉及到对具体下游任务的理解和需求。例如，对于目标检测任务，重建任务应该更注重保留物体的形状和位置信息，而对于图像分类任务，重建任务则应更注重提取图像的语义特征。 那么对应的解决方法： 权衡掩蔽策略。我们把掩蔽策略的选择看作 tradeoff 问题，即前景掩蔽率越高，通过 MIM 获得的信息越多，但同时解码器的重建难度也越高。对此 AutoMAE[8] 将掩模生成和图像重建任务集成到一个完全可微的框架中，用于寻找具有较高信息密度的区域内找到重建难度较低的 patch 来掩蔽。DPPMask[9] 基于行列式点过程 (determinantal point process, DPP)，逐步选择更具有代表性 patches 来保留。特别的，这些方法以及 SemeMAE[10]、HPM 都可以轻易地构建 easy-to-hard 任务。 MAE、SemMAE、AutoMAE 的对比 改变重建任务。使用更简单的任务。例如，MixMIM[11] 和 MixedAE 使用另一幅图像的可见 patch 替换 MIM 的 patch masking&#x2F;zero（可以理解为用 0 来填充 patch），这种创建一个混合图像，然后再进行双重重建的任务，比单纯使用 patch masking&#x2F;zero 再重建的任务要简单。 MixMIM 解耦重建任务。MIM 中由于解码器中也同时输入了编码器输出的编码特征，那么在完成重建任务的时候，就会对这部分也进行优化，这就限制了编码器的表征学习能力。CAE[12] 提出 Latent Contextual Regressor 模块，用于解耦编码和解码的角色，前者专心做内容理解，后者专心去预测目标，从而充分激发编码器表征学习的潜能，进而提升表征质量[13]。 CAE参考 ^Wang, H. et al. Hard Patches Mining for Masked Image Modeling. arXiv preprint arXiv:2304.05919 (2023). ^Wei, C. et al. Masked feature prediction for self-supervised visual pre-training. in Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition 14668–14678 (2022). ^Peng, Z., Dong, L., Bao, H., Ye, Q. &amp; Wei, F. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366 (2022). ^Wang, H. et al. Masked Image Modeling with Local Multi-Scale Reconstruction. arXiv preprint arXiv:2303.05251 (2023). ^Ren, S., Wei, F., Albanie, S., Zhang, Z. &amp; Hu, H. DeepMIM: Deep Supervision for Masked Image Modeling. arXiv preprint arXiv:2303.08817 (2023). ^Huang, Z. et al. Contrastive masked autoencoders are stronger vision learners. arXiv preprint arXiv:2207.13532 (2022). ^Chen, K. et al. Mixed Autoencoder for Self-supervised Visual Representation Learning. arXiv preprint arXiv:2303.17152 (2023). ^Chen, H., Zhang, W., Wang, Y. &amp; Yang, X. Improving Masked Autoencoders by Learning Where to Mask. arXiv preprint arXiv:2303.06583 (2023). ^Xu, J. et al. DPPMask: Masked Image Modeling with Determinantal Point Processes. arXiv preprint arXiv:2303.12736 (2023). ^Li, G. et al. Semmae: Semantic-guided masking for learning masked autoencoders. arXiv preprint arXiv:2206.10207 (2022). ^Liu, J., Huang, X., Liu, Y. &amp; Li, H. Mixmim: Mixed and masked image modeling for efficient visual representation learning. arXiv preprint arXiv:2205.13137 (2022). ^Chen, X. et al. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026 (2022). ^来看看新一代 MIM 青年 CAE(Context AutoEncoder) 如何克服 MAE 中表征学习不充分的问题 - CW不要無聊的風格的文章 - 知乎 https://zhuanlan.zhihu.com/p/519425855","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | 理论：掩蔽图像建模本质上在学习遮挡不变特征","slug":"Masked_Image_Model/MIM-理论掩蔽图像建模本质上在学习遮-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-理论掩蔽图像建模本质上在学习遮-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-%E7%90%86%E8%AE%BA%E6%8E%A9%E8%94%BD%E5%9B%BE%E5%83%8F%E5%BB%BA%E6%A8%A1%E6%9C%AC%E8%B4%A8%E4%B8%8A%E5%9C%A8%E5%AD%A6%E4%B9%A0%E9%81%AE-Glenn/","excerpt":"","text":"题目：Understanding Masked Image Modeling via Learning Occlusion Invariant Feature单位：旷视论文网址：https://arxiv.org/abs/2208.04164论文代码：未公开首次发布时间：2022 年 8 月 8 日 123456@article&#123;kong2022understanding, title=&#123;Understanding masked image modeling via learning occlusion invariant feature&#125;, author=&#123;Kong, Xiangwen and Zhang, Xiangyu&#125;, journal=&#123;arXiv preprint arXiv:2208.04164&#125;, year=&#123;2022&#125;&#125; 前言PixMIM、AutoMAE 都提到了改变掩蔽策略可以提升 MIM 模型性能，而 DeepMIM、LocalMIM 的改进策略都放在编码器和损失函数上，那么究竟哪一种方式才是 MIM 成功的关键？ PixMIM 介绍看这里： AutoMAE 介绍看这里： DeepMIM 介绍看这里： 主要内容 提出了一种新的 RelaxMIM 框架来近似原始的重建 MIM 方法。根据 RelaxMIM 的观点，MIM 可以解释为对比学习的一种特殊情况：数据增强方式是 patch masking，相似性度量与解码器有关。换句话说，MIM 模型本质上学习遮挡不变特征。 基于 RelaxMIM，作者用更简单的信息损失代替相似度度量。令人惊讶的是，性能与原始模型保持相同。这表明，MIM 框架中的重构解码器并不重要，其他测量也可以很好地工作。patch masking 可能是成功的关键。 为了理解为什么 patch masking 很重要，作者使用很少的图像（例如只有 1 张图像）进行 MIM 预训练，然后在 ImageNet 上对编码器进行微调。虽然经过预训练后，学习到的表征缺乏语义信息，但精细化模型的性能仍然显著优于从头开始的训练模型。 符号公式描述对比学习的表述如下： \\underset{\\theta}{\\min}\\underset{x～\\mathcal D}{\\mathbb{E}}\\mathcal M(z_1,z_2), z_1&#x3D;f_\\theta(\\mathcal T_1(x)),z_2&#x3D;f_\\theta(\\mathcal T_2(x))（1） 其中 \\mathcal D 为数据分布； f_θ(\\cdot) 表示由 θ 参数化的编码器网络； \\mathcal T_1(\\cdot) 和 \\mathcal T_2(\\cdot) 是输入数据上的两个数据增强方式，它定义了需要学习的不变性； \\mathcal M(\\cdot, \\cdot) 是距离函数（或相似性度量），用来度量两个特征图 z_1 和 z_2 之间的相似性。 MIM 的表述如下： \\underset{\\theta}{\\min}\\underset{x～\\mathcal D}{\\mathbb{E}}\\mathcal M(d_\\phi(z),x\\odot(1-M)), z&#x3D;f_\\theta(x\\odot M)（2） 其中 \\odot 表示 element-wise product；M 是 patch mask（如图 1）； f_θ(\\cdot) 和 d_\\phi(\\cdot) 分别是编码器和解码器； z 是可学习表征； \\mathcal M(\\cdot, \\cdot) 是相似性度量方法。 图1. patch mask 解释 假设一：MIM 本质上学习遮挡不变特征【背景】学习不变性特征是对比学习这类自监督方法成功的关键：通过最小化各类增强图像的特征差异，可以鼓励模型学习原始图像的增强不变性特征。 那么我们是否可以将 MIM 看作对比学习的一种特殊情况呢？ 【方法】为了验证该假设，首先需要将 MIM 公式 2 改写为近似对比学习公式 1 的形式。 以 MAE 举例，相似性度量方法设置为 l2−distance。 则 MAE 的损失函数公式可以写成： \\begin{equation}L(x,M)&#x3D;||d_\\phi(f_θ(x\\odot M))\\odot(1−M)−x\\odot(1 − M)||^2\\end{equation}（3） 在原始的 MAE 中，编码器只生成未被掩蔽 patch 的 token，而解码器只在训练过程中预测被掩蔽 patch。在此实验中，为了简单起见，假设编码器生成整个输入 patch 的 token，而解码器也是预测整张图像。 在 MAE 中，特征嵌入的维度是远远大于输入图像维度的，也就是说编码器有足够的空间来捕捉输入图像中的所有信息。因此编码器理论上可以实现在不丢失任何信息的情况下对输入图像进行特征提取。 因此，可以假设对于编码器 f_θ(\\cdot) ，存在一个参数为 \\phi’ 的网络 d’_{\\phi’}(\\cdot) ，满足 d’{\\phi’}(f_θ(x\\odot(1-M)))\\odot(1-M)\\approx x\\odot(1-M) 。 据此，可以将公式 3 重写为以下等效形式： L(x,M)&#x3D;||d_\\phi(f_θ(x\\odot M))\\odot(1−M)−d’_{\\phi’}(f_θ(x\\odot(1-M)))\\odot(1-M)||^2\\ s.t.\\quad\\phi’&#x3D;\\underset{\\phi’}{argmim}\\underset{x’～\\mathcal D}{\\mathbb{E}}||d’_{\\phi’}(f_θ(x’\\odot(1-M)))\\odot(1-M)-x’\\odot(1-M)||^2（4） 其中 d’_{\\phi’}(\\cdot) 近似于 f_θ(\\cdot) 的“逆”表示，因此可以让 d’_{\\phi’}(\\cdot) 使用 d_\\phi(\\cdot) 相同的结构。令 d’ &#x3D; d ，可以构建一个新的相似性度量方法： \\begin{equation} \\overline{\\mathcal{M}{\\phi, \\phi^{\\prime}}}\\left(z_1, z_2\\right) \\triangleq\\left|\\left(d\\phi\\left(z_1\\right)-d_{\\phi^{\\prime}}\\left(z_2\\right)\\right) \\odot(1-M)\\right|^2 \\end{equation}（5） 数据增强方法： \\begin{equation} \\mathcal T_1(x)&#x3D;x\\odot M,\\mathcal T_2(x)&#x3D;x\\odot(1-M) \\end{equation}（6） 则可将 MAE 的损失函数公式 4 改为以下对比学习形式： \\begin{equation} \\begin{aligned}L(x, M ; \\theta, \\phi) &amp; &#x3D;\\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(f_\\theta\\left(\\mathcal{T}_1(x)\\right), f_\\theta\\left(\\mathcal{T}_2(x)\\right)\\right) \\\\text { s.t. } \\quad \\phi^{\\prime} &amp; &#x3D;\\underset{\\phi^{\\prime}}{\\arg \\min } \\underset{x^{\\prime} \\sim \\mathcal{D}}{\\mathbb{E}}\\left|\\left(d_{\\phi^{\\prime}}\\left(f_\\theta\\left(\\mathcal{T}_2\\left(x^{\\prime}\\right)\\right)\\right)-\\mathcal{T}_2\\left(x^{\\prime}\\right)\\right) \\odot(1-M)\\right|^2\\end{aligned} \\end{equation}（7） 【结果与讨论】对于公式 7 ，我们可以把他看作是对比学习的一种特殊情况：损失函数用来最小化来自两个掩蔽变换产生的表征之间的差异。因此，可以推测出 MIM 是在鼓励模型学习原始图像的遮挡不变特征。 虽然公式 7 在理论上明确揭示了 MIM 鼓励学习遮挡不变特征，但公式 7 涉及嵌套优化，这是一个缺点，很难计算。 因此，作者将提出了公式 7 的联合优化形式，命名为 R-MAE（或 RelaxMIM）： \\underset{\\theta, \\phi, \\phi^{\\prime}}{\\min} \\underset{x \\sim \\mathcal{D}}{\\mathbb{E}} \\overline{\\mathcal{M}{\\phi, \\phi^{\\prime}}}\\left(f_\\theta\\left(\\mathcal{T}1(x)\\right), f\\theta\\left(\\mathcal{T}2(x)\\right)\\right)+\\lambda\\left|\\left(d{\\phi^{\\prime}}\\left(f_\\theta\\left(\\mathcal{T}_2(x)\\right)\\right)-\\mathcal{T}_2(x)\\right) \\odot(1-M)\\right|^2（8） 假设二：MIM 中的相似性度量是可替换的【背景】比较公式 7 与公式 2 ，我们可以发现，与对比学习相比，MIM 有点不同： 1）数据转换 \\mathcal T(\\cdot) ：传统的对比学习方法通常采用 random crop，而 MIM 方法采用 patch masking； 2）相似性度量 \\mathcal M(\\cdot，\\cdot) ：对比学习通常使用 InfoNCE loss，而 MIM 使用一个相对复杂的公式 \\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(\\cdot, \\cdot\\right) 。 那么 MIM 中的复杂相似性度量方法 \\mathcal M(\\cdot, \\cdot) 是否真的很重要呢？ 【方法】为了解答这一问题，作者使用 InfoNCE loss 取代原有的 \\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(\\cdot, \\cdot\\right) 方法，构建了一个新的孪生 MIM 模型命名为 contrastive MAE（C-MAE）。 使用新的相似性度量方法： \\widetilde{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(z_1, z_2\\right) \\triangleq L_{\\mathrm{NCE}}&#x3D;-\\log \\frac{\\exp \\left(s\\left(z_1, z_2\\right) &#x2F; \\tau\\right)}{\\sum_j \\exp \\left(s\\left(z_1, z_j^{\\prime}\\right) &#x2F; \\tau\\right)}（9） s\\left(z, z^{\\prime}\\right)&#x3D;\\frac{q_{\\phi^{\\prime}}\\left(p_\\phi(z)\\right) \\cdot p_\\phi\\left(z^{\\prime}\\right)}{\\left|q_{\\phi^{\\prime}}\\left(p_\\phi(z)\\right)\\right| \\cdot\\left|p_\\phi\\left(z^{\\prime}\\right)\\right|}（10） 其中， p_\\phi(\\cdot) 和 q_{\\phi’}(\\cdot) 是来自 BYOL（ Bootstrap your own latent - A new approach to selfsupervised learning） 的 project head 和 predict head，均使用 MLPs 实现。 τ 是 softmax 的温度（参考 An empirical study of training self-supervised vision transformers）。 C-MAE 的损失函数： L\\left(x, M ; \\theta, \\phi, \\phi^{\\prime}\\right)&#x3D;\\widetilde{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(f_\\theta\\left(\\mathcal{T}_1(x)\\right), f_\\theta\\left(\\mathcal{T}_2(x)\\right)\\right) C-MAE 的其他配置信息可以查看原文。 【结果与讨论】表 1 展示了 C-MAE 和其他一些自监督方法的微调结果。可以看出： C-MAE 与 MAE 的微调结果相差不多，说明 MIM 中常用的相似性度量方法 \\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(\\cdot, \\cdot\\right) （编码器&#x2F;损失函数设计）在 MIM 中并不重要； C-MAE 与 DINO 等方法的微调结果相差不多，即使前者主要采用随机 patch 掩蔽而后者涉及复杂的数据增强方法，说明数据增强方法在 MIM 中并不重要。 表1：ImageNet 微调中 C-MAE 与 ImageNet 微调预训练方法的比较。所有的模型都是基于 ViT-B 的。假设三： MIM 可以学习一种 data-agnostic 的初始化【背景】如上所述，学习遮挡不变特征是 MIM 方法成功的关键。那么 MIM 如何对不变性进行建模呢？有两种假设： 假设 3.1：遮挡不变性以 data-agnostic（在这里应该指模型性能与数据规模、类型无关） 的方式表示，只有在最重要的输入部分没有被掩盖的情况下，输出特征才是鲁棒的。（√） 假设 3.2：不变性需要从大量数据中获得知识。 【方法】为了验证假设，作者减少了 MAE 预训练的图像数量，只使用从 ImageNet 训练集中随机采样的 1000 张图像中的 1 张，因此在预训练阶段，来自训练数据的语义信息应该非常有限。 【结果与讨论】从表 2 的实验结果可以看出： 使用 MAE 预训练迭代次数为 5 次时，其微调结果比从头开始训练 100 次好得多，与从头开始训练 300 次不相上下。 当预训练图像的数量增加到 1000 时，微调结果没有改善。 由于不太可能只有一张图像包含整个数据集的大部分语义信息，因此实验提供了强有力的证据，证明 MIM 可以学习到一种有利的初始化，更重要的是，这是 data-agnostic 的。 表2. 用不同图像数量预训练的 MAE 微调结果比较。表 3 表明，图像采样策略的选择并不影响微调的精度，进一步表明 MIM 预训练带来的优势中可能不包括类别信息。 表3. 不同图像采样策略的比较。对于 MAE 预训练，分别采用不同的策略从 ImageNet 中采样 1000 张图像。 （本篇博文内容并没有包含论文所有的验证实验，详情请查看原文）","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | 理论：对比学习和掩蔽图像建模究竟有何不同？","slug":"Masked_Image_Model/MIM-理论对比学习和掩蔽图像建模究竟-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-理论对比学习和掩蔽图像建模究竟-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-%E7%90%86%E8%AE%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%8E%A9%E8%94%BD%E5%9B%BE%E5%83%8F%E5%BB%BA%E6%A8%A1%E7%A9%B6%E7%AB%9F-Glenn/","excerpt":"","text":"题目： WHAT DO SELF-SUPERVISED VISION TRANSFORMERS LEARN?单位：基因泰克、NAVER AI Lab论文网址：https://arxiv.org/abs/2305.00729论文代码：未公开首次发布时间：2023 年 5 月 1 日（收录于 ICLR 2023） 123456@article&#123;park2023self, title=&#123;What Do Self-Supervised Vision Transformers Learn?&#125;, author=&#123;Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo&#125;, journal=&#123;arXiv preprint arXiv:2305.00729&#125;, year=&#123;2023&#125;&#125; 主要内容 对比学习（CL）和掩蔽图像建模（MIM）是目前主流的自监督架构。由图 1 可得，CL 在线性探测和小模型方面优于 MIM，而 MIM 在微调精度、大模型和密集预测方面表现出色。 图1. CL 和 MIM 的对比实验作者对 CL 和 MIM 在表示和下游任务的性能方面的差异进行了比较研究。结果表明，基于 ViT 的这两种自监督架构具有以下特性： 1）CL 相较于 MIM 能够更好地捕捉全局依赖，例如物体的形状。这主要体现在 ViT 的后期层。对比学习通过将不同的图像表示引入到相同的空间中，并让模型学习区分它们，从而帮助 ViT 在图像表示空间中实现线性分离。 然而，CL 的这种特性也有一些缺点。它导致所有 query tokens 和 heads 的自注意力变得同质化（homogenized），这意味着所有 tokens 和 heads 的注意力权重变得相似。这种自注意力的同质性降低了表示的多样性，使得模型在应对不同任务和场景时的扩展性和密集预测性能受到影响。 2）CL 利用低频信号，这些信号通常代表了图像中的全局信息和形状。因此，CL 倾向于关注形状特征。相反，MIM 则关注高频信号，这些信号通常代表了图像中的局部信息和纹理。因此，MIM 更倾向于关注纹理特征。 3）CL 在后期层中起着至关重要的作用，而 MIM 主要关注早期层。 这也从理论角度验证了最近一些将 MIM 和 CL 结合工作的有效性。 方法与结果讨论 2.1 从自注意力角度来解释图 1 的现象CL 主要捕捉全局依赖： 通过注意力距离（attention distance）来衡量自注意力（self-attention）的范围。注意力距离被定义为 query tokens 和 key tokens 之间的平均距离，同时考虑它们的自注意力权重。因此，注意力距离在概念上类似于 CNNs 中感受野的大小。 图3显示，CL（MoCo）的注意力距离显著高于 MIM（SimMIM），尤其是在后面的层中。结合图 2 的定性可视化分析，这意味着 CL 的表示包含全局信息和形状特征，因此它可以帮助 ViT 模型在图像中区分不同的物体。 相反，MIM的自注意力主要捕捉局部关系。这表明 MIM 在识别整个物体及其形状方面可能存在困难。 图2. CL 和 MIM 注意力距离的定性可视化分析图3. CL 和 MIM 注意力距离的定量可视化分析CL 的自注意力会变得同质化： 图 2 还展示了来自两个不同空间位置的 query tokens 的注意力图。 与 MIM 相比，CL 的自注意力对于这两个 query tokens 显示出几乎相同的物体形状。作者将这一现象称为注意力坍缩成同质性（attention collapse into homogeneity）。这种坍缩趋势在 CL 的所有 heads 和 query tokens 中都可以观察到。 相比之下，MIM 的自注意力能更好地捕捉到这两个 query tokens 之间的差异，而 CL 的自注意力则更倾向于将它们视为相同的物体形状。 作者还使用了归一化互信息（normalized mutual information，NMI）来测量注意力坍缩。 假设 p(q) 是 query tokens 的分布，且这些 query tokens 在空间坐标上是均匀分布的，即 p(q) &#x3D; 1&#x2F;N ，其中 N 是 tokens 的数量。那么 query tokens 和 key tokens 的联合分布为 p(q, k) &#x3D; π(k|q)p(q) ，其中 π(k|q) 是 softmax 归一化的自注意力矩阵。因此，归一化互信息可以表示为 I(q, k) &#x2F; √(H(q)H(k)) ，其中 I(·, ·) 表示互信息， H(·) 表示边缘熵。 归一化互信息的值较低意味着注意力图对 query tokens 的依赖较弱，这暗示了注意力坍缩成同质性。相反，归一化互信息的值较高表示注意力图对 query tokens 的依赖很强。 如图 4 所示，在后期层中，CL 的互信息显著低于 MIM ，这意味着 CL 的自注意力更容易坍缩成同质化分布。 图4. 关于 NMI 的注意力坍缩程度注意力坍缩会降低表征多样性： 作者猜想，自注意力坍缩成同质性最终会导致同质化的 token 表示。 为了验证猜想，作者使用三种相似性： 不同自注意力 heads 之间（heads）。衡量不同 heads 之间的表示相似性，用于评估不同 heads 是否捕获了多样性的特征。 自注意力前期层和后期层之间（depths）。衡量自注意力前后层表示的相似性，用于评估表示在模型层中的变化程度。 不同 tokens 之间（tokens）。衡量不同 tokens 之间表示的相似性，用于评估模型对不同输入 tokens 的响应程度。 通过计算这三种余弦相似性，可以评估表示在不同自注意力 heads、depths 和 tokens 之间的多样性。如果观察到表示的余弦相似性较高，那么这可能意味着表示具有同质性；反之，则表明表示具有较高的多样性。 如图 5 所示，CL 的相似性在后期层明显高于 MIM，表明 CL 的表示具有显著的同质性。即使增加模型的大小也不能解决 CL 所面临的问题，反而可能使其恶化： 增加 heads 的数量（ViT-S 到 ViT-B；图 5a）提高了 MIM 的表示多样性，但对 CL 的多样性改善甚微。 增加 CL 的深度（ViT-B 到 ViT-L；图 5b）降低了表示的多样性。 这些结果进一步支持了前面提到的猜想，即自注意力坍缩成同质性最终导致同质化的 token 表示。对比学习 CL 的表示在后期层次中的同质性可能限制了模型的泛化能力和性能。 同时，这些发现表明，通过调整模型参数（如 heads 的数量和层数）来改善表示多样性可能对 MIM 更有效，而对 CL 的影响有限。 图5. 不同相似度的对比研究此时，可以解释图 1 的现象： 在线性探测任务中，CL 优于 MIM，因为它能捕捉形状信息，有助于识别物体并区分图像。尽管 MIM 保留了纹理和表示的多样性，但它们与物体或内容的关联可能不如形状那么强。 注意力坍缩阻止了 CL 充分利用 ViTs 的 heads、depths 和 tokens。由于同质化的表示对于改善 token 表示没有太大帮助，使用 CL 训练的 ViTs 浪费了大部分网络能力。因此，在大型模型中，MIM 的微调准确率明显高于 CL。 对于密集预测任务，CL 不太适用，因为 token 特征在空间坐标上呈同质化分布。 2.2 表征如何被转换？CL 将所有 tokens 统一地进行转换，而 MIM 在处理 tokens 时更具个性化： 为了展示 CL 和 MIM 如何转换 token 表示，作者将它们可视化在表示空间中。 图 6 展示了 ImageNet 验证集中单个图像样本在自注意力模块前后（第一层和最后一层）的196个（14×14 patches）tokens。 图 6a 表明 CL 的自注意力将所有 tokens 统一地进行平移。这种现象的发生是因为其同质性。这意味着 CL 关心的是整体而非个别 tokens。然而，如图 6b 所示，CL 通过将表示分布的“中心”相互分离来帮助区分图像。简而言之，尽管 CL 失去了区分 tokens 的能力，但它使得图像在线性表示空间中可分离。 与此相反，如图 6c 所示，MIM 在处理 tokens 时更具个性化，MIM 改变了单个图像中 tokens 之间的距离以及表示分布的体积。 图6. CL 和 MIM 表征转换方式可视化CL 利用低频信号，MIM 则关注高频信号： 可以假设 CL 捕获空间维度上的低频信息，而 MIM 捕获高频信息。 为了从频率的角度支持这个观点，作者对表示进行了傅立叶分析。 图 7 显示了 CL 和 MIM 的相对振幅（表示的最高频率和最低频率之间的振幅差）。 结果表明，CL 的高频振幅明显小于 MIM，说明 CL 主要利用了全局结构和形状等低频空间信息。 相反，MIM 通常使用高频的空间信息，如局部结构和精细的纹理。 图7. CL 和 MIM 的相对振幅CL 是形状偏置的，而 MIM 是纹理偏置的： 基于傅里叶分析的结果，可以假设 CL 和 MIM 各自对形状和纹理都具有偏置。 为了证明这一说法，作者使用 Stylized ImageNet，这是一个使用 AdaIN 进行纹理改变的数据集。 图 8a 报告了在 Stylized ImageNet 上的线性探测结果，以评估预训练模型对形状和纹理的偏好。 与使用监督学习预训练的模型相比，CL 更依赖于图像的形状，而 MIM 更依赖于图像的纹理来进行图像分类。换句话说，CL 对纹理变化具有鲁棒性，而 MIM 对它们较为敏感。 这种差异表明，根据任务需求，可以选择适当的预训练方法以获得对形状或纹理更敏感的模型。例如，在需要关注物体形状的任务中，CL 可能表现更好；而在需要关注纹理和细节的任务中，MIM 可能表现更优。 另外，在 ImageNet 数据集上测量基于频率的随机噪声对准确率影响的实验也显示了同样的结果。 如图 8b 所示，CL 对高频噪声具有鲁棒性，而 MIM 明显更易受影响。这也可以解释 CL 在对抗扰动下的鲁棒性。 图7. 探究 CL 和 MIM 的形状和纹理偏置 2.3 哪些组件在 CL 和 MIM 种起到了重要作用？CL 的后期层和 MIM 的早期层是重要的： 在图 8 中，可以发现： MIM 在模型开始阶段的线性探测准确率高于 CL。而在模型结束时，CL 的表现优于 MIM。这个结果表明，CL 的后期层和 MIM 的早期层在产生线性可分离表示方面起着重要作用。 CL 的准确率随着深度的增加而增加，但 MIM 在模型结束时的准确率却在降低，即 MIM 的后期层在分离表示方面并不是很有帮助。作者将这个现象解释为：具有浅预测 head 的 MIM 方法，如 SimMIM，将主干网络的后期层用作解码器。而具有深度自注意解码器的 MIM，例如 MAE，可以在线性探测性能方面发挥作用。此外，这也解释了为什么 SimMIM 在后期层中的高频分量会下降，如图 7 所示。 即使是 MIM 的最高线性探测准确率也低于 CL。 图8. CL 和 MIM 的线性探测随深度而改变显式解码器有助于 ViTs 进一步利用 MIM 的优势： 就如上面分析的那样，具有浅预测 head 的 MIM 的隐式解码器（如 SimMIM）可能会影响性能。 MAE 通过引入深度显式 ViT 解码器并仅在单独的解码器中重构掩蔽 tokens 来解决这个问题。 图 9a 的结果表明，MAE 在编码器的后几层中的互信息比 SimMIM 低，但在解码器中的互信息较高，这意味着解码器根据相邻的 tokens 重建被掩盖的 tokens。 这种现象表明，MAE 在编码器部分与 SimMIM 具有类似的自注意力特性，但在解码器部分的自注意力表现有所不同。这种高互信息的存在表明，解码器在重建过程中更加关注局部信息，从而有助于在预训练阶段学习丰富的特征表示。 图 9b 展示了 SimMIM 的后四层降低了高频分量。而 MAE 的后几层（除了最后一层）并未减少高频分量。与编码器相比，MAE 的解码器优先考虑低频信息，从而使主干网络能够更有效地利用高频信息。 这种现象表明，通过引入显式解码器，MAE 能够在不损失高频信息的情况下关注低频信息。这使得 MAE 在捕捉图像中的局部纹理和细节方面表现更出色，进而在预训练和下游任务中取得更好的性能。 图9. 每个图的左侧代表编码器，右侧代表解码器。 结论 上述结果表明，将 CL 和 MIM 结合起来训练主干网络可能有助于发挥两种方法的优势。 为了证明 CL 和 MIM 是互补的，作者采用了最简单的方法来调和 CL 和 MIM，即线性组合两种损失，如 \\mathcal L &#x3D; (1 − λ)\\mathcal L_{MIM} + λ\\mathcal L_{CL} ，其中 \\mathcal L_{MIM} 和 \\mathcal L_{CL} 分别表示 MIM 和 CL 的损失， λ 是 CL 的重要性权重。作者发现，这种简单的混合模型通过组合损失训练可以有效地利用两种方法的优势。 图 10a 展示了在 ImageNet 上随着 λ 变化的线性探测和微调准确率。可以看出： 混合模型在这两个方面都优于 MIM（ λ &#x3D; 0 ）和 CL（ λ &#x3D; 1 ）。 图 10b 和图 10c 分别分析了 λ &#x3D; 0.2 的模型在互信息和频域的行为，可以看出： 两个结果都显示混合模型在前几层利用 MIM 特性，在后几层利用 CL 特性。特别是，图 12b 表明，早期层的自注意力根据 query tokens 而改变，但后期层的自注意力则没有改变。同样，图 12c 显示，早期层利用高频信息，而后期层试图利用低频信息。 图10. CL 和 MIM 是互补的 更多实验请查看原文。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | PixMIM：重新思考掩蔽图像建模中的像素重建","slug":"Masked_Image_Model/MIM-PixMIM重新思考掩蔽图像建-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-PixMIM重新思考掩蔽图像建-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-PixMIM%E9%87%8D%E6%96%B0%E6%80%9D%E8%80%83%E6%8E%A9%E8%94%BD%E5%9B%BE%E5%83%8F%E5%BB%BA-Glenn/","excerpt":"","text":"题目：PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling作者单位：上海人工智能实验室、西蒙菲莎大学、香港中文大学论文网址：https://arxiv.org/abs/2303.02416论文代码：https://github.com/open-mmlab/mmselfsup首次发布时间：2023 年 3 月 4 日 前言前面介绍过微软亚研院的新 MIM 架构 DeepMIM，与 CVPR23 的 LocalMIM 一样都是主张将 Deep Supervision 引入 MIM 中，后者提到，从底层到高层特征图的尺度由粗到细对各种视觉任务（检测，分割和分类）都有增益。然而今天介绍的 PixMIM 认为，在 MIM 任务中模型应该更关注浅层特征，即形状偏置，而不用浪费建模能力来关注短期依赖和高频细节。 DeepMIM 解读看这里： LocalMIM 解读看这里： 主要内容 早期的 MIM 方法主要从三个方面来改进建模： 1）重建目标，将 Pixel 替换为 Frequency、HOG 等来提升表征质量； 2）额外任务，例如，CMAE 除了重建任务以外，引入了一个对比学习的任务； 3）知识蒸馏，利用强大的预训练模型来蒸馏学习，例如，MASKDISTILL 使用教师网络输出的表征作为 target。 但这些方法要么会使整个框架复杂化，要么不可避免地引入不可忽视的训练成本（证明？）。 本篇论文从输入 patch（保留更多的前景） 和重建目标（使模型更关注低频信号）两个方面入手，提出了一个简单而有效的方法 PixMIM。 分析 本篇论文基于 MAE 实验分析得出两个重要的观察结论： 1）当 MAE 使用的 Random Resized Crop 图像增强方式与 75% 的掩蔽策略结合时，输入的可见 patch 中就只剩平均 17.1% 的前景。这种较低的前景覆盖范围阻碍了模型有效捕获形状和语义先验的能力，从而限制了表征的质量，如图1所示。 图1. MAE 的输入 patch 的可视化。对于每个例子，从左到右展示了原始图像、Random Resized Crop 之后的图像，以及由 MAE 的掩蔽策略产生的可见 patch。Random Resized Crop 与 75% 掩蔽策略的耦合导致输入中前景对象的覆盖率低，损害了表征质量。分析方法如下： 使用 PSSL（Distilling ensemble of explanations for weakly-supervised pre-training of image segmentation models 提出的方法）对 ImageNet-1K 取二值掩模，并提出了一个前景覆盖率百分比度量 \\mathcal J(\\mathcal F) 来评估图像处理操作 \\mathcal F(\\cdot) ，用表示。如图 2 所示， \\mathcal J(\\mathcal F) 定义为 A_2 和 A_1区域之间的比值。A_1 和 A_2 分别为原始图像 I 和处理后的图像 \\mathcal F(I) 中的前景对象的面积。 图2. 目标覆盖率的计算方法。在上例中，A1 是前景区域。A2 是黄色区域的面积。蓝色的矩形是由数据增强产生的裁剪图像。目标覆盖率由 A2 和 A1 之间的比率得到。然后，利用该度量来研究 MAE 对增强函数 \\mathcal A(\\cdot) 和腐败函数 \\mathcal M(·) 的选择是如何影响对象的覆盖范围的。如表 1 所示，MAE 对 \\mathcal A(\\cdot) 使用了常用的 Random Resized Crop，对 \\mathcal M(·) 使用了 75% 的掩蔽操作。最终发现 \\mathcal J(\\mathcal A) &#x3D; 68.3 %，而 \\mathcal J(\\mathcal M◦\\mathcal A)&#x3D; 17.1 %，这表明在 MAE 的输入中可能缺乏前景信息。 表1. 各种 MIM 方法。aug：数据增强。mask：掩码比率。MSA：多头自注意力。RRC：Random Resized Crop。前景对象通常比背景编码更多的语义概念。DeiT III（Deit III: Revenge of the vit）认为，缺少前景可能会导致监督学习中的 sub-optimal optimization。在 MIM 的背景下，Random Resized Crop 和掩蔽操作的耦合可能会阻碍表征学习。 2）自 MAE 出现以来，大多数 MIM 方法都采用原始像素作为重建目标。训练目标需要对掩蔽 patch 进行完美的重建，包括复杂的细节，例如纹理，浪费建模能力来关注短期依赖和高频细节。根据最近对形状和纹理偏置的研究（Partial success in closing the gap between human and machine vision，Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness），更依赖于形状偏置的模型往往在迁移学习中表现更好，对领域变化更鲁棒。然而，重建细粒度的细节不可避免地会引入对纹理的偏置，降低表征质量。 方法 根据上面提到的两个方面：输入 patch 和 重建目标。PixMIM 使用了更为保守的图像增强方法，并构建了一个低频目标生成器。 图3. PixMIM 的架构。首先使用低通滤波器从重构目标中去除高频分量，从而降低了纹理主导细节的重构，并降低学习低频模式的优先级。然后，使用 Simple Resized Crop 替换常用的 Random Resized Crop，以缓解输入 patch 中的前景缺失的问题。 3.1 更为保守的图像增强方法基于上述分析，我们希望在模型的输入 patch 中保留更多的前景信息。由于高掩蔽比对于 MIM 学习有效表示至关重要，最直接的策略是保持腐败 \\mathcal M(\\cdot) 不变，但使增强函数 \\mathcal A(\\cdot) 更加保守。 Simple Resized Crop 是 AlexNet 中使用的图像增强方法，先将短边 resize 到指定分辨率，然后再中心裁剪到指定分辨率（例如，358×558 → 224×558 → 224×224）。 Center Crop 总是从图像的中心截取固定大小的裁剪。 由图 4 可以看出 Simple Resized Crop + 75% 掩蔽策略可以保留最大的前景覆盖率。 图4. 前景覆盖率分析。即使在 MAE 的 75% 掩蔽策略下，SRC 保留比例也高于 RRC 和 CC。请注意，当应用 MAE 的掩蔽策略时，前景覆盖率的上限为25%。(RRC：Random Resized Crop，SRC：Simple Resized Crop，CC：Center Crop）需要注意的是，当没有图像掩蔽时，Simple Resized Crop 将 \\mathcal J (\\mathcal F) 从 Random Resized Crop 的 68.3% 提高到 88.2%，表明它提供的多样性不如 Random Resized Crop，这解释了 DeiT III 观察到的监督图像分类的性能退化。但与监督学习不同的是，MIM 中的 75% 图像掩蔽已经提供了足够的随机性，并且 Simple Resized Crop 的使用不会像在监督学习中那样损害多样性（证明？）。 3.2 低频目标生成器1）从空间到频率的域转换，使用二维离散傅里叶变换（DFT） \\mathcal F_{DFT} 。 2）低频分量提取，使用低通滤波器 \\mathcal F_{LPF} ： \\mathcal F_{LPF}(u,v)&#x3D;\\begin{cases} 1, \\sqrt{((u-u_c)^2+(v-v_c)^2)}\\leq r,\\ 0, otherwise. \\end{cases} 其中 v_c 和 u_c 为频谱的中心坐标。 r 是圆形理想低通滤波器的带宽，用于控制有多少高频分量将从频谱中被滤掉， r ∈[0，min(\\frac{H}{2},\\frac{W}{2})] 。 提取过程用 \\mathcal F_{LPF}(u,v)⊗\\mathcal F_{DFT}(I_i)(u,v) 表示。 3）重建目标的生成，使用离散傅里叶反变换（IDFT） \\mathcal F_{IDFT} 重建 RGB 图像： Y&#x3D;\\mathcal F_{IDFT}(\\mathcal F_{LPF}(u,v)⊗\\mathcal F_{DFT}(I_i)(u,v)) 结果 图5. MAE 和 PixMIM 的频率分析。计算了不同频率间隔下的重建图像的信噪比（PSNR），并从 ImageNet-1K 中获取 50,000 张图像的平均值。PixMIM 将模型的关注点转向低频分量，以减少纹理偏置。表2. MIM 方法对各种下游任务的性能比较。报告了对 ImageNet-1K 的微调（ft）和线性探测（lin）实验、COCO 和 ADE20K 的结果。所有实验的主干网络都是 ViT-B。∗：结果来自于运行官网代码。†：由于 MAE、LSMAE 和 ConvMAE 用于目标检测时没有统一的微调 Epoch，因此使用相同数量的 Epoch 对 PixMIM图6. 性能与 PixMIM 的 epoch 图。在不同的训练 epoch，PixMIM 在不同的评估方法中始终为基线 MIM 方法带来显著的收益。左： ImageNet 微调。中间：ImageNet 线性探测。右图：ADE20K 语义分割。具体结果查看原文。 使模型更关注低频信号的观点是否与 CVPR23 的 LocalMIM（Masked Image Modeling with Local Multi-Scale Reconstruction）和 DeepMIM（DeepMIM: Deep Supervision for Masked Image Modeling）观点矛盾呢？ 且保留更多的前景是否会使得自监督训练任务可能过于简单，模型无法学习有效的表征呢？","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | MixedAE：Patch Mix 的调教手册","slug":"Masked_Image_Model/MIM-MixedAEPatch-Mi-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-MixedAEPatch-Mi-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-MixedAEPatch-Mi-Glenn/","excerpt":"","text":"题目：Mixed Autoencoder for Self-supervised Visual Representation Learning单位：香港科技大学、华为诺亚方舟实验室论文网址：https://arxiv.org/abs/2303.17152论文代码：未公开首次发布时间：2023 年 3 月 30 日（收录于 CVPR 2023） 123456@article&#123;chen2023mixed, title=&#123;Mixed Autoencoder for Self-supervised Visual Representation Learning&#125;, author=&#123;Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan&#125;, journal=&#123;arXiv preprint arXiv:2303.17152&#125;, year=&#123;2023&#125;&#125; 前言前面介绍的论文（Understanding Masked Image Modeling via Learning Occlusion Invariant Feature）提到， MIM 中的 patch masking&#x2F;zero（可以理解为用 0 来填充 patch） 可以看作是一种数据增强方式。从这个角度来看，可以再衍生出一种 MIM 的改进方向：Input augmentation。 MAE 原文中做过这样的实验，输入图像加入 color jittering 后，迁移的效果反而降低，这说明相比于对比学习， MIM 对数据增强的方法可能有不同的偏好。 MixMIM 提出使用另一幅图像的可见 patch 替换该图像的 patch masking&#x2F;zero，即创建一个混合图像，然后再进行双重重建。其间，也尝试使用了其他增强方式，比如 patch shuffle、patch zoomin 等等，最终的实验表明，patch mix 是个简单有效的方法。 （PS：我认为 Input augmentation 与 Masking strategy 的区别在于前者是针对 patch masking&#x2F;zero 这种方法的，而后者是针对如何进行或者对哪个部分使用 patch masking&#x2F;zero 的） 然而 MixedAE 证明了如果直接在 MIM 框架中引入 patch mix，会导致模型性能下降，因此提出了 homologous recognition（同源识别） 的辅助代理任务。图 1 展示了 MixedAE 的有效性。 图1. 在 ImageNet-1K 上的微调精度。ID 表示 instance discrimination。 Understanding Masked Image Modeling via Learning Occlusion Invariant Feature 介绍看这里： MixMIM 介绍看这里： 主要内容 使用 patch mix 替换 MAE 中的 patch masking&#x2F;zero，构建了一个简单的 baseline 结构。 利用该 baseline 分析了直接引入 patch mix 会导致 MIM 模型性能下降的原因：互信息的增加。 为了解决这个问题，作者提出了 homologous recognition（同源识别） 的辅助代理任务，不仅通过明确要求每个 patch 识别同源 patches 来缓解互信息的增加，而且还可以执行 object-aware 的自监督预训练，以获得更好的下游密集感知性能。 分析 2.1 构建一个 mix baseline使用 patch mix 替换 MAE 中的 patch masking&#x2F;zero，我们可以构建一个简单的 baseline 结构。 给定 Batch size 为 B ，则输入图像可以表示为 \\left{x_i\\right}^B_{i&#x3D;1} 。 将输入图像分为 B×r 个组，每个组会生成一张混合图像，其中 r\\in (0, 0.5] 是混合比。 每组将会不重叠、顺序地从 Batch 中取 1&#x2F;r 张图像，按下面方式进行 patch 混合： $$ \\hat x^j&#x3D;\\sum_{i&#x3D;1}^{1&#x2F;r}\\mathbb I(M^j&#x3D;i)x_i^j $$ 其中， j\\in [1, Br] 表示第 j 组， \\mathbb I(\\cdot) 表示指示器， \\mathbb I(M^1&#x3D;1) 表示第 1 组第 1 张图像的掩蔽策略。 将 \\hat x^j 喂入 encoder 后得到 \\hat z^j 。 然后通过插入 [MASK] token 来实现 unmix： $$ z^j_i&#x3D;\\mathbb I(M^j&#x3D;i)\\hat z^j+[1-\\mathbb I(M^j&#x3D;i)][MASK] $$ 再将 unmix 后的 \\left{z^j_i\\right}^{1&#x2F;r}_{i&#x3D;1} 送入 decoder 中，得到 y^j_i 。 重建损失函数可以写为： $$ \\mathcal L_{recon}&#x3D;\\sum^{1&#x2F;r}_{i&#x3D;1}[1-\\mathbb I(M^j&#x3D;i)(y_i^j-x_i^j)^2 $$ 2.2 实验 ImageNet ADE20K Top-1 Acc. mIoU MAE 82.7 46.1 mix baseline 82.4 45.0 由表格可以看出，我们构建的 mix baseline 性能甚至比 MAE 更差。 2.3 分析原因以 r&#x3D;0.5 为例， X_1 ， X_2 表示为两个输入图像， M 表示为随机掩模。则混合输入可以表示为： $$ \\sigma_{mix}(\\left{X_1,X_2\\right},M)&#x3D;\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)X_2 $$ 而 MAE 输入可以表示为： $$ \\sigma_{mix}(X_1,M)&#x3D;\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0 $$ 因此，以 X_1 作为重建目标，可以将混合输入与重建目标 X_1 之间的互信息（MI）表示为： $$ \\begin{aligned} I(\\sigma_{mix}(\\left{X_1,X_2\\right},M);X_1) &amp;&#x3D; I(\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)X_2;X_1)\\ &amp; &#x3D; H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)X_2)\\ &amp; &#x3D; H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0+\\mathbb{I}(M&#x3D;1)\\vec 0+\\mathbb{I}(M&#x3D;2)X_2)\\ &amp; &#x3D; H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0,\\mathbb{I}(M&#x3D;1)\\vec 0+\\mathbb{I}(M&#x3D;2)X_2)\\ &amp; \\geqslant H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0)\\ &amp; &#x3D; I(\\sigma_{MAE}(X_1,M);X_1) \\end{aligned} $$ 公式推理证明混合输入 \\sigma_{mix}(\\left{X_1,X_2\\right},M) 与重建目标 X_1 之间的 MI，不小于 MAE 输入 \\sigma_{MAE}(\\left{X_1\\right},M) 与 X_1 之间的 MI。 当 x_2 为重建目标时，上述结论也成立。 因此，与减少 MI 的 patch mask&#x2F;zero 不同，直接混合反而会增加模型输入和重建目标之间的 MI，从而简化重建任务。 混合带来的 MI 增加是 target-invariant 的，这表明当目标是有监督学习的标签或对比学习的正样本时，上述结论也成立。这可能解释了为什么直接混合有利于有监督学习和对比学习，而不利于 MIM。 方法 导致 MI 增加的一个因素是，MAE 在 ViT 中使用了 global self-attention，通过这种方式，每个查询 patches 将不可避免地关注来自其他图像的异源 patches。由于生成式建模的不确定性，异源 patches 可能提供了完成重建的捷径。如图 2 所示，黄瓜的绿色是重建图中狐狸身后森林的“有价值”线索。 图2. 生成式建模的不确定性为了解决这个问题，作者提出了一种新的代理任务，称为同源识别，以强制每个查询明确识别并只关注同源 patch。 图3. Mixed Autoencoder（MixedAE）的网络结构1）同源 attention：通过使用 TopK(\\cdot) 的采样操作，强制每个查询 patches 只关注注意分数最高的关键 patches，从而动态识别同源 patches。具体来说，同源 attention 可以表述为： $$ A_{HomoAtt}&#x3D;softmax(TopK(qk^T&#x2F;\\sqrt D_h)) $$ 默认情况下，除第一层外，ViT 中的所有 self-attention 操作都被同源 attention 替换。 2）同源对比：旨在通过鼓励编码器提取同源 patches 的特征相似，而异源 patches 特征不同。对比损失为： $$ \\mathcal{L}_{HomoCon}&#x3D;-\\sum^L_{l&#x3D;1}\\sum_{l+}log\\frac{exp(cos(\\hat z^j_l,\\hat z^j_{l+})&#x2F;\\tau)}{\\sum_{l’\\neq l}^{L}exp(cos(\\hat z^j_l,\\hat z^j_{l’})&#x2F;\\tau)} $$ 其中， \\tau 为温度， cos(\\cdot, \\cdot) 为余弦相似度。 3）Segment embedding：类似 MixMIM，为了降低预训练难度，除了位置嵌入之外，MixedAE 在混合输入\\hat x^j 中还添加了 Segment 嵌入。 Segment 嵌入对于来自同一图像的 patches 是共享的，而对于来自不同图像的 patches 则是不同的，如图 4 所示。 图4. 不同的 Segment 嵌入代表不同的图像。4）混合模式：为了在不同的训练开销下进行公平的比较，MixedAE 采用了两种混合模式，如图 5 所示。 Compose：每一组生成一个单一的混合样本。 Full：每组独立地进行 1&#x2F;r 次采样来生成 1&#x2F;r 个混合样本。 如果未另行指定，则默认采用 Compose。 图5. 当混合比 r&#x3D;0.5 时，两种混合模式的可视化。最终的损失函数为： $$ \\mathcal L_{MixedAE}&#x3D;\\mathcal L_{recon}+\\lambda \\mathcal L_{HomoCon} $$ 其中平衡权重 λ 默认设置为 0.1。 结果 图 6 可以看出，MAE 关注更具鉴别性的 patches ，如边界，而 MixedAE 更关注前景 patches。因此，作者在前面强调 MixedAE 具有 object-aware。 图6. 从 ImageNet-1K（第 1-3 列）、COCO（第 4-6 列）和 ADE20K（第 7-9 列）的图像的注意力热图可视化。 MAE 和 MixedAE 都在 ImageNet-1K 上进行了 300 个 epoch 的预训练。由表 1 可以得出： 有效性： MixedAE 在不同的预训练时间和额外开销下取得了最先进的性能。 效率： MixedAE 始终超过强 iBOT 基线，而且只需要 53.4% 的预训练开销。 object-aware：当迁移到下游的密集感知任务时，取得了更显著的改进。 表 1. 在 ImageNet-1K 上预训练方法之间的迁移性能比较。∗：部署了一个轻量级解码器来与 BEiT 保持类似的预训练开销。†：在 Tesla V100 GPUs 上估计的 GPU-days。††：iBOT 的有效 epoch。由表 2 可以得出： 1600 个 epoch 预训练的 MixedAE 在所有 11 个分类数据集上均实现了比 MAE 更好的结果，平均准确率为 86.9%，超过了所有同类方法。 表2. 11 个下游分类任务的迁移性能比较。更多实验结果请查看原文。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | Img2Vec：基于 token 多样性原则选择教师模型","slug":"Masked_Image_Model/MIM-Img2Vec基于-token-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-Img2Vec基于-token-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-Img2Vec%E5%9F%BA%E4%BA%8E-token-Glenn/","excerpt":"","text":"题目：Img2Vec: A Teacher of High Token-Diversity Helps Masked AutoEncoders单位：腾讯、浙江大学、北京大学论文网址：https://arxiv.org/abs/2304.12535论文代码： 未公开首次发布时间：2023 年 4 月 25 日 123456@article&#123;pan2023img2vec, title=&#123;Img2Vec: A Teacher of High Token-Diversity Helps Masked AutoEncoders&#125;, author=&#123;Heng Pan and Chenyang Liu and Wenxiao Wang and Li Yuan and Hongfa Wang and Zhifeng Li and Wei Liu&#125;, journal=&#123;arXiv preprint arXiv:2304.12535&#125;, year=&#123;2023&#125;,&#125; 前言前面对 MIM 的小总结中提到，传统的像素级别重建任务可能会限制模型的表达能力。为了提高模型的性能，可以将重建目标替换为更高级的语义表示，如 MaskFeat 使用 HOG 特征，BEiT-v2 使用 CLIP 特征等。这将迫使模型学习更丰富的特征表示，从而有助于下游任务的性能。 究竟哪些高级特征对预训练最有效？ 我们知道这些高级语义特征可以由另一个预训练模型或预训练模型（教师模型）的指数移动平均（EMA）生成。 那么这个问题就可以转化为：在 MIM 中如何选择教师模型对预训练最有效？ 对此，Img2Vec 提出了基于 token 多样性的原则选取教师模型。 主要内容 作者进行了初步分析，探讨了不同高级语义特征作为重建目标的影响以及对下游任务的影响，得出结论：在MIM中，具有出色表现的教师模型未必能带来更好的学生模型。 为解决这一问题，作者提出了一个基于多样性原则的方法来选择 MIM 中的教师模型。基于这一原则，具有高 Token 多样性的较小模型可以表现更优。 此外，作者还提出了一种新的自监督预训练框架，即 Img2Vec，使用小型 ResNet-50 模型作为教师模型，同时引入多特征学习和全局语义学习，以增强 Img2Vec。 方法 2.1 教师模型选择的标准： token 多样性作者采用 ViT-B 作为学生模型，并对其进行 1600 个 epoch 的预训练。采用在不同自监督模型（MAE、SimCLR、DINO，为了公平对比，不使用 CLIP）预训练下的不同教师模型的输出特征作为重建目标。实验结果如表 1 所示，可视化如图 1 所示。 表1. 不同教师模型比较图1. 不同教师模型比较可视化 最后一列显示了学生模型的 top-1 的微调精度。与 MAE-ViT-B 获得的 84.3% 的准确率相比，使用参数量更大的 MAE-ViT-L 作为教师模型可以将性能略微提高 0.1%。这可能归功于 ViT-L 更强的特征提取能力。 相比之下，在相同的教师模型下，与 MAE ViT-B（84.6% 对 84.3%）相比，DINO ViT-B 可以带来显著的改善。这一显著结果表明，教师模型的选择比模型大小和性能影响力更大。 因此，问题来了：影响教师模型选择最重要的因素是什么？ 作者通过计算目标 token 的余弦相似度来可视化它们的关系，如图 2 所示。第一列中带有红色框的区域是 query patch。 图2. 来自不同教师模型的目标相似性的可视化。颜色越亮，值就越大，反之亦然。在图 2 中，可以发现 MAE ViT-B 中的 tokens 之间更为相似，而 DINO ViT-B 中的相似的 tokens 要少得多。 为了对这一现象进行定量分析，作者提出了一个名为“token 多样性”的指标。定义如下： 对于具有 K 个 tokens \\left{y_i\\right}^K_{i&#x3D;1} 的教师特征样本，计算所有 tokens 之间的余弦相似性，并使用最大最小归一化将值映射到 [0，1] 中。然后再将这些相似性平均为样本值。 N 个样本的多样性为：diver&#x3D;1-\\frac{1}{N}\\sum^N_{n&#x3D;1}sim_n其中， sim_n&#x3D;\\frac{1}{K(K-1)}\\sum^K_{i\\neq j}norm\\left{cosine(y_i,y_j)\\right} 是第 n 个样本的相似性。更大的 diver 说明了 tokens 越多样化。 表 1 中还报告了 MAE ViT-B、MAE ViT-L 和 DINO ViT-B 的 token 多样性。这一比较验证了假设，即具有更高 token 多样性的 DINO ViT-B 训练出的学生模型性能更好。 在 MIM 预训练中，掩蔽 ptaches 是通过可见 patches 重建的。如果掩蔽目标和可见目标高度相似，这就意味着，模型不需要学习到区分掩蔽目标和可见目标之间的特征，而只需要记住像素之间的具体匹配关系。这种情况下，模型的泛化能力会受到限制，因为它没有学习到更高级别的抽象特征。这会降低模型的表示学习能力，并导致模型在下游任务中的性能下降。 举例来说，如果一个狐狸的耳朵被遮住了，那么一个基于 MAE 的教师模型可能会容忍学生模型将它预测为狐狸的可见身体特征，因为它们都属于狐狸的一部分。然而，一个基于 DINO 的教师模型则要求学生更准确地预测，因为它需要学生区分不同的视觉区域，而不是只依赖像素级别的匹配关系。 表征的多样性反映了模型的不同辨别能力，即对于不同的视觉区域，模型学到的特征不同，这有助于提高模型的表示学习能力和泛化能力，从而在下游任务中表现更好。 考虑到 ViT 在全局注意力机制中会处理所有的 tokens，它们的输出 tokens 往往彼此相似。而 ConvNets 具有局部性的归纳偏差，它提取邻居特征。因此，ConvNets 可能是生成 MIM 目标的良好替代方案。 从表 1 和图 2 可以看出，SimCLR 和 DINO 预训练的 ResNet-50 比 ViT 获取了更高的 token 多样性。 通过以上定性和定量分析，可以假设 token 多样性是衡量 MIM 重建目标的一个有价值和必要的视角。 2.2 Img2Vec 架构根据以上讨论，作者提出了 MIM 新架构：Img2Vec。 Img2Vec 采用高标记多样性模型，DINO ResNet-50 作为教师模型。此外，还包含两个新的模块，多块特征学习和全局语义学习。整个框架如图 3 所示。 图3. Img2Vec 架构多块特征学习和全局语义学习是 MIM 的常见手段了。 多块特征学习是指将编码器中所有 block 的输出平均后作为编码器的总输出。 全局语义学习是指将编码器最后一个 block 的输出特征经过 mlp 和平均池化，与教师模型输出的重建目标经过平均池化后做损失计算。 结果 如表 2 所示，为了进行公平的比较，所有方法都只在 ImageNet 训练集上进行预训练。 ViT-B、ViT-L、ViT-H 表示各个自监督模型使用这三个模型在 ImageNet-1K 上的 top-1 微调精度； Linear 表示在 ImageNet-1K 上的线性探测结果； AP^{bbox} 和 AP^{mask} 表示各个自监督模型使用 Mask R-CNN 作为检测器在 COCO 上的对象检测和实例分割性能； mIoU 表示各个自监督模型使用 UperNet 在 ADE20k 上的语义分割结果。 表2. 不同模型在不同下游任务上的比较如表 3 所示，多块特征学习模块的在不同教师模型上的消融实验。 表3. 多块特征学习模块的在不同教师模型上的消融实验","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | HPM：引入蒸馏概念自动生成掩蔽策略","slug":"Masked_Image_Model/MIM-HPM引入蒸馏概念自动生成掩蔽-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-HPM引入蒸馏概念自动生成掩蔽-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-HPM%E5%BC%95%E5%85%A5%E8%92%B8%E9%A6%8F%E6%A6%82%E5%BF%B5%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%8E%A9%E8%94%BD-Glenn/","excerpt":"","text":"题目：Hard Patches Mining for Masked Image Modeling单位：中国科学院、旷视、中国科学院大学论文网址：https://arxiv.org/abs/2304.05919论文代码： https://github.com/Haochen-Wang409/HPM首次发布时间：2023 年 4 月 12 日（收录于 CVPR 2023） 123456@article&#123;wang2023hard, title=&#123;Hard Patches Mining for Masked Image Modeling&#125;, author=&#123;Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang&#125;, journal=&#123;arXiv preprint arXiv:2304.05919&#125;, year=&#123;2023&#125;&#125; 前言“Learning Where to Mask”目前是 MIM 中改进掩蔽策略的主流方向之一，它分为人工选定和自动生成。 人工选定：事先就决定好掩蔽策略，例如： PixMIM 认为，掩蔽较多的前景 patches 会阻碍模型有效捕获形状和语义先验的能力，从而限制了表征的质量，因此 PixMIM 使用了可以保留最大的前景覆盖率的 Simple Resized Crop + 75% 掩蔽策略。 自动生成：由模型训练来得到合适的掩蔽策略，例如： 而 AutoMAE 认为，掩蔽策略是一个 trade-off 的问题，前景掩蔽率越高，通过掩蔽图像建模获得的信息越多，但同时解码器的重建难度也越高。对此，AutoMAE 将生成对抗网络引入 MIM 来搭建一个完全可微的框架。 而本文介绍的 HPM 从改进掩蔽策略的角度，引入蒸馏概念来自动生成合适的掩蔽策略。 原文作者对本文的介绍： 主要内容 传统的 MIM 方法通常需要先人工给定掩蔽策略，比如 MAE 的随机掩蔽，BeiT 的逐块掩蔽等，目的是构造具有挑战性的重建任务。 这个过程我们可以认为是在训练学生（即模型）解决给定任务（即预测掩蔽 patches），如图 1(a) 所示。 但如果预先给定了掩蔽策略，就无法控制重建任务的困难程度。 为此，本文提出了 Hard Patches Mining（HPM），一个全新的 MIM 框架，通过让模型学习自己“出题”的能力，使其可以同时站在学生和教师的立场上，被迫对图像内容有更全面的理解，从而通过生成更理想的重建任务来引导自己，如图 1(b) 所示。 具体来说，就是将 MIM 中人工给定掩蔽策略改为自动生成掩蔽策略，而生成的原则是让重建任务更具备挑战性（困难）。 那么，如何让模型自己选取需要掩蔽的 patches，以使得重建任务更困难呢？ 我们可以用重建损失来量化重建任务的困难程度，一般来说，重建损失越大，表示重建任务越困难，则对应的掩蔽策略就是我们需要的。 因此，作者引入了一个辅助的损失 predictor，用以预测每个 patch 的重建损失。并提出了一个 Easy-to-Hard 的掩蔽策略生成方法。 图1. 传统的 MIM 预训练范式与本文提出的 HPM 之间的比较。(a) 传统的方法可以理解为训练学生（即模型）解决给定任务（即预测掩蔽 patches）；b) 本文提出的 HPM 预训练范式使该模型既是教师又是学生，通过自适应调整掩蔽策略来产生一个具有挑战性的重建任务。2. 分析1600 个 epoch 预训练的 MAE 作为 MIM 模型，使用超过 10 种不同掩蔽策略产生的平均 patch 级重建损失来可视化每个 patch 重建的困难程度，如图 2 每组的前两张图片所示。 我们可以发现，通常图像的前景区域更难重建（拥有更大的重建损失）。 作者提出一个重建损失预测器，用以预测每个 patch 的重建损失。如图 2 每组的后两张图片，使用 200 个 epoch 预训练过的 ViT-B 来验证该预测器的有效性。 可以看出，具有较大预测损失的 patches 往往是有判别性的（前景几乎被屏蔽），因此屏蔽这些 patches 会使得重建任务更具有挑战性。 图2. 验证实验。每组第一张：输入图像；第二张：超过 10 种不同掩蔽策略的平均 Patch 级重建损失可视化；第三张：重建损失预测器预测的重建损失可视化；第四张：根据预测的重建损失选择掩蔽的 patches（例如，预测的重建损失达到 top-75% 的 patches 被掩蔽）3. 方法（参考原文作者对本文的介绍）HPM 由一个学生模型（ f_{θ_s} 、 d_{\\phi_s} 和 d_{ψ_s} ）和一个教师模型（ f_{θ_t} 、 d_{\\phi_t} 和 d_{ψ_t} ）组成。两个模型使用相同的网络结构。 其中 f_θ(\\cdot) 、 f_\\phi(\\cdot) 和 d_ψ(\\cdot) 分别是编码器、解码器和预测器。 为了使得结果尽可能一致，教师模型的参数将由学生模型指数平滑更新而来。 在每次预训练迭代中，将划分成 patches 的图像 x \\in \\mathbb R^{H×(P^2C)} 输入至教师网络，得到每个 patch 预测的重建损失 \\hat {\\mathcal L}^t&#x3D;d_{\\psi_t}(f_{\\theta_t}(x)) 。 基于 \\hat {\\mathcal L}^t ，使用 Easy-to-Hard 的掩蔽策略生成方法来产生掩蔽图像 M\\in \\left{0,1\\right}^N ，用于学生模型的重建任务。 学生模型的损失函数为真实重建任务和预测任务的损失之和： \\mathcal L &#x3D; \\mathcal L_{rec} +\\mathcal L_{pred} 其中， \\mathcal L_{rec} 就是传统的 MIM 重建损失： \\mathcal L_{rec}&#x3D;\\mathcal M(d_{\\phi_s}(f_{\\theta_s}(x\\odot M)),(x\\odot(1-M)))) \\mathcal M(\\cdot,\\cdot) 表示某种度量函数。 图3. HPM 框架 3.1 重建损失预测器为了使得重建损失预测器得到的损失预测值，与真实的重建损失尽可能一致，一种简单的方法就是，直接最小化真实重建损失 \\mathcal{L}_{\\mathrm{rec}} 和损失预测值 \\hat{\\mathcal L}&#x3D;d_{\\psi_s}(f_{\\theta_s} (\\mathbf{x} \\odot \\mathbf{M})) 之间的均方误差（MSE），即： \\mathcal{L}_{\\mathrm{pred}} &#x3D; \\left( d_{\\psi_s}(f_{\\theta_s} (\\mathbf{x} \\odot \\mathbf{M})) - \\mathcal{L}_{\\mathrm{rec}} \\right)^2 \\odot (1 - \\mathbf{M}) 这里的 \\mathcal{L}_{\\mathrm{rec}} 梯度已停止更新，作为预测损失 \\hat{\\mathcal L} 的金标准。 然而，我们的目标是确定图像中的困难 patches，因此我们需要学习 patches 之间的相对关系。而 MSE 关注的是预测损失值和真实损失值之间的绝对差异。在训练过程中，真实重建损失 \\mathcal{L}_{\\mathrm{rec}} 会随着训练的进行而减少，损失预测器可能更关注于损失值的准确性，而不是提取不同patches之间的相对困难程度。 为此，作者提出了一种基于二元交叉熵的相对损失作为替代方案。 具体来说，给定一张含有 N 个 patch 的图片，其真实的重建损失为 \\mathcal{L}_{\\mathrm{rec}} \\in \\mathbb{R}^N，我们的目的是预测这 N 个 patch 之间重建损失的相对大小，即 \\texttt{argsort}(\\mathcal{L}_{\\mathrm{rec}}) 。然而， \\texttt{argsort}(\\cdot) 并不可导，作者将问题转化为了预测两两 patches 之间的大小关系。 \\begin{aligned} \\mathcal{L}_{\\mathrm{pred}} &#x3D; -\\sum_{i&#x3D;1}^N \\sum_{j&#x3D;1 \\atop j\\neq i}^N \\mathbb{I}^{+}_{ij} \\log \\left( \\sigma(\\hat{\\mathcal{L}}^s_i - \\hat{\\mathcal{L}}^s_j) \\right) -\\sum_{i&#x3D;1}^N \\sum_{j&#x3D;1 \\atop j\\neq i}^N \\mathbb{I}^{-}_{ij} \\log \\left( 1 - \\sigma(\\hat{\\mathcal{L}}^s_i - \\hat{\\mathcal{L}}^s_j) \\right), \\end{aligned} 其中 \\hat{\\mathcal{L}}^s &#x3D; d_{\\psi_s}(f_{\\theta_s}(\\mathbf{x} \\odot \\mathbf{M})) \\in \\mathbb{R}^N 表示的是学生模型输出的损失预测值，而 i, j&#x3D;1,2,\\dots,N 是 patch indexes。 \\sigma(\\cdot) 是 \\texttt{sigmoid} 函数，即 \\sigma(z) &#x3D; e^z &#x2F; (e^z + 1) 。 \\mathbb{I}^{+}_{ij} 和 \\mathbb{I}^{-}_{ij} 是两个指示函数，表示 patch i 和 patch j 的真实重建损失大小，定义如下： \\mathbb{I}^{+}_{ij} &#x3D; \\left{ \\begin{aligned} &amp;1, &amp;&amp;\\mathcal{L}_{\\mathrm{rec}}(i) &gt; \\mathcal{L}_{\\mathrm{rec}}(j) \\mathrm{\\ and\\ } \\mathbf{M}_i&#x3D;\\mathbf{M}_j&#x3D;0, \\ &amp;0, &amp;&amp;\\mathrm{otherwise}, \\end{aligned} \\right. \\mathbb{I}^{-}_{ij} &#x3D; \\left{ \\begin{aligned} &amp;1, &amp;&amp;\\mathcal{L}_{\\mathrm{rec}}(i) &lt; \\mathcal{L}_{\\mathrm{rec}}(j) \\mathrm{\\ and\\ } \\mathbf{M}_i&#x3D;\\mathbf{M}_j&#x3D;0, \\ &amp;0, &amp;&amp;\\mathrm{otherwise}, \\end{aligned} \\right. 其中 \\mathbf{M}_i&#x3D;\\mathbf{M}_j&#x3D;0 表示的是 patch i 和 patch j 都应当是被掩蔽的。 3.2 Easy-to-Hard 掩蔽策略生成方法最简单的方式就是图 2 介绍的，直接将预测的重建损失 \\texttt{argsort}(\\hat{\\mathcal L}^t)&#x3D;\\texttt{argsort}[d_{\\psi_t}(f_{\\theta_t} (\\mathbf{x}))] 达到 top-75% 的 patches 掩蔽掉。 然而，在早期训练阶段，学习到的特征表示容易被丰富的纹理所淹没，这意味着，此时，大的预测重建损失不等同于该 patch 就难以判别。 为此，作者提出了一种 Easy-to-Hard的掩蔽策略生成方法，提供一些合理的提示，引导模型逐步重建困难 patches。 具体来说，对于第 t 轮 epoch，由 \\texttt{argsort}(\\hat{\\mathcal L}^t) 决定的 \\alpha_t \\cdot \\gamma N 个 patches 被掩蔽掉，然后剩余的 1-\\alpha_t \\cdot \\gamma N 个 patches 被保留， \\gamma 表示 mask radio， N 表示 patches 总数。而 \\alpha_t 被定义为： \\alpha_t&#x3D;\\alpha_0+\\frac{t}{T}(\\alpha_T-\\alpha_0) 其中， T 表示 epoch 的总轮数， \\alpha_0,\\alpha_T\\in[0,1] 是可调的超参数。 也就是说， \\alpha_t 随着 epoch 轮数的增加，会以线性方式从 \\alpha_0 增加到 \\alpha_T 。 这就相当于，随着 epoch 轮数的增加，重建任务由简单到困难。 整个训练过程的伪代码如图 4 所示： 图4. HPM 预训练伪代码4. 结果表1. 对不同重建目标的消融实验。表2. 不同掩蔽策略的消融实验。研究了不同的 α_0、α_T 和 γ 的影响。α_T 较大表示重建任务难度越大。表3. 不同掩蔽策略的消融研究。研究了 argmax(·) 对重建损失预测值和“easy-to-hard”方式的有效性。argmin(·) 表示掩蔽简单 patches。表4. 损失预测公式的消融实验。表5. 在下游任务上的性能表6. 与 ImageNet-1K 分类任务上 SOTA 模型的比较。表7. 与 ADE20k 语义分割任务上 SOTA 模型的比较。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | FreMAE：基于傅里叶变换的 MIM 用于医学图像分割","slug":"Masked_Image_Model/MIM-FreMAE基于傅里叶变换的-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-FreMAE基于傅里叶变换的-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-FreMAE%E5%9F%BA%E4%BA%8E%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E7%9A%84-Glenn/","excerpt":"","text":"题目： FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation单位：北京科技大学、中佛罗里达大学、伯明翰大学、理海大学论文网址：https://arxiv.org/abs/2304.10864论文代码：未公开首次发布时间：2023 年 4 月 21 日 123456@article&#123;wang2023fremae, title=&#123;FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation&#125;, author=&#123;Wang, Wenxuan and Wang, Jing and Chen, Chen and Jiao, Jianbo and Sun, Lichao and Cai, Yuanxiu and Song, Shanshan and Li, Jiangyun&#125;, journal=&#123;arXiv preprint arXiv:2304.10864&#125;, year=&#123;2023&#125;&#125; 主要内容 目前 MIM 在医学领域应用的缺陷： 在某种程度上，MAE 仅将原始像素作为重建目标，主要依赖于局部特征表示，而没有充分利用全局信息。 由于该模型只有最后一个 block 的输出被送到解码器中用于重建任务，缺乏来自其他阶段的监督来提供多尺度信息。 由于高采集成本和患者隐私，通常小规模医学图像数据集的训练样本相对有限，需要行量身定制的设计。 如何解决？ 在自然图像中，详细的纹理信息主要存在于高频分量中，低频分量携带丰富的全局信息，如图 1 所示，该原则同样也可以应用于医学领域。 图1. 将相应的高&#x2F;低通滤波器应用于傅里叶频谱，分别获得傅里叶频谱、高频分量和低频分量的可视化基于此原则，作者提出了一种新的基于 MIM 框架，即FreMAE，用于解决上述问题。 FreMAE 首先屏蔽掉一部分随机选择的图像像素，然后在傅立叶域中预测输入图像的相应缺失频谱。由于同一器官的医学图像本质上对应于相似的特征，因此 FreMAE 使用困难的跨域重建任务，以避免使用捷径进行模型学习。 利用所提出的双边聚合解码器对原始图像依次应用傅立叶变换，并对转换后的傅立叶频谱依次应用低&#x2F;高通滤波器，以获得预期的重建目标。这种多阶段监督方法可以更好地指导模型预训练，从而产生更好的分割表示。 提出了一种有效的前景掩蔽策略作为原始随机掩蔽的替代方案，该策略被证明更适合于医学图像分割的纹理和细节建模。 方法 前景掩蔽策略 与自然图像不同，医学图像中前景和背景分布极不平衡。因此，随机掩蔽策略将不可避免地导致生成的掩模主要覆盖医学图像的背景像素，并且保留了太多对象的前景像素。为此，需要提出一种简单而有效的前景掩蔽策略来解决这一不均匀分布问题。 由于医学图像通常包含不同的通道（医学图像通常是灰度图像，这里的意思应该是不同的切片数&#x2F;深度），每个通道拥有不同的前景区域，因此取这些通道的重叠部分作为最终的遮罩区域。总体的前景遮罩策略可以定义为： 对于每个通道，像素值为 0 的表示背景，不为 0 的表示前景； 将所有通道的二值遮罩进行逐像素的逻辑或操作，得到最终的前景遮罩。 这种方法能够在多通道医学图像中有效地识别前景区域，帮助进行自监督预训练。 通用编码器 正如在第一节中强调的，图像的高层次和低层次信息分布在傅立叶谱的不同频率带中。因此，建议分别利用低通和高通傅立叶谱作为监督信号（即重建目标）。最直观的方法是使用相同的高通傅立叶谱直接监督多个低级阶段，反之亦然。然而，这种直观的方式主要存在两个缺点： 不合理性：这种方法违反了模型在不同低级阶段学习的原始意图，因为在不同低级阶段学到的特征表示应该是不同的，而不是相同的。 过于简单：这种监督方法过于直接且简单，并没有充分利用分层结构捕获的多阶段特征之间的相关性来帮助模型更好地执行 MIM 务。 为了克服这些缺点，可以采用以下方法： 对于低级和高级阶段，分别设计不同的监督信号，以便在不同阶段学习不同的特征表示。例如，在低级阶段，可以关注低通傅立叶谱中的结构信息；而在高级阶段，可以关注高通傅立叶谱中的纹理和细节信息。 利用分层结构捕获的多阶段特征之间的相关性来设计更复杂的监督方法。例如，可以通过在不同阶段之间共享信息或融合多阶段特征来帮助模型更好地执行 MIM 任务。这将充分利用嵌套结构在各层之间捕获的信息，有助于提高模型性能。 具体来说，在所提出的双向聚合解码器（bilateral aggregation decoder, BAD）内部，不同阶段的编码特征分别以自底向上和自顶向下的方式汇聚到最低阶段（即具有最大空间分辨率）和最高阶段（即具有最小空间分辨率）。换句话说，双向聚合解码器将不同阶段的特征图分别聚合到最低和最高分辨率。 对于 ViT，第 4、8、12 层的特征图采用类似 UNETR 中的反卷积模块分别上采样 8、4、2 倍，然后输入到 BAD。每个相邻阶段捕获的特征将输入到卷积块中，以实现空间分辨率和通道维度的严格对齐。 接下来，高阶和低阶的聚合特征表示将通过引入的频率映射块（FMB）映射到频域，然后通过低通和高通滤波器得到相应的高通和低通预测频谱，用于计算重建损失。 具体来说，FMB 由二维离散傅立叶变换（2D-DFT）、频域感知器（Frequency Domain Perceptron, FDP）和二维离散傅立叶逆变换（2D-IDFT）组成，可以表示为： 计算高阶和低阶聚合特征表示的 2D-DFT，将其映射到频域。 将映射到频域的特征表示输入到 FDP 中，进一步处理和提取有用信息。FDP 是一个包含多个卷积层和激活函数的神经网络模块，用于在频域中学习特征。 计算 FDP 输出的 2D-IDFT，将其映射回空域。 经过这个过程，得到了低阶和高阶的聚合特征表示的频率谱。 图2. FreMAE 架构频率损失 为了缓解不同频段频谱之间的权重不平衡，便于困难频段的重建，采用聚焦 focal frequency loss 作为损耗函数 \\mathcal L_{freq} ，实现低频映射和高频映射权重的梯度更新，定义为： \\mathcal{L}_{freq}&#x3D;\\frac{1}{HW}\\sum^{H-1}_{u&#x3D;0}\\sum^{W-1}_{v&#x3D;0}w(u,v)\\odot\\gamma(f(u,v),\\hat{f}(u,v))^2 其中， f(u,v) 表示在空间频率坐标 (u, v) 上经过 2D-DFT 后的预测值， \\hat{f}(u,v) 表示对应的金标准， γ(f, \\hat{f}) 计算实际值和预测值之间的欧几里得距离的平方，作为它们的频率距离。 ω 是给定位置的频谱权重矩阵，用于抑制容易频率的权重。具体计算公式如下： ω(u, v) &#x3D; γ(f(u, v), \\hat{f}(u, v))^β γ(f, \\hat{f}) &#x3D; \\sqrt{ (R − \\tilde R)^2 + (I − \\tilde I)^2} β 是缩放因子（默认为 β&#x3D;1 ）。 总损失 \\mathcal L&#x3D;\\mathcal L_{freq}(F_H(P_{low}), F_H(T))+\\alpha\\mathcal L_{freq}(F_L(P_{high}),F_L(T)) 其中 F_H 和 F_L 分别表示高通和低通频率滤波器。 T 表示原始图像。 P_{low} 是通过最高阶段获得的， P_{high} 是相反的。 α 是高级语义信息分支的权重（默认情况下为 α&#x3D;3 ）。 结果 图3. 通过 FreMAE 在频域内的重建结果的可视化。表1. 自监督学习框架比较。‘-’代表从头开始的训练。表2. 在 BraTS 2019、ISIC 2018 和 ACDC 2017 数据集的性能比较。表3. 关于重建目标和监督方案的消融研究。表4. 关于掩蔽策略的消融研究。表5. 关于掩蔽比的消融研究。表6. 自监督预训练样本数量的消融研究。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | EMAE：解决 MIM 数据利用率低和预测结果不一致性问题","slug":"Masked_Image_Model/MIM-EMAE解决-MIM-数据利用-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-EMAE解决-MIM-数据利用-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-EMAE%E8%A7%A3%E5%86%B3-MIM-%E6%95%B0%E6%8D%AE%E5%88%A9%E7%94%A8-Glenn/","excerpt":"","text":"题目：Effificient Masked Autoencoders with Self-Consistency单位：中国科学院、中国科学院大学、商汤、鹏城实验室论文网址：https://arxiv.org/abs/2302.14431论文代码：未公开首次发布时间：2023 年 2 月 28 日 主要内容 MIM 的高随机掩蔽比将导致两个严重问题： 数据未被有效利用，导致预训练效率低； 例如，在每个 epoch 中 MAE 仅利用整个图像的 25% 训练模型。相比之下，BERT 使用了 85% 的文本语料库。由于 MIM 的数据利用率不足，预训练 epoch 通常高于 MLM（masked language modeling），MAE 的 1600 个 epoch 相当于 MLM 的 40 个 epoch。此外，对于 MAE 需要 1600 个 epoch，而对于有监督学习只需要 300 个epoch。 预训练模型的不确定性和不一致性高，即同一块区域在不同 epoch 的随机掩蔽策略下，产生的预测可能不一致，如图 1 所示。 图1. 不同的随机掩蔽策略对应不同的 MAE 重建结果。(a) 由不同的随机掩蔽策略采样的掩蔽图像。(b) 对应的 MAE 重建结果。对于 (b) 的三个重建，只有第一个代表一只正常的牛，第三个甚至重建了一只狗。可见 MAE 重建的语义是不一致的。为了解决这些问题，作者提出了一种具有自一致性的高效掩码自动编码器（EMAE），主要从两方面进行改进： 1）将图像逐步分成 K 个不重叠的部分，每个部分由掩蔽策略随机生成，具有相同的掩蔽比。然后，在每个 epoch 中，所有部分并行进行 MIM 任务并生成预测。 2）设计自一致性模块，以进一步维护部分之间重叠掩码块的预测一致性。 ImageNet 上的实验表明，与 MAE（1600 个 epoch）相比，EMAE 在 ViT Base 下仅用 300 个预训练epoch 就获得了更高的结果。EMAE 在各种下游任务（如对象检测和语义分割）上也始终获得最先进的传输性能。 方法 图2. EMAE 模型结构。整个图像首先被逐渐划分为 K 个不重叠的部分，并且每个部分具有相同数量的可见 patches。然后，每个部分被送入编码器-解码器架构，并行执行 MIM 任务。此外，自一致性模块指导预测部分之间的重叠的 patches 被拉在一起。 2.1 有效利用全部数据为了提高数据的利用率，EMAE 将使用全部数据训练模型。 具体来说，就是要将输入图像 x 划分为 K 个具有相同数量且不重叠的可见 patches 的部分，然后所有部分并行执行 MIM 任务。 首先将整个图像分成 N 个图像 patches。然后，设置一个长度为 N 的随机张量 t ，其值在 [0,1] 上均匀分布。张量按值按升序排序，排序后的索引 ids 用如下表示： ids&#x3D;s(t) 其中 s(\\cdot) 表示按值按升序排序。 将长度为 N 的排序索引等分划分为 K 个部分 ids_1，ids_2，…， 和 ids_K ： ids_i&#x3D;ids[(i-1)×\\frac{N}{K}: i×\\frac{N}{K}] 其中 i\\in\\left{1,2，…，K\\right} 。 因此， N 个图像 patches 可以根据索引 ids_i 等分为 K 个部分 x_{1}，x_{2}，…， 和 x_{K} ，且每个部分具有相同数量且不重叠的可见 patches x_{v_i} ： x_{v_i}&#x3D;d(x_i, ids_i) 其中 d(\\cdot) 表示根据索引 ids_i 从输入 x_i 中查找可见 patches。 对应的，掩蔽 patches 为： x_{m_i}&#x3D;d(x_i, ids-ids_i) 此时，任一部分 x_{i} 都有 \\frac{N}{K} 个可见 patches， N-\\frac{N}{K} 个掩蔽 patches x_{m_i}。 则任一部分的掩蔽比都为 (N−\\frac{N}{K})&#x2F;N &#x3D;(K−1)&#x2F;K ，算法伪代码如图 3 所示。 图3. 算法伪代码根据上述描述，当 K 设置为 4 时，各部分的掩蔽比为 75%（与 MAE 的掩蔽比相同）。 该设计保证了迭代中图像的每个 patches 都可以采样一次，使得整个图像都可以应用于模型的训练，从而提高了数据的利用率。 损失函数定义如下： \\mathcal{L}_{whole}(x)&#x3D;\\mathbb E_{i\\in[1,K]}\\mathcal{L}(x_{v_i},x_{m_i})&#x3D;\\mathbb E_{i\\in[1,K]}||g(f(x_{v_i})) − x_{m_i}||^2 其中， g(\\cdot) 表示解码器。 2.2 自一致性模块进一步地，需要鼓励预训练模型的预测在来自同一图像的不同输入可见 patches 下是一致的。 根据 2.1 节介绍的内容，每个部分 x_{i} 具有 N-\\frac{N}{K} 个掩蔽 patches x_{m_i} ，对应的，都会生成 N-\\frac{N}{K} 个预测 x_{p_i} 。 显然，任何两个部分的预测之间都存在一定的重叠 patches，比例为 \\frac{K-2}{K-1} 。 重叠位置 s_{ij} 可以通过掩蔽 patches x_{m_i} 和 x_{m_j} 得到，其中 i, j \\in \\left{1,2,3,…,K\\right} 且 i \\neq j ： s_{ij}&#x3D;x_{m_i}\\cap x_{m_j} EMAE 的自一致性模块用来指导每个重叠位置的预测保持一致。如图 2 所示，自一致性模块将任意两个预测 x_{p_i} 和 x_{p_j} 之间的重叠预测拉到一起，这使重叠重建结果之间的平均绝对误差最小化，以提高一致性。 损失函数定义如下： \\mathcal{L}_{consistency}(x)&#x3D;\\mathbb{E}_{i\\in [1,K],j\\in[i+1,K]}\\mathcal{L}_{sc}(x_{v_i}, x_{v_j})&#x3D;\\mathbb{E}_{i\\in [1,K],j\\in[i+1,K]}(||sg[x_{p_i}]-x_{p_j}||+||x_{p_i}-sg[x_{p_j}]||)\\odot s_{ij} 其中 sg[·] 表示停止梯度。对于任一部分的每次预测，将与其他部分的预测进行 K−2 次计算。 总损失为： \\mathcal{L}_{total}(x)&#x3D;\\mathcal{L}_{whole}(x) + \\mathcal{L}_{consistency}(x) 结果 由表 1 可以得出： 在线性探测中，相同的预训练 epoch（300 和 800）下，ViT-base 的 EMAE 比 MAE 的精度高 6.0%～6.7%。 ViT-base 的 EMAE epoch 300 的分类结果与 MAE 2400 epoch 的分类结果相当。（GPU-days？） ViT-large 的 EMAE epoch 800 的分类结果与 ViT-huge 的 MAE 1600 epoch 的分类结果相当。 由于基于对比学习的方法具有图像语义一致性的假设，并且该假设与线性探测任务的先验一致，因此 EMAE 的线性结果略低于基于对比学习的算法。 表1. SOTA 自监督学习方法在 ImageNet top-1 精度比较。LP：线性探测；FT：端到端微调。表 2 、3 、4 可以得出，EMAE 的性能优于之前的 SOTA 自监督学习方法。 表2. 使用带 FPNs 的 Mask R-CNN 对 COCO 上的目标检测和实例分割结果进行微调。表3. 使用 ViTDet 对 COCO 上的目标检测和实例分割结果进行微调。表4. 使用 UperNet 对 ADE20K 上的语义分割结果进行微调。表 5、6 反映了 EMAE 中全数据利用、自一致性模块以及 K 值取值的影响。图 4 是自一致性模块有效性的可视化证明。 表5. 消融实验。全数据利用和自一致性模块的影响。表6. 消融实验。K 值取值的影响。图4. 不同的随机掩蔽策略对应不同的 EMAE 重建结果。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | DPPMask：可以保留具有代表性 patches 的掩蔽策略","slug":"Masked_Image_Model/MIM-DPPMask可以保留具有代表-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-DPPMask可以保留具有代表-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-DPPMask%E5%8F%AF%E4%BB%A5%E4%BF%9D%E7%95%99%E5%85%B7%E6%9C%89%E4%BB%A3%E8%A1%A8-Glenn/","excerpt":"","text":"题目：DPPMask: Masked Image Modeling with Determinantal Point Processes单位：中国科学院深圳先进技术研究院、中国科学院大学、之江实验室、香港中文大学论文网址：https://arxiv.org/abs/2303.12736论文代码：未公开首次发布时间：2023 年 3 月 13 日（2023 年 3 月 25 日 v2 版本上传） 123456@article&#123;xu2023dppmask, title=&#123;DPPMask: Masked Image Modeling with Determinantal Point Processes&#125;, author=&#123;Xu, Junde and Lin, Zikai and Zhou, Donghao and Yang, Yaodong and Liao, Xiangyun and Wu, Bian and Chen, Guangyong and Heng, Pheng-ann&#125;, journal=&#123;arXiv preprint arXiv:2303.12736&#125;, year=&#123;2023&#125;&#125; 前言掩蔽策略被证明是 MIM 中较为重要的部分。PixMIM 使用 Simple Resized Crop 代替原始 MAE 使用的 Random Resized Crop，为了保留更多的前景 patch；AutoMAE 更是将生成对抗网络用于优化掩蔽策略。DPPMask 基于行列式点过程 (determinantal point process, DPP)，提出了 DPPMask 的 MIM 掩蔽策略，可以保留更具有代表性 patches。 PixMIM 解读看这里： AutoMAE 解读看这里： 主要内容 在实际情况中，强制模型重建无法恢复的内容是不合理的。 考虑一个简单的情况，如图 1 所示。 图 1 的顶行显示了 MIM 的基本逻辑：成功的重建意味着网络捕捉了正确的语义特征。 而图 1 的底行则显示了一个失败的情况：如果遮蔽过程恰好遮蔽了原始图像的一个重要语义，如图 1 中的书，则会改变原始图像的语义并使网络难以从其余部分中恢复它。 在这种情况下，如果继续强制模型重建原始图像，则模型可能会填充遮挡的图像，干扰学习原始特征的过程。此外，随着遮挡率的增加，原始语义信息被扭曲的概率也会增加。 作者将这种情况称为 misalignment problem，即遮蔽图像的语义与原始图像的语义不匹配。因此，misalignment problem 将导致不恰当样本对的对齐，最终会损害下游任务的性能。 图1. MIM 中的 misalignment problem 的说明。在对比学习中，InfoMin 原则（What Makes for Good Views for Contrastive Learning?）表明：一个图像的两个增强视图应该保留与任务相关的信息，同时最小化不相关的细微干扰。 作者认为 MIM 中也应该拥有如下原则： 所选的可见 patches 应该具有足够的代表性，以覆盖原始图像的整个语义信息。 掩蔽比应设置为较高水平，以最小化同一图像的不同 masks patches 之间的无关信息共享。 对此，作者基于行列式点过程 (determinantal point process, DPP)，提出了 DPPMask 的 MIM 掩蔽策略。 DPP 是一个集合概率模型，它可以从一个底层集合中采样出具有多样性和高质量的子集。在采样过程中，DPP 会计算每个 patch 的距离，并选择与已选子集不相似的 patch。这个过程使得网络集中于具有更多代表性信息的 patch。 misalignment problem 分析 图2. (a) 输入图像；(b) 某一个 epoch&#x2F;step 选择的掩蔽图像。MIM 的训练过程中，每个 epoch&#x2F;step 会随机选择不同的掩蔽图像 ，如图 3 Input Space 所示，橙色、绿色、蓝色虚线框分别表示不同 epoch&#x2F;step 选择的掩蔽图像，通过训练网络利用这些掩蔽图像来重建原始图像。 在重建过程中，网络会根据每个 epoch&#x2F;step 选择的掩蔽图像将学习到的信息构建一个语义空间。如图 3 Semantic Space 所示，假设网络可以根据绿色虚线框选择的掩蔽图像学习到原始图像的语义，则重建的目标可以定为使得网络从不同掩蔽图像学习到的信息尽可能的接近原始图像语义，这是通过最小化语义空间中每个数据点之间的距离来实现的。当错误地聚合了语义偏差的数据点（图 3 中的橙色点）时，就会出现未对齐的情况，从而导致语义空间不对齐。 在实际情况中，语义在图像中的分布并不均匀。这些语义很可能因为随机掩蔽策略而被忽略。当 MIM 将不同的掩蔽图像拉到一起时，具有不同语义的两个掩蔽图像会错位。 如果错位的语义是理解图像的重要线索，那么可能会严重影响下游任务性能。为此，需要一种新的掩蔽采样策略，以保留更具有代表性的 patches。 图3. misalignment problem 的说明 3. 方法作者基于行列式点过程 (determinantal point process, DPP)，提出了 DPPMask 的 MIM 掩蔽策略。 DPP 的介绍如下： 对于给定的集合 Z ， Discrete Point Processes 是定义在该集合的所有子集上的一个采样概率。 而 Determinantal Point Process 将 Discrete Point Processes 问题里复杂的概率计算转换成简单的行列式计算，通过核矩阵（kernel matrix） L 的行列式计算每一个子集的采样概率。 即存在一个实数型、PSD（半正定）矩阵 L ，对于集合 Z 的所有子集 Y ， Y 采样的概率与矩阵 L_Y 的行列式成正比： $$ \\mathcal{P}(Y)\\propto det(L_Y) $$ 在本文中，首先计算列表中任意两个 patches 之间的相似度 S_{ij} （先将 patches 表示成特征向量 S_i,S_j ，然后使用相似度度量方法计算相似度），得到相似度矩阵 S ，然后用矩阵 S 的行列式 det(S)&#x3D;S_{ii} × S_{jj} − S_{ij} × S_{ji} 来表示 patches 集合多样性程度（因为矩阵是一组向量的集合，而矩阵的行列式的物理意义为矩阵中的各个向量构成的平行多面体体积的平方。这些向量彼此之间越不相似，相似性度量方法计算出的结果就会越小，构成的平行多面体的体积也就越大，矩阵的行列式也就越大，对应的 patches 集合的多样性也就越高）。 DPPMask 的目的便是从候选 patches 集合 Z 中选择能够最大化后验概率的 patches 子集 Y 来保留： Y_{MAP}&#x3D;\\underset{Y\\in Z}{argmax}, det(L_Y) 该问题是一个 NP-hard 问题。因此，在实际场景中，多使用贪心算法来获取该问题的近似解。 求解过程简单来说就是，每次从候选集中贪心地选择一个能使边际收益最大的 patch 加入到最终的结果子集中，直到满足停止条件为止，即每次选择 patch j 添加到结果集 Y_g 中， Y_g 初始化为空集，patch j 需要满足下面的等式： $$ j&#x3D;\\underset{i\\in Z\\backslash Y_{g}}{argmax}\\ l o g d e t(L_{Y_{g}\\cup{i}})-l o g d e t(L_{Y_{g}}) $$ 其中行列式 det(L_{Y_g}) 直接求解复杂度较高，可以使用其他方法简化计算过程。感兴趣的可以查看博文和原论文。 在本文中，作者将核矩阵 L 设置为 L_{ij} &#x3D; exp(-\\frac{||S_i-S_j||^2}{\\epsilon}) ，其中 \\epsilon 经验值为 1。 另外，如果一味地执行贪心算法，会导致过度保留。如图 4 所示，由于天空的 patches 彼此过于相似，前景比背景更多样，此时贪心的选择将只关注前景。这种情况会使得 MIM 任务过于简单，这对特征学习没有帮助。 图4. 对三种不同清除率进行定性比较，一个合适的清除率可以保持原始图像的语义，同时保持增强输入的多样性。为了解决这一问题，作者设置了一个称为清除率 \\tau\\in (0,1) 的参数作为最大边际增益的阈值。 具体来说，在每次迭代中，都会监控下一个 patch 与所选子集的距离，如果该距离低于清除率，则贪心的选择过程将中止，子集由随机 patches 填充。清除率可以调整 DPPMask 的 “严重” 程度，当 \\tau &#x3D; 0 表示选择过程完全由贪心算法选择，而 \\tau &#x3D; 1 表示完全随机抽样。 DPPMask 的算法伪代码如图 5 所示： 图5. DPPMask 的采样算法伪代码 4. 结果通过观察表 1 、表 2 和表 3，可以得到： 在 ImageNet-100 数据集上，MAE 和 iBOT 的微调任务性能均得到了提升。 在 MAE 中，加入 DPPMask 的精度提高了0.2%； 在 iBOT 中，加入 DPPMask 的精度提高了0.4%。 随着阈值 τ 的减小，预训练损失相应地变小。然而，当 τ 过低时，贪心采样会过度保留，使任务过于简单，网络无法学习有用的特征（？）。 对于线性探测，加入 DPPMask 的 MAE 性能显著下降。这是因为 DPP 使样本空间缩小，以清除未对齐的样本。将更少的样本聚集在一起会使特征空间更连续，并且线性可分离性更低。 相比于其他采样方法，DPPMask 取得了更好的成绩。 表1. MAE+DPPMask 在 ImageNet- 100上 的详细结果。表2. iBOT+DPPMask 在 ImageNet- 100上 的详细结果。表3. 与 ImageNet-1K 上的其他采样方法的比较。观察图 4，可以得到： Mask Ratio 越大，越容易造成 misalignment 问题。作者发现对于微调任务来说，0.75 并不是一个好的 Mask Ratio 值。 加入 DPPMask 的 MAE 在任意的 Mask Ratio 上都取得了更高的性能。 图6. 不同 Mask Ratio 下在 ImageNet-100 上 MAE 的精度。观察表 4 和表 5 ，说明在 COCO 和 CLEVR 数据集的多标签分类任务上，DDPMask 也能取得较高的精度。 表4. 在 COCO 上的多标签分类精度。表5. 在 CLEVR 上的多标签分类精度。2023 年 3 月 25 日上传的 v2 版中增加了附录实验，详细结果请查看原文。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | DeepMIM：将 Deep Supervision 引入掩蔽图像建模","slug":"Masked_Image_Model/MIM-DeepMIM将-Deep-S-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-DeepMIM将-Deep-S-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-DeepMIM%E5%B0%86-Deep-S-Glenn/","excerpt":"","text":"题目：DeepMIM: Deep Supervision for Masked Image Modeling作者单位：微软亚研院、剑桥论文网址：https://arxiv.org/abs/2303.08817论文代码：https://github.com/OliverRensu/DeepMIM首次发布时间：2023 年 3 月 15 日 123456@article&#123;ren2023deepmim, title=&#123;DeepMIM: Deep Supervision for Masked Image Modeling&#125;, author=&#123;Ren, Sucheng and Wei, Fangyun and Albanie, Samuel and Zhang, Zheng and Hu, Han&#125;, journal=&#123;arXiv preprint arXiv:2303.08817&#125;, year=&#123;2023&#125;&#125; 1. 主要内容这篇论文也是在该团队 MIM 宇宙基础上提出的新架构 DeepMIM。主要内容是在 Masked Image Modeling 训练过程中加上 Deep Supervision，可以促进浅层学习更有意义的表示，加快模型收敛速度并扩大注意力的多样性。 Deep Supervision（即在神经网络的中间层引入额外的监督）在早期的深度学习中，Deep Supervision 可以解决深度神经网络训练梯度消失和收敛速度过慢的问题， 而被广泛应用于 CV 领域，随着 normalization 和 residual connection 的出现，Deep Supervision 逐渐被淘汰。 Deep supervision 解读看这里： 2. 方法DeepMIM 采用编码器-多解码器架构进行 ViT 预训练的掩模和预测任务。具有12个Transformer 块的 ViT-B 作为编码器，4个独立的具有 4 层 Transformer 块的解码器分别置于编码器的第 6、8、10、12 个 Transformer 块后。 在 Masked Image Modeling 上构建这种架构的难度主要在于，如何从原始输入中提取监督信号来指导中间层的学习。浅层 ViT 产生的特征区别较差，这些中间特征可能没有能力重构过于复杂的目标。 对此，DeepMIM 提出可选择的 Hybrid Target Generator 模块，将预训练好的 MAE 产生的模糊重建结果与原始像素按比例混合后作为中间层的监督信号。 t &#x3D; αx + (1 − α)\\hat{x}. t_i 表示解码器 g^i_ξ 的重建目标。对于 g^1_ξ 、 g^2_ξ 和 g^3_ξ ，将 α 分别设置为 0、1&#x2F;3 和 2&#x2F;3。 尽管使用 Hybrid Target 作为中间层监督信号可以提高微调性能，但存在额外的计算开销。所以作者建议，只有在有一个现成的一个预训练过的 MIM 模型时，才使用 Hybrid Target。 不使用 Hybrid Target，所有解码器设置为 α &#x3D; 1 。 总损失是由 N 个额外解码器和 1 个主解码器产生的 N+1 个 \\mathscr{L}_2 重建损失之和： \\mathcal{L} &#x3D; \\sum_{i&#x3D;1}^N||M(t_i)-p_i||^2_2+||M(x)-p||^2_2 其中， M(\\cdot) 表示提取目标 masked patches 的操作，N 为额外解码器的数量。 DeepMIM 也可以应用于一系列具有不同重建目标的 MIM 模型。 图1 DeepMIM 网络框架 这篇论文让我想到了 CVPR23 的 LocalMIM（Masked Image Modeling with Local Multi-Scale Reconstruction）。这两篇应该是同时期论文，DeepMIM 在 arxiv 上提交的时间是 2023 年 3 月 15 ，LocalMIM 提交的时间是 2023 年3 月 9 日，从格式上看，猜测应该是在 CVPR 上撞车了)。 LocalMIM 同样是将 Deep Supervision 引入到了 Masked Image Modeling 中，但他是从多尺度重建的角度讲故事。 DeepMIM 主解码器和中间层解码器的监督信号在不使用 Hybrid Target 情况下是一致的。而 LocalMIM 不同层解码器的监督信号是不同的，使得较低层重构细尺度的监督信号，而较高层重构粗尺度的监督信号。 图2 LocalMIM 网络框架 LocalMIM 解读看这里： 讨论 DeepMIM 使用训练集和验证集损失曲线，证明在下游任务的微调期间的表现要由于 MAE。根据该团队另一篇论文（On data scaling in masked image modeling）的理论，预训练中的验证损失可以很好的指示模型在下游任务的微调期间的表现。 On data scaling in masked image modeling 解读看这里 图3 MAE 和 DeepMIM-MAE 的训练损失（左）和验证损失（右）的比较。只显示了 DeepMIM-MAE 的最后一层的重建损失 DeepMIM 使用 centered kernel alignment（CKA）来识别最后一层产生的特征和中间层产生的特征之间的对应关系。从第一层到倒数第二层，DeepMIM-MAE 的 CKA 得分总是超过 MAE，说明DeepMIM-MAE 的中间层的特征更具鉴别性。 图4 使用 CKA 评估来自最后一层的特征和来自中间层的特征之间的对应关系。蓝色：MAE；红色：DeepMIM-MAE DeepMIM 使用 CKA 来计算来自 MAE 第 8 层的特征与来自 DeepMIM-MAE 所有层的特征之间的相似性，MAE 的中间层（第8层）特征与 DeepMIM-MAE 的较浅的层特征（第3层和第4层）具有最大的对齐。相比之下，DeepMIM-MAE 的中间（第8层）特征与 MAE 的更深块（第层和第10层）的特征更加紧密。本研究表明，DeepMIM显著增强了特征对浅层的鉴别能力。 图5 由 CKA 评估的交叉特征相似性 DeepMIM 计算不同 attention heads 之间的余弦相似性来探索 head 的多样性。根据该团队另一篇论文（Revealing the dark secrets of masked image modeling）的说法，更多样化的 head 表明有更强的代表能力。研究表明，与MAE相比，DeepMIM 产生的 head 更多样化。 Revealing the dark secrets of masked image modeling 解读看这里： 图6 比较 MAE（左）和 DeepMIM-MAE（右）在不同层上的 head 余弦相似度 为了进一步评估来自浅块的特征的质量，DeepMIM 冻结了浅层的一个子集，并对其余的层进行微调。当可训练块的数量从 1 个（只有最后一个层是可训练的）变化到12个（所有层都是可训练的）时，DeepMIM-MAE 的性能始终显著优于 MAE。 图7 当将可训练层的数量从 1 改变到 12 时，DeepMIM-MAE 的性能始终明显优于 MAE DeepMIM 随机初始化预训练的 ViT-B 的最后 K 个层，然后以端到端的方式在 ImageNet 上对 ViT-B 进行微调。DeepMIM-MAE 在每种情况下都始终优于 MAE，这表明浅层的良好表示有利于更深层的学习，尤其是当它们被随机初始化时。 图8 在 ImageNet-1K 上的端到端微调","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | CCViT：基于 Patches 质心重建的 MIM 框架","slug":"Masked_Image_Model/MIM-CCViT基于-Patches-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-CCViT基于-Patches-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-CCViT%E5%9F%BA%E4%BA%8E-Patches-Glenn/","excerpt":"","text":"题目：Centroid-centered Modeling for Efficient Vision Transformer Pre-training单位：武汉大学、京东、悉尼大学论文网址：https://arxiv.org/abs/2303.04664论文代码：未公开首次发布时间：2023 年 3 月 8 日 前言陶老师挂名的一篇论文。 之前介绍的都是以像素为重建目标的 MIM 模型，另一种主流的重建目标是 tokens，比如 BEiT、iBOT 等。而 CCViT 找出了以 tokens 为重建目标的 MIM 序列模型的缺陷，并提出了一种基于 patches 质心的 tokenizer 方式，可以有效缩短 tokens 生成的时间。 DeepMIM 介绍看这里： PixMIM 介绍看这里： AutoMAE 介绍看这里： 主要内容 目前以重建目标为改进方向的 MIM 方法主要有两种类型：一种是基于像素重建的 MIM，另一种基于 token 或高级特征重建的 MIM。 然而，这两种方式都有其缺点： 首先，它们都引入了一个冗余模块来将潜在表示转换为原始像素。基于像素的 MIM（如MAE）需要一个冗余解码器。基于 token 的 MIM，如 BEiT，需要一个 tokenizer 模型来将图像像素转换为离散tokens。 此外，来自 tokenizer 的 tokens 并不能表示对应的图像 patches，因为 tokenizer 是基于整个图像生成 tokens，而不是单个 patch。因此，即使某个 patch 保持不变，如果来自其他 patches 的像素被修改，则该 patch 生成的 token 也可能被改变。 tokenizer 的训练通常需要消耗大量时间。 对此，CCViT ： 1）提出了一种 patch 质心 tokenizer，用于生成局部性 token； 2）提出了一种新的掩蔽策略：质心 patch 替换，可以结合 BEiT 的 blockwise masking； 3）提出了像素与 tokens 结合的混合重建目标。 方法 2.1 构建 patches 的质心 codebook使用 k-means 聚类方法，从 N×n 个 patches \\left{x_i^p\\right}^{N×n}_{i&#x3D;1} ，展平为向量形式\\left{x_i^v \\in \\mathbb{R}^{C×P^2}\\right}^{N×n}_{i&#x3D;1} 后，寻找其 K 个质心 \\left{\\mathcal C_k\\in\\mathbb R^{C×P^2}\\right}^K_{k&#x3D;1} ，损失函数为： $$ \\mathbb{E}(\\mathcal C_1,…,\\mathcal C_K)&#x3D;\\frac{1}{N}\\sum_{i&#x3D;1}^{N}||x_i^v-\\mathcal C_{a(i)}||2\\ a(i) &#x3D; argmin{k∈\\left{1，…，K\\right}}||x^v_i−\\mathcal C_k||_2 $$ 其中， N 是训练图像数量， n&#x3D;HW&#x2F;P^2 是单张图像 x\\in\\mathbb R^{C×H×W} 的 patches 数量。 训练完成后，可以得到一个质心 codebook \\left{\\mathcal C_k\\in\\mathbb R^{C×P^2}\\right}^K_{k&#x3D;1} 和对应的 index codebook \\left{k\\right}^K_{k&#x3D;1} 。 给定一个图像 patch 将其展平为向量形式 x_i^v∈\\mathbb R^{C×P^2} ，其 index token t_i 可以通过查找最近质心的索引得到： t_i&#x3D;argmin_{k\\in \\left{1,…,K\\right}}||x_i^v-\\mathcal C_k||_2 同时，将该 patch x_i^v∈\\mathbb R^{C×P^2} 替换为 \\mathcal C_{t_i}\\in\\mathbb R^{C×P^2} 后，可以得到一个质心 patch。 图1. 构建 patches 的质心 codebook 2.2 设置掩蔽策略掩蔽策略使用了 BEiT 的 blockwise masking 和质心 patch 替换两种方式。 质心 patch 替换是指在 blockwise masking 后，随机使用对应的质心 patch 替换剩下的未被屏蔽的 patch。 给定掩蔽比 r_m 和替换比 r_{re} ，掩蔽位置 \\mathcal M∈\\left{1，…，n\\right}^{r_m×n} ，替换位置 \\mathcal R∈\\left{1，…，n\\right}^{r_{re}×n} 。被替换位置的前提是该位置未被屏蔽，即 \\mathcal M \\bigcap R&#x3D;∅ 。 给定一个图像 x∈\\mathbb R^{C×H×W} 及其 patches \\left{x_i^p\\right}^n_{i&#x3D;1} ，经过掩蔽策略后转换为： $$ \\tilde{x}&#x3D;\\left{x_i^p \\boldsymbol{E}p \\mid i \\notin \\mathcal{M} \\bigcup \\mathcal{R}\\right}{i&#x3D;1}^n \\bigcup\\left{e_m \\mid i \\in \\mathcal{M}\\right}{i&#x3D;1}^n \\bigcup\\left{C{a(i)} \\boldsymbol{E}p \\mid i \\in \\mathcal{R}\\right}{i&#x3D;1}^n\\ a(i) &#x3D; argmin_{k∈\\left{1，…，K\\right}}||x^v_i−\\mathcal C_k||_2 $$ 其中， e_m 是可学习的 mask token embedding， x \\boldsymbol{E}_p 表示 patch embedding 的计算过程。 图2. 设置掩蔽策略 2.3 使用 ViT 作为主干网络编码器分为像素预测模块和质心 index 预测模块。 质心 index 预测模块使用完整的 ViT 网络，使用线性头 lin.1(\\cdot) 将输入向量 \\boldsymbol{H}0&#x3D;\\left[\\boldsymbol{E}_{\\mathrm{CLS}}, \\tilde{x_1} \\boldsymbol{E}_p, \\ldots, \\tilde{x_n} \\boldsymbol{E}_p\\right] 通过 L 层 token block ψ_t^L(\\tilde x) 后产生的表征映射到质心 index 空间中。 由于基于质心的建模具有局部性，需要 CLS token 来显式地聚合全局表示，以补偿由于将原始图像 patch 降采样到质心所造成的信息损失。所以添加了一个像素预测模块用于收集全局信息。 将从第 l 层获取的早期表征定义为 [h^l_1，…, h^l_n] ，最后一层第 L 层获取 CLS token 定义为 h^L_{CLS} 。 像素预测模将表征 \\boldsymbol{H}p&#x3D;\\left[h^L_{CLS},h^l_1，…, h^l_n\\right] 输入两层 pixel ViT block ψ_p^2(H_p) 中，再使用线性头 lin.2(\\cdot) 将输出映射到原始像素空间中。 图3. CCViT 完整框架损失函数如下： $$ \\begin{aligned}\\mathcal{L}_{\\mathrm{CE}} &amp; \\left.&#x3D;-\\sum{\\tilde{x}} \\sum_{i \\in \\mathcal{T}} \\log \\left(p_{\\mathrm{CIM}}\\left(t_i \\mid \\tilde{x}\\right)\\right)\\right) \\ p_{\\mathrm{CIM}}\\left(t_i \\mid \\tilde{x}\\right) &amp;&#x3D; softmax_{t_i} (lin.1◦ψ_t^L(\\tilde x)) \\ \\mathcal{L}_{\\mathrm{MSE}} &amp; &#x3D;-\\sum{\\tilde{x}} \\sum_{i \\in \\mathcal{T}} \\frac{1}{\\lambda\\left(\\tilde{x}^{\\mathcal{T}}\\right)}\\left|x^{p \\mathcal{T}}-\\psi_p^2\\left(\\boldsymbol{H}p\\right)^{\\mathcal{T}}\\right|2 \\\\mathcal{L}_{\\mathrm{CIM}} &amp; &#x3D;\\mathcal{L}{\\mathrm{CE}}+\\mathcal{L}_{\\mathrm{MSE}}\\end{aligned} $$ 结果 实验参数设置请查看原文。 1）基线模型对比。从表 1 可以得出： CCViT 比 MAE 和其他基于像素重建的 MIM 方法精度高，说明 CCViT 方法可行且有效。 CCViT 比其他训练周期更长的 MIM 方法相比人有竞争力，这表明质心是比只用 token 和只用 pixel 更有效的重建目标。 CCViT 比 BEiTv2 精度低，说明 CLIP Feature 作为重建目标还是顶的。 表1. ImageNet-1K 上图像分类 top-1 accuracy (%) 和 ADE20K 上语义分割的 mIoU (%) 的微调结果。†：MAE 复现结果。2）消融实验。从表 2 可以得出： 将 tokens 和像素同时作为目标可以获得最好的结果，说明基于质心的建模中同时学习 tokens 和像素是有益的。 加上随机替代策略比仅用 blockwise masking 结果要好，说明使用随机替换策略将鼓励模型学习像素和 tokens 的对齐。 表2. 在 ImageNet-1K 和 ADE20K 上进行的消融实验。“Rep.” 指是否使用随机替代策略3）在 tokenizer 训练过程中，给 patch 随机加噪声，观察对其他 patches tokens 生成的影响。在从表 3 可以得出： BEiT 和 BEiTv2 的 tokenizer 即使没有改变 patches 像素也会改变相应的 tokens（只有 1.41% 和3.97% 的 tokens 保持不变），这意味着它们不能保证图像 patches 和 tokens 之间的局部对应关系。 表3. 不同 tokenizers 的抗噪声能力比较。使用 tokens 不变 patches 的比率作为评价指标。4）从表 4 可以得出： 基于质心的 tokenizer 相比于 BEiT 等基于 dVAE 的 tokenizer 拥有更快的速度和推理时间； 基于质心的 tokenizer 在预测精度上优于 BEiT 和 BEiTv2 tokenizer。这一方面表明，基于质心的 tokenizer 能够通过上下文来预测 token，另一方面，BEiT 和 BEiTv2 没有学习到足够的 patch 间关系，无法通过 patch 间关联进行掩蔽推理。这反映了 BEiTv2 的改进本质上是由离散特征蒸馏所带来的。 表4. 不同 tokenizers 的性能","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | AutoMAE：将生成对抗网络用于优化掩蔽策略","slug":"Masked_Image_Model/MIM-AutoMAE将生成对抗网络用-Glenn","date":"un55fin55","updated":"un55fin55","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-AutoMAE将生成对抗网络用-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-AutoMAE%E5%B0%86%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%94%A8-Glenn/","excerpt":"","text":"题目：Improving Masked Autoencoders by Learning Where to Mask作者单位：上海交通大学论文网址：https://arxiv.org/abs/2303.06583论文代码：未公开首次发布时间：2023 年 3 月 12 日 前言为了减少前景 patch 的掩蔽概率，PixMIM 使用 Simple Resized Crop 代替原始 MAE 使用的 Random Resized Crop。然而今天介绍的 AutoMAE 用实验证明，略微增加前景 patch 的掩蔽概率可以提高线性探测结果，但过度提高前景 patch 的掩蔽概率时反而会降低性能。 PixMIM 解读看这里： 1. 主要内容MIM 的输入 Patch 通常采用随机掩蔽的方式，这在很大程度上忽略了 Patch 之间的信息密度差异。那么是否存在比随机采样更好的掩蔽策略，我们如何学习它？ 对此，本篇论文提出了 AutoMAE 框架。如图1 所示，相比于独立训练指示器的 SemMAE（Semmae: Semantic-guided masking for learning masked autoencoders） ，AutoMAE 是在一个端到端完全可微的 MAE 框架下优化掩蔽策略的初步研究。 图1. 掩蔽策略的比较。(a) MAE：以均匀的概率随机掩码 75% 的图像补丁。(b) SemMAE 使用一个手动设计的 easy-to-hard 的 masking schedule，由一个独立训练的语义指示器引导。(c) AutoMAE 是一个完全可微的框架，它将一个反向训练的掩码生成器集成到掩码图像建模中。 2. 分析什么应该是一个好的掩蔽策略？一般来说，像素重建的难度与相应的图像 patch 的信息密度有关。 如果我们掩盖了大量的“困难” Patch（如前景对象），模型可能无法感知高语义区域； 然而，如果我们掩盖了大量的“简单” Patch（如背景），自监督训练任务可能过于简单，模型无法学习有效的表示。 在初步的实验中，可以发现这两种情况都导致学习的视觉表征的退化。作者将这种经验现象称为 MAE 中的 patch selection dilemma（patch 选择困境）。 初步实验随机选择 ImageNet 数据集的一个子集（10%），并使用提供的前景边界框（什么方法？）来指示前景位置，如图2 右所示。 图2 左中 β 表示前景 patch 相对于背景 patch 提高的掩蔽概率（例如，总掩蔽率为 75%，当 β&#x3D;15 % 时，前景 patch 掩蔽率为 45%，背景 patch 掩蔽率为 30%）。手动提高边界框内 patch 的掩蔽率，并使用这些样本进行自监督预训练，图2 左显示了线性探测结果。从图中可以得到两个观察结果： 与随机掩蔽相比，略微增加前景 patch 的掩蔽概率，显著提高了线性探测结果。 过度提高前景 patch 的掩蔽概率，可能会影响 pretext task 的有效性，降低其性能。 可以看出，掩蔽策略是一个 trade-off 的问题，前景掩蔽率越高，通过掩蔽图像建模获得的信息越多，但同时解码器的重建难度也越高。一个自然的解决方案是将掩模生成和图像重建集成到一个完全可微的框架中。也就是说，我们希望在获得的具有较高信息密度的区域内找到重建难度较低的 patch 。 图2. 通过 β 提高掩蔽概率对前景边界框内的 patch 的影响。模型在 ImageNet 数据集的一个子集（10%）上进行训练。红色虚线表示使用原始随机掩蔽策略的结果。 3. 方法通过上述实验，现在需要设计一个新的自监督框架，它可以学习选择信息更丰富的前景 patch 来掩蔽，且后续的重建难度较低，其中需要解决两个关键的问题： 如何在没有任何明确监督的情况下指导 ViT 模型挖掘具有较高信息密度的前景 patch？ 如何通过自适应学习的掩蔽策略来控制 pretext task 的难度？ 图3. AutoMAE 的端到端框架。 3.1 掩模生成器对于第一个问题，作者提出了一个可微的掩模生成器 G 来生成包含每个图像 patch 重要权重的掩模图像。掩模生成器包含一个预训练的 ViT 编码器（参数冻结）和两个可训练的卷积层。通过预先训练的 ViT 编码器从最后一个 Transformer 块中提取 multi-head self-attention maps，再通过卷积层进行进一步处理以生成掩模。此时获得的掩膜图像中包含了模糊的不同语义区域的指示，但仍需要突出该样本的前景。 因此，作者引入了一种以前景为中心的对抗性训练策略，来指导掩模生成器在可能存在前景的 patch 上产生更高的权重。具体来说，首先随机生成一张伪掩模图像（在全黑图像中随机涂白一个矩形，然后在整张图像上加随机噪声，其中白色矩形区域多加个 \\alpha&#x3D;0.5 ），作为“真实”样本，该样本可以模拟前景和背景补丁之间的差异，并使用对抗性训练策略来最小化生成的掩模和“真实”采样之间的分布偏移。 multi-head self-attention maps A_i 计算公式如下： $$ A_i&#x3D;softmax(q^c_iK_i^T&#x2F;\\sqrt {d’}) $$ 其中， q_i^c \\in \\mathbb{R}^{1×d’} 表示 [CLS] token 的 query embedding， K_i\\in \\mathbb {R}^{\\frac{hw}{p^2}×d’} 表示其他 patch token 的 key embedding。 两个卷积层再加 ReLU 的操作 f_\\theta(\\cdot) 如下： $$ F &#x3D; f_θ(A) $$ 最终掩模图像 M\\in\\mathbb{R}^{1×\\frac{h}{2}×\\frac{w}{2}} 计算公式如下： $$ f_i’&#x3D;log(\\frac{exp(f_i)}{\\sum^{hw&#x2F;p}_{k&#x3D;1}exp(f_k)})+z $$ $$ m_i&#x3D;\\frac{exp(f_i’)}{\\sum^{hw&#x2F;p}_{k&#x3D;1}exp(f_k’)} $$ 其中， z 表示从 Gumbel 分布中采样的随机噪声，这里的 log(softmax(\\cdot)) 用于稳定训练过程。 f_i 是 F 的展平， m_i 是掩膜图像 M 的展平。 在全黑图像 M^b\\in\\mathbb{R}^{1×\\frac{h}{2}×\\frac{w}{2}} 中随机涂白一个矩形 X&#x3D;(20%\\sim80%)M^b ，伪掩模图像 M^p 的计算公式如下： $$ m^p_i&#x3D; \\epsilon + \\begin{cases} \\alpha,i\\in X\\ 0,i\\in X \\end{cases} $$ 其中， \\epsilon\\sim U(0,1) 表示随机噪声。 α 表示矩形内的额外权重，在实验中设置为 0.5。 使用 LSGAN（Least squares generative adversarial networks） 中的损失函数进行对抗性训练： $$ \\mathcal{L}_{adv}&#x3D;-\\mathbb{E}_M[(D(M)-c)^2] $$ $$ \\mathcal{L}{adv}^D&#x3D;\\mathbb{E}{M^p}[(D(M^p)-b)^2]+\\mathbb{E}_M[(D(M)-a)^2] $$ 其中 D 表示鉴别器， a&#x3D;−1 ， b&#x3D;1 ， c&#x3D;0 。 3.2 梯度回传对于第二个问题，为了在保证掩模具有较高信息密度的同时，后续的重建难度较低，作者直接将梯度从 MAE 传播回掩模生成器，并同步训练这两个模块。MAE 通过重建损失来约束掩模生成器不生成过于困难的掩模图像，从而鼓励掩模生成器产生容易推断掩蔽信息的重要 Patch，而鉴别器则约束掩模生成器更加关注前景而非背景。 允许来自掩码自动编码器的梯度可以传播回掩码生成器，作者改变了输入 patch token Z&#x3D;\\left{z_i\\right}^n_{i&#x3D;1} 的公式，生成的新输入 patch token Z’&#x3D;\\left{z’_ i \\right}^n_{i&#x3D;1} 的计算公式如下： $$ z’_i &#x3D; z_i\\cdot m_i+z_i\\cdot sg(1 − m_i) $$ 其中， sg(\\cdot) 表示停止梯度操作。 结合对抗损失，掩码生成器的最终损失函数可以写为： $$ L_G &#x3D; L_{recon} + λL_{adv} $$ 其中，作者通过 grid search 将 λ 设置为 0.2。 （中间还有个生成 Final Mask 的操作没说，那个就是利用生成的掩膜经过几个 Top-K 操作，感兴趣的可以去原文 3.3 小节查看） 4. 结果图4. 由掩模生成器产生的 ImageNet 上的高加权掩模图像。具体结果查看原文。 又是一篇观点矛盾的论文，PixMIM 认为较高的前景掩蔽率阻碍了模型有效捕获形状和语义先验的能力，从而限制了表征的质量；而 AutoMAE 认为略微增加前景 patch 的掩蔽概率可以提高模型性能。 说实话，感觉 AutoMAE 的模型有些复杂了。","categories":[{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]}],"categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"},{"name":"论文速览","slug":"论文速览","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88/"}],"tags":[{"name":"arXiv","slug":"arXiv","permalink":"https://chenluda.github.io/tags/arXiv/"},{"name":"Organization","slug":"Organization","permalink":"https://chenluda.github.io/tags/Organization/"},{"name":"WeChat","slug":"WeChat","permalink":"https://chenluda.github.io/tags/WeChat/"},{"name":"DataBase","slug":"DataBase","permalink":"https://chenluda.github.io/tags/DataBase/"},{"name":"Zhihu","slug":"Zhihu","permalink":"https://chenluda.github.io/tags/Zhihu/"},{"name":"Markdown","slug":"Markdown","permalink":"https://chenluda.github.io/tags/Markdown/"},{"name":"Conference paper","slug":"Conference-paper","permalink":"https://chenluda.github.io/tags/Conference-paper/"},{"name":"OpenReview","slug":"OpenReview","permalink":"https://chenluda.github.io/tags/OpenReview/"},{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]}