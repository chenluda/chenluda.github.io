{"meta":{"title":"Glenn","subtitle":"","description":"","author":"Glenn","url":"https://chenluda.github.io","root":"/"},"pages":[{"title":"关于","date":"un66fin66","updated":"un66fin66","comments":false,"path":"about/index.html","permalink":"https://chenluda.github.io/about/index.html","excerpt":"","text":"123456789101112131415&#123; name: &#x27;Glenn&#x27; age: 25, gender: &#x27;男&#x27;, profession: &#x27;学生&#x27;, address: &#x27;云南省昆明市&#x27;, education: [ [&#x27;本科&#x27;, &#x27;昆明理工大学&#x27;, &#x27;生物医学工程&#x27;, &#x27;2017-2021&#x27;], [&#x27;硕士&#x27;, &#x27;昆明理工大学&#x27;, &#x27;计算机应用技术&#x27;, &#x27;2021-2024&#x27;], ], github: &#x27;https://github.com/chenluda&#x27;, blog: &#x27;blog.glennblog.xyz&#x27;, email: &#x27;chenluda01@outlook.com&#x27;,&#125; 商业合作联系: &#99;&#x68;&#x65;&#x6e;&#108;&#117;&#x64;&#x61;&#48;&#x31;&#64;&#x6f;&#117;&#x74;&#x6c;&#x6f;&#111;&#x6b;&#x2e;&#99;&#x6f;&#109;"},{"title":"categories","date":"un55fin55","updated":"un66fin66","comments":true,"path":"categories/index.html","permalink":"https://chenluda.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"un55fin55","updated":"un55fin55","comments":true,"path":"links/index.html","permalink":"https://chenluda.github.io/links/index.html","excerpt":"","text":""},{"title":"tags","date":"un55fin55","updated":"un66fin66","comments":true,"path":"tags/index.html","permalink":"https://chenluda.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Catalyst Plus：从入门到应用，从自然到科学，畅行科研宇宙","slug":"速览/us从入门到应用从自然到科学畅行科研宇-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/速览/us从入门到应用从自然到科学畅行科研宇-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/%E9%80%9F%E8%A7%88/us%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%BA%94%E7%94%A8%E4%BB%8E%E8%87%AA%E7%84%B6%E5%88%B0%E7%A7%91%E5%AD%A6%E7%95%85%E8%A1%8C%E7%A7%91%E7%A0%94%E5%AE%87-Glenn/","excerpt":"","text":"找到一款非常好用的科研平台，分享给大家。 催化剂加（Catalyst Plus）官网：https://www.researchercosmos.com/home 核心团队来自浙江大学，我们希望通过降低科研人员获取全球最新研究成果的壁垒，减少科研全生命周期数据整合处理的执行成本，使研究人员可以专注于真正重要的事情，从而高效进行开创性的研究工作。 从体验角度来说，与我近期使用的由粤港澳大湾区数字经济研究院开发的 ReadPaper 相比，浙江大学推出的Catalyst Plus，优劣之处均分明。 两者均致力于帮助科研人员提高工作效率，然而侧重点有所不同。ReadPaper 更注重提升论文阅读的用户体验和知识分享，而 Catalyst Plus 则专注于实现科研资料的有效整合。这两个特性都是我在日常科研工作中所需的。 我仅站在我自己的角度对 Catalyst Plus 的功能作出一些介绍和评价： 功能名称 功能描述 个人评价 1. 求事台 类似浏览器的 new tab，可以添加一些网页组件、功能组件等。 评分：★★☆☆☆（2&#x2F;5）可以快速进入日常频繁使用的网页，但有缺陷，下文会详细指出。 2. Copilot 科研大模型，除正常的 chat 外，还集成了翻译、总结、复述、语法检查等现成功能。 评分：★★★☆☆（3&#x2F;5）让软件戴上“AI 大模型赋能”帽子的必需品，不予评价。但的确有用。 3. 订阅 类似 RSS，可以订阅期刊、作者、关键词等。 评分：★★★★★（5&#x2F;5）只能说这个功能是懂我的，这个功能是我 inoreader 的上位替代。但也有缺陷，在下文中会指出。 4. 期刊库 可以查找期刊，查看期刊年文章数、版面费等详细信息。 评分：★★★★☆（4&#x2F;5）是我需要的，但操作体验感不太好。我觉得重点应该放在期刊详情上，而不是查找期刊上，当然也可以放在放在查找期刊上，但分类标准不应该是按主题？ 5. 文库 文献分类存储，文献阅读。 评分：★★★★☆（4&#x2F;5）因为之前在用 ReadPaper 的缘故，Catalyst Plus 的论文阅读中规中矩，各个功能也都有。下文做详细对比。 6. 科研支持 查看实验室耗材、设备、科研品牌，科研基金项目，全球专利以及学术报告。 评分：★★☆☆☆（2&#x2F;5）写本子可以有用，但对我来说没有太大用处:) 7. 思维空间 Beta 版 团队协作知识库。 评分：★★☆☆☆（2&#x2F;5）在类似飞书等团队协作软件中经常可见，只不过将团队的概念改为课题组，应该是想做成论文阅读笔记组内分享、周记分享这样的形式。试用了下，完成度不高。 总的来说，某些功能使用起来还是非常 nice 的，建议大家通过目录直接跳转到 3、4、5 小节。 求事台评分：★★☆☆☆（2&#x2F;5） 如图 1 所示，在浏览器的 new tab 插件里应该经常可以见过，主要功能就是通过添加一些网页组件、功能组件，快速进入某个使用频繁的网页，或者快速知晓所需的信息，提高效率。 但是有个 bug，如图 2 和图 3 所示，许多科研网站需要科学上网，但平台没有内置代理（可能有，我没找到），当开了全局代理时，平台的某些设置又会报错。 图1. 求事台图2. 没有内置代理图3. 开全局代理时不兼容 Researcher Copilot评分：★★★☆☆（3&#x2F;5） Researcher Copilot 的开发基于大型科研领域全学科语言模型「UItron⌋，我们整合了十亿级学术基础数据，涵盖论文、学者、会议、图表、公式、专利、基金等多样化数据类型，致力于颠覆现有学术检索路径、简化繁琐的科研流程，使科研者得以将时间和精力投入更具创新性的科研工作中。 之前泛读论文养成了个习惯，喜欢把 Introduction 部分直接扔给 ChatGPT 做分点总结，背景、现有研究、现有研究的缺陷、方法的提出，整条逻辑链两三分钟就可以清晰呈现。 很多人本能地抗拒使用这些大模型，是因为它有着学习成本。但当真正的养成使用习惯时，你会发现确实可以提高效率。 （此功能收费） 图4. Researcher Copilot 订阅评分：★★★★★（5&#x2F;5） 极力推荐，这也是吸引我使用这款软件的功能。 类似 RSS 订阅功能，可以快速追踪本领域 TOP 期刊的 Pre-proof 的论文。 相比于图 6 的常用 RSS 阅读器 Inoreader，图 5 Catalyst Plus 的论文卡片信息更有针对性，除了中文翻译的标题外，还有发布日期、作者信息等。 除了期刊订阅外，还有作者订阅和关键词订阅。 先说关键词订阅，之前我都是在 arxiv 上搜索关键词，这样有可能会漏掉很多正式期刊中发布的论文，而如果在 Google Scholar 上搜索的话又会漏掉 arxiv 上发布的最新论文。 如图 7 所示，Catalyst Plus 解决了这个难题，直接追踪所有的 :)，而且这论文卡片上展示的期刊名也是我爱的。 作者订阅其实也是所需的功能，但是显然还不完善。 如图 8 所示，无法根据单位或发表期刊精确确定作者。 但是不影响我对这个功能的喜欢。另：希望再加上会议论文和 blog 的订阅。 图5. Catalyst Plus 的订阅图6. Inoreader图7. 关键词订阅图8. 作者订阅 期刊库评分：★★★★☆（4&#x2F;5） 期刊库是我很喜欢的功能，类似小木虫，可以查看期刊 IF、年文章数、国人占比等详细信息，也可以查阅期刊论文。 图9. 期刊库本来这个功能也能打五分的，可惜用户操作体验不好，以及功能重心偏移。 首先大家期待的期刊库分类应该是：主题 -&gt; SCI&#x2F;EI&#x2F;中科院 -&gt; 领域几区，甚至后面再按投稿难易程度，审稿周期等字段分级，最后再加个分类分级条件收藏。 而这些功能完全可以在一个页面里完成，类似本软件的超能检索页面，左侧条件选择栏，右侧期刊目录，多简洁。web 设计中的大忌就是疯狂跳转。 最后，一样地，希望加上会议库。 图10. 期刊主题查询 文库评分：★★★★☆（4&#x2F;5） 在文库中可以对收藏的论文进行分类整理，利用大模型可以对论文进行简单总结，如图 12 所示。 这里有个逻辑问题，就是需要先创建好分类文件夹，然后再将对应文章加入，而普通人的习惯是先查看论文，然后再分类，这样的话就需要关闭论文，回到文库界面进行操作。 如图 13 所示，论文阅读器的功能基本齐全。标注、翻译等等。 但是缺少参考文献查看的功能，点击引用标注时会跳转页面。 图11. 文库图12. 论文总结图13. 论文阅读 科研支持评分：★★☆☆☆（2&#x2F;5） 这个模板应该是针对青年学者的。 图14. 数据中心图15. 科研基金图16. 全球专利图17. 学术报告 思维空间 Beta 版评分：★★☆☆☆（2&#x2F;5） 在类似飞书等团队协作软件中经常可见，只不过将团队的概念改为课题组，应该是想做成论文阅读笔记组内分享、周记分享这样的形式。目前完成度不高。 图18. 思维空间 Beta 版 如果觉得有用的话，可以帮我填写下邀请码： Catalyst Plus 学术码：wzoIsC4CJnkT ReadPaper 注册码：Ynno 我的邀请码是【Ynno】，科研神器ReadPaper限时邀请注册中，点击链接即可接受我的邀请： [免费科研神器]ReadPaper实在太好用了-ReadPaper","categories":[{"name":"推荐","slug":"推荐","permalink":"https://chenluda.github.io/categories/%E6%8E%A8%E8%8D%90/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"}]},{"title":"MedSeg | 腹部多器官和肿瘤分割的增量学习","slug":"Medical_Image_Segmentaion/Seg-腹部多器官和肿瘤分割的增量学-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/Medical_Image_Segmentaion/Seg-腹部多器官和肿瘤分割的增量学-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/Medical_Image_Segmentaion/Seg-%E8%85%B9%E9%83%A8%E5%A4%9A%E5%99%A8%E5%AE%98%E5%92%8C%E8%82%BF%E7%98%A4%E5%88%86%E5%89%B2%E7%9A%84%E5%A2%9E%E9%87%8F%E5%AD%A6-Glenn/","excerpt":"","text":"题目：Continual Learning for Abdominal Multi-Organ and Tumor Segmentation作者单位：约翰霍普金斯大学论文网址：https://arxiv.org/abs/2306.00988 论文代码：https://github.com/MrGiovanni/ContinualLearning 首次发布时间：2023 年 6 月 1 日 12345678@misc&#123;zhang2023continual, title=&#123;Continual Learning for Abdominal Multi-Organ and Tumor Segmentation&#125;, author=&#123;Yixiao Zhang and Xinyi Li and Huimiao Chen and Alan Yuille and Yaoyao Liu and Zongwei Zhou&#125;, year=&#123;2023&#125;, eprint=&#123;2306.00988&#125;, archivePrefix=&#123;arXiv&#125;, primaryClass=&#123;eess.IV&#125;&#125; 主要内容深度学习模型存在“灾难性遗忘”问题，新数据的学习可能会覆盖之前获取的知识。 医学领域的多器官和肿瘤分割任务需要模型动态地扩展到新的类别，然而现有增量学习方法大多需要存取旧数据，而这在实践中受到隐私法规的限制，难以实施，例如： Ozdemir 等人将蒸馏损失应用到医学图像分割。Liu 等人引入了一个内存模块来存储不同器官类别的典型表现。这些方法依赖于少数具有代表性的样本，而这可能在实践中无法获取； 一项最近的研究主要关注网络扩展，通过冻结编码器和解码器并在学习新类时添加额外的解码器来解决遗忘问题。尽管这些方法在减轻遗忘问题上取得了成功，但它们为网络参数带来了巨大的内存成本。 因此，作者确定了在设计多器官和肿瘤分割框架时必须解决的两个主要开放问题： Q1：能否在不需要具有代表性的样本的情况下缓解遗忘问题？ Q2：能否设计一个新的网络架构，允许在不同的增量学习步骤之间共享更多的网络参数？ 这篇研究提出了一个新的增量多器官和肿瘤分割方法，这种方法不需要保存旧数据，且减小了内存和计算的负担。概括来说，总共分为三个部分： 受增量学习中的知识蒸馏方法的启发，在新到达的数据上为旧类生成软伪注释。这使模型能够在不保存旧数据的情况下回忆旧知识。通过这种简单的策略，能够为旧类保持合理的性能； 在共享的编码器和解码器之上为每个类提出了图像感知的分割头。这些头允许使用单一的 backbone 和轻松扩展到新类，同时带来较小的计算成本。 将对比语言-图像预训练（CLIP）应用到此问题中，利用文本嵌入的高级视觉语义信息，增强训练效果。 通过使用三个数据集进行验证，该方法在保持对旧类别知识的理解和学习新类别的能力上，都表现出优越的性能，且内存效率高。 方法首先定义增量器官分割问题：给定一序列部分标注的数据集 \\left{D_1, D_2, . . . , D_n\\right} ，每个数据集有其器官类别 \\left{C_1, C_2, . . . , C_n\\right} ，然后一次使用一个数据集来顺序地学习一个单一的多器官分割模型。当在第 i 个数据集 D_t 上训练时，先前的数据集 \\left{D_1, D_2, . . . , D_{n-1}\\right} 不可用。模型需要预测所有已看见数据集 \\left{D_1, D_2, . . . , D_t\\right} 的累积器官标签： \\begin{array}{l}\\hat{Y}_{j}&#x3D;\\arg!\\operatorname*{max}_{c\\in C_t}P(Y_{j}&#x3D;c|X)\\ C_{t}&#x3D;\\cup_{\\tau\\leq t} C_{\\tau}\\end{array} 其中， j 是体素索引， X 是来自 D_t 的图像， P 是模型学习的概率函数，而 \\hat Y 是输出的分割掩膜。 多器官分割的伪标签一个在某些器官类别上进行预训练的分割模型在微调新类别时会完全忘记旧的类别。使用伪标签可以大大缓解这个问题，并保留现有的知识。 具体来说，利用先前学习步骤 t − 1 的输出预测 \\hat Y_{t−1} ，这个预测值包括了在那一步学习的所有旧类别 C_{t−1} ，作为当前步骤旧类别的伪标签。对于新类别，仍然使用真实的标签。形式上，当前学习步骤 t 中类别 c 的标签 \\tilde L^c_t 可以表达为： \\tilde{L}_{t}^{c}&#x3D;\\left\\begin{array}{l l}L_{t}^{c}&amp;\\mathrm{if}c\\in C_{t}-C_{t-1}\\ \\hat{Y}_{t-1}^{c}&amp;\\mathrm{if}c\\in C_{t-1}\\end{array}\\right. 其中， L^c_t 代表了从数据集 D_t 获取的在步骤 t 中类别 c 的真实标签。通过利用这种方法，可以保持原有知识，并在学习新类别时防止模型忘记之前学习的信息。接下来提出的模型只使用旧类别的伪标签进行训练，而不需要任何其他的知识蒸馏或正则化。 多器官分割模型图 1 展示了本文所提模型架构的整体框架。它由一个主干网络，一组图像感知的器官特定输出头，和文本驱动的头部参数生成组成。 图1. 所提模型架构的整体框架。 主干网络：对于增量学习来说，理想的模型应能学习到足够通用的表示，从而轻松适应新的类别。使用 Swin UNETR 作为主干网络，因为它在自监督预训练和转移到各种医学图像分割任务上表现出强大的性能。 Swin UNETR 的编码器是 Swin Transformer，解码器是几层反卷积层。使用类似 U-Net 的跳跃连接来将低级别编码器特征和解码器特征融合。 图像感知器官特定头：原始的 Swin UNETR 网络的输出层是一个 Softmax 层，预测每个类别的概率。作者提出将输出层替换为多个图像感知的器官特定头。首先，在最后的编码器特征上使用全局平均池化（GAP）层，得到当前图像 X 的全局特征 f 。然后，对于每一个器官类别 k ，学习一个多层感知器（MLP）模块，将全局图像特征映射到一组参数 θ_k ： θ_k &#x3D; MLP_k(GAP(E(X))) 其中 E(X) 表示图像 X 的编码器特征。器官类别 k 的输出头是一系列使用参数 θ_k 作为卷积核参数的卷积层。这些卷积层应用于解码器特征，输出器官类别 k 的分割预测： P(Y_{j}^{k}&#x3D;1|X,\\theta_{k})&#x3D;\\sigma(\\mathrm{Conv}(D(E(X));\\theta_{k})) 其中， E 是编码器， D 是解码器， σ 是 Sigmoid 非线性层， P(Y_{j}^{k}&#x3D;1) 表示像素 j 属于器官类别 k 的预测概率。每个类别的预测通过二元交叉熵损失进行优化。这种设计允许对新引入的和先前学习的类别进行独立的概率预测，从而在增量学习过程中最小化新类别对旧类别的影响。此外，这种设计允许对一个像素属于多个类别（例如，一个器官上的肿瘤）的情况进行多标签预测。 文本驱动的头部参数生成：进一步为分割头提供了关于每个器官类别的语义信息，使用 CLIP 生成目标器官名字的文本嵌入。具体来说，通过预训练的 CLIP 文本编码器和医学提示（例如，”[CLS] 的计算机断层扫描”，其中 [CLS] 是器官类别名）生成器官名字嵌入。然后，使用文本嵌入 ω 和全局图像特征 f 一起生成器官分割头的参数： θ_k &#x3D; MLP_k([GAP(E(X)), ω_k]) 其中 ω_k 是器官类别 k 的文本嵌入。CLIP 嵌入携带高级语义，并有能力连接相关概念。因此，它指导 MLP 模块为每个器官类别生成更好的卷积参数。更重要的是，固定长度的 CLIP 嵌入能够将预训练模型适应到开放词汇的分割，并扩展到新的类别。 结果与讨论数据集：在两种数据设置下实证评估了所提出的模型。 训练和增量学习都在JHH 数据集上进行。它有多个已注释的类别，可以分为三组：腹部器官（其中七个类别在步骤 1 中学习：脾、右肾、左肾、胆囊、肝、下腔静脉、胰腺）、胃肠道（其中三个类别在步骤 2 中学习：胃、结肠、小肠）、其他器官（其中四个类别在步骤 3 中学习：主动脉、门静脉和脾静脉、腹腔干、上腹腺动脉）。 先在 BTCV 数据集上进行训练，然后在 LiTS 数据集上进行增量学习。BTCV 数据集包含 47 个腹部 CT 图像，描绘了 13 个器官。LiTS 数据集包含 130 个增强的腹部 CT 扫描，用于肝脏和肝脏肿瘤分割。在步骤 1 的学习中使用了 BTCV 中的 13 个类别（脾、右肾、左肾、胆囊、食道、肝、胃、主动脉、下腔静脉、门静脉和脾静脉、胰腺、右肾上腺、左肾上腺），在步骤 2 的学习中使用了 LiTS 中的肝肿瘤。 结果：使用 JHH 数据集和公共数据集的增量分割结果分别在表 1 和表 2 中显示。 表1. JHH 数据集上的基准增量学习方法表2. 公共数据集上的基准增量学习方法表3. JHH_organ 数据集中七个类别的基准增量学习方法。在三个增量学习步骤中展示了每个类的 Dice 分数。表4. JHH_gastr 数据集中三个类的基准增量学习方法。在两个增量学习步骤中展示了每个类的 Dice 分数。表5. JHH_cardiac 数据集中四个类的基准增量学习方法。在最终的增量学习步骤中展示了每个类的 Dice 分数。表6. 从 BTCV 到 LiTS 数据集的基准增量学习方法。在两个增量学习步骤中展示了每个类的 Dice 分数。 通过简单地使用伪标签技术（LwF），能够在记忆旧类别上达到相当好的性能（在 JHH 数据集的步骤 2 中，腹部器官的 Dice 为 0.777，步骤 3 中为 0.767；在 BTCV 器官的步骤 2 中，Dice 为 0.770）。 所提出的方法在旧类别中表现出最少的遗忘，并且在适应新数据和新类别上有更好的能力。 为了评估所提出的模型设计，还对 JHH 数据集进行了消融研究，结果显示在表 7 中。 表7. JHH 数据集的消融研究。具体来说，消融了器官特异性分割头以及 CLIP 文本嵌入带来的性能提升。 表 3 的第一行显示了使用伪标签（LwF）学习的基线 Swin UNETR 模型的性能。 第二行引入了器官特异性分割头，但对每个器官使用的是 one-hot 嵌入，而不是 CLIP 文本嵌入。 第三行给出了完整方法的性能。 结果显示，通过将模型调整为使用器官特异性头部作为分割输出，能够实现大幅度的改进（例如，对胃肠道的步骤 2 改进了 0.144，步骤 3 改进了 0.179）。通过应用 CLIP 文本嵌入，能够进一步提高性能（例如，通过在胃肠道的步骤 2 和步骤 3 中增加 0.019 和 0.027）。 最后，图 2 展示了所提出方法的定性分割结果，以及在 JHH 数据集上最好的基线方法 ILT。每列一个案例，每个步骤两个案例。可视化结果表明，所提出的方法成功地分割出正确的器官，而最好的基线方法在整个增量学习过程中都性能不佳。 图2. 在 JHH 数据集上持续学习步骤 2 和 3 中本文提出模型和基线模型 ILT 的可视化比较。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Medical Image Segmentation","slug":"Medical-Image-Segmentation","permalink":"https://chenluda.github.io/tags/Medical-Image-Segmentation/"}]},{"title":"MedSeg | IFE：基于曲率或信息熵的通道注意力机制","slug":"Medical_Image_Segmentaion/IFE基于曲率或信息熵的通道注意力机-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/Medical_Image_Segmentaion/IFE基于曲率或信息熵的通道注意力机-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/Medical_Image_Segmentaion/IFE%E5%9F%BA%E4%BA%8E%E6%9B%B2%E7%8E%87%E6%88%96%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E9%80%9A%E9%81%93%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA-Glenn/","excerpt":"","text":"题目：Instructive Feature Enhancement for Dichotomous Medical Image Segmentation作者单位：深圳大学、Shenzhen RayShape Medical Technology Co.、苏黎世联邦理工学院论文网址：https://arxiv.org/abs/2306.03497论文代码：https://github.com/yezi-66/IFE首次发布时间：2023 年 6 月 6 日 12345678@misc&#123;liu2023instructive, title=&#123;Instructive Feature Enhancement for Dichotomous Medical Image Segmentation&#125;, author=&#123;Lian Liu and Han Zhou and Jiongquan Chen and Sijing Liu and Wenlong Shi and Dong Ni and Deng-Ping Fan and Xin Yang&#125;, year=&#123;2023&#125;, eprint=&#123;2306.03497&#125;, archivePrefix=&#123;arXiv&#125;, primaryClass=&#123;cs.CV&#125;&#125; 主要内容 背景介绍： 由于成像方法的不同，医学图像具有不同的表现形式。来自同一模态但不同部位的图像在整体结构上具有高度的相似性，但在细节和纹理上具有多样性。 现有的医学图像分割模型在特定模态或解剖结构数据集上训练的模型可能不适应新的数据集。 现有研究： 有些研究增加了骨干网络的深度或宽度，如 UNet++，使用嵌套结构和密集的跳跃连接；DeepLabV3+，它将膨胀卷积和特征金字塔池与一个有效的解码器模块相结合； 还有些工作专门设计即插即用的功能模块，如 Inception 及其变体，深度可分卷积，注意力机制和多尺度特征融合。 像 nnUNet 这样的框架通过集成关键数据集属性，为多个分割任务开发了一个自适应分割管道，达到 SOTA。 现有研究的缺陷： 大多数现有的医学图像分割的模型架构都是经过精心设计的。尽管这些工作都很有前途并且可以灵活使用，但它们通常需要针对不同的分割任务进行手工调整。 nnUNet 需要大量的设计工作，而且成本非常昂贵。 作者观察到特定特征通道中丰富的纹理和尖锐的边缘线索对于准确地分割目标是至关重要的且具有指导意义的。以往的分割网络往往忽视了特征通道的重要性，这可能会限制其在一般分割任务中的性能。 方法提出： 曲率（Curvature）可以表示图像的边缘特征。熵（Information entropy）可以描述图像的纹理和内容的复杂性。 基于曲率和熵，作者提出了提出了一种简单、通用、有效的特征增强方法，即 IFE。基于局部曲率或全局信息熵准则，可以自适应地选择具有丰富纹理线索和强可分辨性的特征通道来增强原始特征。 IFE 是即插即用的，适用于不同的分割任务，它鼓励模型关注纹理丰富的特征，这些特征对于模糊和具有挑战性的边界识别特别重要，同时实现简单性、通用性和一定的可解释性。 方法IFF 提供了两种方法，即基于曲率的特征量化方法和基于信息熵的特征量化方法，量化出的参数表示了每个特征通道的内容丰富度。这些参数越大，对应通道特征的纹理和细节就越丰富。 通过选择一定比例的具有高曲率或信息熵的通道特征，并将它们与原始特征相结合。IFE 通过对分割网络架构进行微小修改来提高性能。 基于曲率的特征选择对于嵌入欧几里得空间 R^3 中的二维曲面，存在两种曲率：高斯曲率和平均曲率。与高斯曲率相比，平均曲率可以更好地反映表面的不均匀性。 Gong 提出了一个计算公式，只需要简单的线性卷积就可以获得近似的平均曲率，如下所示： C&#x3D;[C_1\\quad C_2 \\quad C_3] *X 式中， C_1&#x3D;[α,β,α]^T ， C_2&#x3D;[β,γ,β]^T ， C_3&#x3D;[α, β, α]^T ， α 、 β 和 γ 的值分别为 −1&#x2F;16 、 5&#x2F;16 、 −1 。 * 表示卷积， X 表示输入图像， C 是平均曲线。 图 1 展示了该过程，从图中可以看出曲率图像可以有效地突出特征中的边缘细节。 图1. 使用曲率进行特征选择。 基于信息熵的特征选择信息熵反映了强度分布的空间和聚集特征。它可以公式化为： E&#x3D;-\\sum_{i&#x3D;0}^{255}\\sum_{j&#x3D;0}^{255}P_{i,j}l o g_{2}\\left(P_{i,j}\\right) ;P_{i,j}&#x3D;f\\left(i_{n},j_{n}\\right)&#x2F;\\left(H\\times W\\right) i_n 表示第 n 个 3×3 滑动窗口中中心像素的灰度值， j_n 表示该窗口中剩余像素的平均灰度值。 (i_n,j_n) 出现在整个图像中的概率由 P_{i,j} 表示， E 表示信息熵。 图2. 用 2D 信息熵进行特征选择。图像上的每个像素对应一个灰度或颜色值，范围从0到255。而特征映射中的每个元素都表示在输入图像的特定位置上卷积的激活水平。 给定输入特征 F_x ，如图 3 所示，通过滑动窗口获得的元组 (i,j) 被转换为直方图，表示激活水平的大小和邻域内的分布。这涉及到重新排列特征图的激活级别。图 3 展示了直方图转换方法 histc 和信息熵 E 计算方法。概率 P_{hist}(i,j) 将用于计算信息熵。 图3. 直方图转换方法和信息熵计算方法 构造增强的特征虽然 IFE 可以应用于各种深层神经网络，但本文的研究主要集中在广泛应用的分割网络。 图 4 展示了嵌入在代表性网络中的 IFE 的框架，例如 DeepLabV3+、UNet、nnUNet 和 SINetV2。前三种是经典的分割网络。由于分割任务类似于伪装目标检测，比如低对比度和模糊边缘，作者也考虑了 SINetv2。 如图 4 所示，在 UNet 和 nnUNet 的中间层，DeepLabV3+ 的低级特征以及 SINetV2 的 TEM 输出上接入 IFE。 图4. 经典网络中的 IFE 接入位置。当输入图像被编码到特征空间时，不同的通道特征在不同的方向和频率上保留了纹理。值得注意的是，同一个通道所包含的信息可能在不同的图像上有所不同，如图 5 所示。 例如，肺部 CT 特征图的第 15 通道包含有价值的纹理和细节，而主动脉 CT 特征图的同一通道可能不会提供有意义的信息内容。然而，他们的第二通道功能都侧重于边缘细节。通过保留原始特征，可以动态地从输入特征中选择对当前对象分割有较大贡献的信道特征。 图5. SINetV2 stage 3 的特征图可视化。第一排是肺，第二排是主动脉。2、15 和 17 是通道的索引。IFF 可以显式地增加模型对通道信息的敏感性。 具体来说，对于输入图像 X ，使用参数为 θ_x 的编码器 Encoder(X,θ_x) 提取深度特征 F_x &#x3D; [ f_1,f_2,f_3,… ,f_C ]∈\\mathbb R^{\\mathbb C * \\mathbb H * \\mathbb W} ，IFE 可以表示为： F’_x&#x3D;max\\left{S(F_x),r\\right} F’_x 是所选的特征图， S 是量化方法（基于曲率或信息熵）， r 是选择比例。 但通过 pixel-wise addition 增强原始特征可能会引入不需要的背景噪声。所以使用 concatenate 操作直接连接特征，将 F&#x3D;[F_x,F′_x] 作为网络下一阶段的输入。 结果与讨论作者在自己构建并公开的数据集 Cosmos55k 上进行验证实验。 如图 6 所示，Cosmos55k 提供了 7 种成像模式，包括 CT、MRI、X光、眼底等，涵盖了 26 个解剖结构，如肝脏、息肉、黑色素瘤和脊椎等。图像只包含一个标记的对象，减少了具有不同结构的多个对象的混淆。 图6. Cosmos55k。Cosmos55k 包括 55023 张图像，其中 31548 张用于训练，5884 张用于验证，17591 张用于测试。 UNet、DeeplabV3+、SINetV2 和 nnUNet 的定量结果如表 1 所示。从表中可以得出结论，IFE 可以提高网络在大多数细分指标上的性能。 此外，图 7 显示 IFE 有助于模型在大多数模态和解剖结构中表现更好。 图 8 给出了定性比较。IFE 有助于定位可能难以注意到的物体中的结构，并增强对边缘灰度变化的敏感性。IFE 可以在具有挑战性的场景中显著提高基础模型的分割精度。 表1. IFE 的定量比较。+C、 +E 表示基于曲率或信息熵的 IFE。DLV3+ 是 DeepLabV3+。* 表示 DSC 通过 t 检验，p＜0.05。图7. 模态（a）和解剖结构（b）中 UNet（蓝色）和 UNet+E （粉色）的 DSC。Hist、Colo 和 Derm 是组织病理学、结肠镜检查和皮肤镜检查。图8. 配备 IFE 的不同模型的定性比较。红色和绿色分别表示预测和金标准。在将 IFE 应用于不同网络时，选择合适的选择比例 r 至关重要。 不同网络的编码器提取特征的能力并不相同，更有利于分割结果的信道特征的比例也不同。 为了分析 r 的影响，作者使用 UNet 进行了实验。如表 2 所示， r 过大或过小都会导致模型性能下降。 表2. 在 UNet 上关于选择比例 r 的消融研究。 感觉验证实验不够 solid，缺少和其余通道注意力机制的对比实验，不过应该是有补充材料的，后续关注下。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Medical Image Segmentation","slug":"Medical-Image-Segmentation","permalink":"https://chenluda.github.io/tags/Medical-Image-Segmentation/"}]},{"title":"MedSeg | T-Loss：用于医学图像分割的鲁棒损失函数","slug":"Medical_Image_Segmentaion/Loss用于医学图像分割的鲁棒损失函-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/Medical_Image_Segmentaion/Loss用于医学图像分割的鲁棒损失函-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/Medical_Image_Segmentaion/Loss%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E9%B2%81%E6%A3%92%E6%8D%9F%E5%A4%B1%E5%87%BD-Glenn/","excerpt":"","text":"题目：Robust T-Loss for Medical Image Segmentation作者单位：巴塞尔大学、lucerne school of computer science and information technology论文网址：https://arxiv.org/abs/2306.00753论文代码：https://robust-tloss.github.io首次发布时间：2023 年 6 月 1 日 12345678@misc&#123;gonzalezjimenez2023robust, title=&#123;Robust T-Loss for Medical Image Segmentation&#125;, author=&#123;Alvaro Gonzalez-Jimenez and Simone Lionetti and Philippe Gottfrois and Fabian Gröger and Marc Pouly and Alexander Navarini&#125;, year=&#123;2023&#125;, eprint=&#123;2306.00753&#125;, archivePrefix=&#123;arXiv&#125;, primaryClass=&#123;cs.CV&#125;&#125; 主要内容 背景介绍： CNNs 和 ViTs 已成为语义分割的标准方法。 使用这些模型进行监督训练需要大量标注数据，但在医学领域获取标注数据昂贵且耗时。 医学图像标注可能受到人为偏差和标注者一致性差的影响，同时自动挖掘和众包方法获得的标签由于噪声水平较高，其质量也存在挑战。 现有研究： 先前的研究主要集中在标签修正、基于噪声转移矩阵的损失函数修正和鲁棒损失函数等方法。 医学领域的语义分割中关于噪声标签的研究相对较少，主要集中在抗噪声网络架构的开发、领域特定先验知识的结合、在记忆前更新噪声掩模的最新策略等策略。 现有研究的缺陷： 先前的其他方法通常存在一些限制，例如更多的超参数、对网络架构的修改或复杂的训练过程。 相比之下，鲁棒损失函数提供了一个更简单的解决方案，因为只需要对单个建模组件进行简单的改变就可以将其整合进来。然而，先前的鲁棒损失函数在记忆化噪声标签方面存在问题。 方法提出： 提出一种新的鲁棒损失函数 T-Loss，灵感来自于 Student-t 分布的负对数似然。 T-Loss 具有简单的参数形式，能够在反向传播过程中自适应地学习标签噪声的最优容忍度水平。 方法 2.1 问题定义假设 x_i∈\\mathbb R^{c×w×h} 为输入图像， y_i∈\\left{0,1\\right}^{w×h} 是带有噪声的标注二值分割掩码，其中 c 表示通道数， w 表示图像宽度， h 表示图像高度。 给定一组图像 \\left{x_1，…，x_N\\right} 和相应的掩码 \\left{y_1，…，y_N\\right} ，目标是训练具有参数 w 的模型 f_w ，使得 f_w(x) 近似于任何给定图像 x 的精确二值分割掩码。 2.2 为什么选用 Student-t 分布Student-t 分布在处理错误项时具有比通常的高斯形式更大的噪声容忍度。 D 维变量 y 的 Student-t 分布由概率密度函数（PDF）定义： p(y|\\mu,\\Sigma;\\nu)&#x3D;\\frac{\\Gamma\\left(\\frac{\\nu+D}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)}\\frac|\\mathbf{\\Sigma}|^{-1&#x2F;2}(\\pi\\nu)^{D&#x2F;2}\\left[1+\\frac{(\\mathbf{y}-\\boldsymbol{\\mu})^{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{y}-\\boldsymbol{\\mu})}\\boldsymbol{\\tau}\\right]^-\\frac{\\nu+D}{2} 而 D 维变量 y 的 高斯分布由概率密度函数（PDF）定义： p(y | \\mu, \\Sigma) &#x3D; \\frac{1}(2\\pi)^{D&#x2F;2} |\\mathbf{\\Sigma}|^{1&#x2F;2} \\exp\\left(-\\frac{1}{2} (y - \\mu)^T \\mathbf{\\Sigma}^{-1} (y - \\mu)\\right) 其中 μ 和 \\Sigma 分别是相关的多元正态分布的均值和协方差矩阵， Γ 是自由度， |·| 表示行列式。 从此公式可以看出 Student-t 分布的尾部 \\left[1+\\frac(\\mathbf{y}-\\boldsymbol{\\mu})^{T}{\\mathbf{\\Sigma}^{-1}(\\mathbf{y}-\\boldsymbol{\\mu})}\\boldsymbol{\\tau}\\right]^-\\frac{\\nu+D}{2} 遵循幂律，比通常的二次指数形式更重。这意味着， y 离 μ 越远（即，越是异常值），这一项的值越大，从而降低了 y 的概率。但是，由于取了 -\\frac{(ν + D)}{2} 次幂，所以当 y 离 μ 非常远时，这个降低的速度比高斯分布要慢，从而使得 Student-t 分布在处理大的异常值时，比高斯分布有更好的表现，对异常值更具有鲁棒性。 2.3 T-Loss 构建由于通常的均方误差（MSE）损失函数是通过最小化正态分布的负对数似然得出的，应用相同的转换，可以得到以下形式的损失函数：\\begin{array}{c}-\\log p({y}|\\mu,\\Sigma;\\nu)&#x3D;-\\log\\Gamma\\left(\\frac{\\nu+D}{2}\\right)+\\log\\Gamma\\left(\\frac{\\nu}{2}\\right)+\\frac{1}{2}\\log|{\\Sigma}|+\\frac{D}{2}\\log(\\pi\\nu)+\\frac{\\nu+D}{2}\\log\\left[1+\\frac{({y}-\\mu)^{T},{\\Sigma}^{-1}({y}-\\mu)}{\\nu}\\right]\\end{array} 对于每一张图像，损失函数的函数形式可以通过令 y &#x3D; y_i 和近似值 μ &#x3D; f_w(x_i) 得到，并进行汇总，表示为： \\mathcal{L}_\\mathrm{T}&#x3D;\\frac{1}{N}\\sum_{i&#x3D;1}^{N}-\\log p(y_{i}|f_\\mathrm{w}(x_{i}),\\Sigma;\\nu) 方程中的协方差矩阵 Σ 具有 D(D+1)&#x2F;2 个自由参数，需要从数据中进行估计。对于图像来说，这个参数数量可以很容易地达到 10^4 或更大，这使得通常的计算变得非常复杂，并且可能会降低模型的泛化能力。因此，为了简化计算和提高模型的泛化能力，将 Σ 设定为单位矩阵 I_D ，尽管知道图像中的像素注释并不是独立的。对于一张图像，损失函数的项可以简化为： \\begin{array}{c}{-\\log p({\\bf y}|\\mu,{\\bf I}_{D};\\nu)&#x3D;-\\log\\Gamma\\left(\\frac{\\nu+D}{2}\\right)+\\log\\Gamma\\left(\\frac{\\nu}{2}\\right)+\\frac{D}{2}\\log\\left(\\pi\\nu\\right)}{+\\frac{\\nu+D}{2}\\log\\left[1+\\frac({\\bf y}-\\mu)^{2}{\\nu}\\right]}\\end{array} 假设 δ &#x3D; |y_i − f_w(x_i)| ，并固定 ν 的值。 当 δ 接近 0 时，损失函数与 MSE 相关，对小误差敏感； 当 δ 较大时，损失函数与对数函数相关，对大误差的惩罚较小。 因此损失函数对异常值的敏感性可以由参数 ν 调节。 作者使用梯度下降算法同时优化参数 ν 和 w 。对 ν 进行了重新参数化，即 ν &#x3D; e^\\tilde ν + \\epsilon ，其中 \\epsilon 是为了保证数值稳定性而引入的保护项。通过联合优化参数 ν 和 w ，可以调整损失函数对预测误差的容忍度。参数 ν 的调整可以改变损失函数对偏差的惩罚程度，进而影响模型的训练和性能。重新参数化 ν 的目的是为了提高优化过程的数值稳定性，确保损失函数的计算可靠。 结果与讨论 在没有具有真实噪声和干净分割掩码的公共基准的情况下，作者在 ISIC 2017 和 Shenzhen 两个数据集中人为地注入额外的掩码噪声，以测试模型对低注释质量的鲁棒性。 这模拟了注释者因疲劳或其他因素导致错误的真实风险。以概率 α∈\\left{0.3，0.5，0.7\\right} 随机采样一部分训练数据，并应用由 β∈\\left{0.5，0.7\\right}^3 控制的形态学变换噪声。形态学变换包括侵蚀、膨胀和仿射变换，它们分别缩小、扩大和移位注释区域。 表 1 和图 1 展示了在 ISIC 2017 数据集上的实验结果，从中可以得出结论： 传统损失函数在无噪声或低噪声水平下表现良好。 随着噪声水平的增加，传统鲁棒损失函数的性能显著下降。 T-Loss 损失函数在高噪声情况下仍保持较好的性能。 在 ISIC 数据集上，T-Loss 在最极端的噪声场景下获得了较高的 Dice 分数（0.788 ± 0.007）。 表1. 具有不同噪声比率的 ISIC 2017 数据集上的 Dice 分数。这些数值是在最后 10 个 epoch 内的平均分数上，基于 3 个不同的随机种子计算得到的平均值和标准差。图1. 在 ISIC 2017 数据集上，在训练过程中，对比训练集预测结果与真实标注的 Dice 分数，针对每种类型的噪声掩模（α &#x3D; 0.7，β &#x3D; 0.7）。模型在大约前 20K 次迭代后记忆了噪声标签，从而对除 T-Loss 之外的所有损失函数的 Dice 分数产生了负面影响。表 2 展示了在 Shenzhen 数据集实验结果，从中可以得出结论： 在肺部分割任务中，所有鲁棒损失函数在低噪声水平下表现良好。 随着噪声水平的增加，所有鲁棒损失函数的 Dice 分数下降。 T-Loss 始终取得最高的 Dice 分数，即使在最具挑战性的场景中也是如此。 表2. 在 Shenzhen 数据集上，使用不同的噪声比例进行的 Dice 分数实验。这些数值是在最后 10 个 epoch 内的平均分数上，基于 3 个不同的随机种子计算得到的平均值和标准差。图 2 展示了参数 \\tilde ν 初始值在 ISIC 2017 数据集上对结果的影响，从中可以得出结论： 参数 \\tilde ν 在训练过程中动态调整注释噪声的容忍度。 参数 \\tilde ν 的初始值对收敛速度有一定影响，但对最终的Dice分数没有显著影响。 在训练过程中， \\tilde ν 逐渐收敛到一个稳定的解。 图2. 在皮肤病变分割任务中，参数 ν 的行为。左图：在不同标签噪声水平下 ν 的收敛性。中图：对于 α &#x3D; 0.7，β &#x3D; 0.7， ν 初始值的敏感性。右图：Dice 分数对 ν 初始值的敏感性。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Medical Image Segmentation","slug":"Medical-Image-Segmentation","permalink":"https://chenluda.github.io/tags/Medical-Image-Segmentation/"}]},{"title":"MIM | SiamMAE：用于从视频中学习视觉对应关系的 MAE 简单扩展","slug":"Masked_Image_Model/论文阅读李飞飞-MIM-SiamMAE-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/Masked_Image_Model/论文阅读李飞飞-MIM-SiamMAE-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/Masked_Image_Model/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%9D%8E%E9%A3%9E%E9%A3%9E-MIM-SiamMAE-Glenn/","excerpt":"","text":"题目：Siamese Masked Autoencoders作者单位：斯坦福大学、普林斯顿大学论文网址：https://arxiv.org/abs/2305.14344论文代码：未公开首次发布时间：2023 年 5 月 23 日 123456@article&#123;gupta2023siamese, title=&#123;Siamese Masked Autoencoders&#125;, author=&#123;Gupta, Agrim and Wu, Jiajun and Deng, Jia and Fei-Fei, Li&#125;, journal=&#123;arXiv preprint arXiv:2305.14344&#125;, year=&#123;2023&#125;&#125; 主要内容将 MIM 方法从图像扩展到视频以学习对应关系并非易事，原因有二： 类似 MAE 的架构学习的特征专门用于像素重建任务，在微调方面显示出出色的下游性能，但在 zero-shot 设置下传播性能不佳； 视频的时间维度具有独特性。与图像（近似各向同性）不同，视频中的时间维度是有方向性的。这意味着视频数据不能简单地视为图像数据的集合，而应该考虑到时间维度中的连续性和动态变化。 因此，对空间和时间信息进行相同的处理可能并不是最优的选择。 为了解决这些局限性给，作者提出了 Siamese Masked Autoencoders（SiamMAE）：用于从视频中学习视觉对应关系的 MAE 的简单扩展。 SiamMAE 从一个视频中随机选择两帧，将未来帧的 patches （95%）随机屏蔽，而过去帧保持不变； 编码器独立处理这两帧，以提取对应的特征表示； 将特征表示输入由一系列 cross-attention 层组成的解码器，以预测未来帧中丢失的 patches。 对称遮罩的简单扩展方法（即在过去和未来的帧中都进行遮罩）可能会浪费模型的容量，因为它需要模型在低级别的图像细节上花费大量的计算能力。而 SiamMAE 这种不对称遮罩方法（asymmetric masking approach）可以专注于理解和学习高级别的动态信息，例如物体的运动和变化，而不是低级别的图像细节。这可能会提高模型处理视频数据的性能，尤其是在预测未来帧中丢失的 patches 或理解视频中的时间连续性和动态变化等任务上。 方法SiamMAE 的框架如图 1 所示： 图1. SiamMAE 框架Patchify： 给定具有 L 帧的视频，首先随机采样 2 帧 f_1 和 f_2 。这两帧之间的距离是通过从预先确定的帧间隙范围中选择一个随机值来确定的。 然后，将每个帧转换为非重叠的 N×N 个 patches 序列。 最后，将 position embeddings 添加到 patches 的 linear projections 中，并添加 [CLS] tokens。此阶段不使用任何时间位置嵌入。 Masking： 在第一帧（ f_1 ）中不遮罩任何 patches（0%），并在第二帧（ f_2 ）中遮罩非常高的比例（95%）的 patches。通过提供整个过去帧作为输入，网络只需要将过去帧的 patches 传播到未来帧中的适当位置。 为了进一步增加任务的难度，对具有较大时间间隙的两个帧进行了采样。尽管对未来的预测本质上是不确定的，可能产生多个看似合理的结果，但提供少量可见 patches 作为第二帧的输入，可以产生具有挑战性但易于处理的自监督学习任务。 Patchify 和 Masking 的操作可视化如图 2 所示。 图2. Kinetics-400 验证集的可视化（掩蔽率 90%）。对于每个视频序列，先采样一个 8 帧的片段，帧间隙为 4。显示原始视频（顶部） ，SiamMAE 输出（中部），屏蔽未来帧（底部）。使用 f1 作为视频片段的第一帧，f2 作为其余帧。Encoder：SiamMAE 探索了两种 Encoder。 Joint Encoder，将来自两帧的未遮罩 patches 连接起来，然后由标准的 ViT 编码器进行处理； Siamese Encoder，不对称遮罩作为信息瓶颈来独立处理两帧。 Decoder：SiamMAE 探索了三种 Decoder。 Joint Decoder，将来自两帧的所有 tokens 连接起来，然后由标准的 Transformer Block 进行处理； Cross-Self Decoder，这种解码器类似于原始的 Transformer 模型的编码器-解码器设计。每个解码器块由一个交叉注意力层和一个自注意力层组成。来自第二帧的 tokens 通过交叉注意力层关注来自第一帧的 tokens，然后通过自注意力层相互关注； Cross Decoder，这种解码器由只有交叉注意力层的解码器块组成，其中来自第二帧的 tokens 关注来自第一帧的 tokens。 表格结果与讨论如表 1 和图 3 所示，SiamMAE 在视频对象分割（DAVIS-2017）、人体姿态传播（JHMDB）和语义部分传播（VIP）三个下游任务上的性能超过了所有之前的对比学习和自监督对应学习方法。 表1. 在视频对象分割（DAVIS-2017）、人体姿态传播（JHMDB）和语义部分传播（VIP）三个下游任务上模型性能对比图3. 在视频对象分割（DAVIS-2017）、人体姿态传播（JHMDB）和语义部分传播（VIP）三个下游任务上模型性能可视化结果表 2 对 SiamMAE 方法进行了消融研究，以了解每项设计决策对最终结果的影响。这些设计包括：siamese encoder、cross-self decoder、不对称掩蔽比率（asymmetric masking ratio，95%）、帧采样间隔（frame sampling gap，4-48）。 表2. 在 DAVIS-2017 上 SiamMAE 的消融实验FrameMAE： FrameMAE 是 SiamMAE 方法的一种变体，其将 MAE 扩展到视频帧上，即使用 Joint Encoder 和 Joint Decoder，并配合对称的掩蔽比率。如表 2 (a) 所示： 实验结果显示，当掩蔽比率过高（90%）或过低（50%）时，FrameMAE 的性能会显著下降。90% 的掩蔽比率使得学习时序对应关系的任务变得具有挑战性（损失增大），因为可用于学习的 patches 数量不足。 而掩蔽比率为 50% 时，任务变得更容易（损失降低），由于图像的空间冗余，网络可以在不依赖时序信息的情况下重建帧。 采用不对称掩蔽比率的 SiamMAE 效果最佳。 Encoder-decoder design： 编码器-解码器设计是 SiamMAE 的重要设计决策之一。研究表明，不同编码器和解码器的组合会影响模型的性能。如表 2 (b) 所示： 在所有的解码器设计中，Joint Encoder 的性能都显著低于 Siamese Encoder。这可以归因于训练和测试设置之间的差异，因为在测试阶段，每一帧都是独立处理的。 Siamese Encoder 和 Cross Decoder 的组合在所有 Siamese Encoder 组合中性能最差。此外，还观察到，训练损失较高，且重建的帧在空间上不连贯，原因是 f_2 中的所有 patches 都是独立处理的。 Joint Encoder 和 Cross-Self Decoder 的组合性能超过所有其他组合。 Masking： 这里对 Joint Encoder 和 Cross-Self Decoder 的遮罩方案进行了研究。 随机对称遮罩的效果较差，也比对应的 FrameMAE 配置差。 该研究还探讨了网格化的遮罩采样策略，该策略保留每个交替的 patches。这是一个相对容易的任务，因为遮罩模式使网络能够利用并学习空间-时间相关性。虽然我们看到了显著的提升（41.5到48.2），但与 SiamMAE 相比，性能仍然显著低。此外，研究还探讨了不同的不对称掩蔽比率。我们发现一个明显的趋势：将遮罩比率从50%增加到95%会提高性能（49.0%到58.1%）。 Data augmentation： 如表 3(a) 所示，该研究还探讨了不同的数据增强策略的影响。 表3. 在 DAVIS-2017 上 SiamMAE 的数据增强消融实验 与图像和视频领域的研究结果类似，SiamMAE 不需要大量的数据增强就可以达到竞争性的性能。 随机裁剪（缩放范围在 [0.5, 1]）和水平翻转效果最好，而增加颜色抖动则会导致性能下降。 SiamMAE 通过依赖视频中可用的自然数据增强来获得优越的结果。 Frame sampling： 视频数据是丰富的数据增强源，例如姿势、光照视点、遮挡等的变化。为了有效地利用这一点，表 3 (b) 展示了帧采样的重要性。 随着帧采样间隔的增加，性能有所提高。自然视频通常表现出逐渐的时间变化，因此，增加帧间隔导致更强大的自然数据增强，从而提高性能。 Training schedule： 消融研究都是基于 400 epochs 的预训练。图 4 研究了训练 epochs 对 ViT-S&#x2F;16 和 ViT-S&#x2F;8 模型在三个下游任务中的影响。 图4. 训练 epochs 和 patches 尺寸对三个下游任务的性能影响在所有的 patch 尺寸和所有的任务中，随着训练时间的增加和 patches 尺寸的减小，准确度都会逐渐提高。 可视化结果与讨论作者可视化了 ViT-S&#x2F;8 模型的自注意力图。用 [CLS] token 作为 query，并从最后一层的 single head 可视化 ImageNet 的 720p 图像的注意力图。 由图 5 可以看出模型关注对象的边界。例如，它可以清楚地勾勒出标志性的对象（如第一行第一列的羊），多个对象（如第三行第六列的三个棒球运动员），甚至在场景混乱时也能做到这一点（如第二行第四列的鸟）。 图5. ViT-S&#x2F;8 模型的自注意力图","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | MR-MAE：完美利用 MIM 的高级与低级特征","slug":"Masked_Image_Model/论文阅读-MIM-MR-MAE完美利用-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/Masked_Image_Model/论文阅读-MIM-MR-MAE完美利用-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/Masked_Image_Model/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-MIM-MR-MAE%E5%AE%8C%E7%BE%8E%E5%88%A9%E7%94%A8-Glenn/","excerpt":"","text":"题目：Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking作者单位：上海人工智能实验室、香港中文大学论文网址：https://arxiv.org/abs/2303.05475论文代码：https://github.com/Alpha-VL/ConvMAE首次发布时间：2023 年 3 月 9 日 123456@article&#123;gao2023mimic, title=&#123;Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking&#125;, author=&#123;Gao, Peng and Zhang, Renrui and Fang, Rongyao and Lin, Ziyi and Li, Hongyang and Li, Hongsheng and Yu, Qiao&#125;, journal=&#123;arXiv preprint arXiv:2303.05475&#125;, year=&#123;2023&#125;&#125; 主要内容在 MIM 中，常见的观点是 MAE 重建解码器后的底层 RGB 信号，忽视了对编码器高层次语义的监督。为了改善这个问题，大多数现有的解决方案会使用 DINO、CLIP 等高级编码特征来代替像素重建目标。 然而，作者提出了一种名为 MR-MAE 的方法，也就是 ‘Mimic before Reconstruct for Masked Autoencoders’，通过使用预训练的现成特征编码器正则化中间表示来增强 MAE，其目标是在预训练阶段无干扰地同时学习高级和底层表示。 如图 1(b) 所示，将低级和高级监督直接应用于解码器输出会导致语义冲突，因此 MR-MAE 将低级和高级监督分别应用于不同的图像 tokens 和网络层。通过在 ImageNet-1K 上进行微调的 top-1 准确率可以从 83.0% 提高到 85.5%。 图1. 使用 MR-MAE 进行预训练。(a) 原始 MAE 仅重建低级 RGB 像素。(b) 将低级和高级监督直接应用于解码器输出会导致语义冲突。(c) MR-MAE 将低级和高级监督分别应用于不同的图像 tokens 和网络层。通过在 ImageNet-1K 上进行微调的 top-1 准确率可以从 83.0% 提高到 85.5%。 方法MR-MAE 的整体流程如图 2 所示。MR-MAE 将 25% 的可见 tokens 输入到 Transformer 编码器中，以得到中间表示 E_v 。这与仅依赖于解码器进行低级重构的方法有所不同，MR-MAE 从编码器中提取的丰富的高级语义特征，如 DINO 或 CLIP 来引导中间表征 E_v ，如图 2 (2) 所示。 在此过程中，图像首先输入到预训练的编码器中，以提取 DINO 或 CLIP 特征，记为 F^D_v，F^C_v\\in\\mathbb R^{l_v×C} 。然后，在编码器上增加一个特性模拟头（mimic head），即一个线性投影层，使得可见表示 E_v 去 mimic F^D_v 或 F^C_v 。最后，MR-MAE 的 L2 模拟损失（mimic loss）定义为：\\mathcal L_{\\mathcal M}&#x3D;\\frac{1}{l_v}||L(E_v)-F_v||^2_2 ，其中 L 表示 mimic head。 另一方面，在 MR-MAE 中，为了整合低级和高级信息，采用了一个轻量级的解码器来重构 75% 的被掩蔽的 RGB 像素，如图 2(3) 所示。 在 MR-MAE 中，特征模拟损失 \\mathcal L_{\\mathcal M} 和掩码 tokens 的重建损失 \\mathcal L_{\\mathcal R} ，各自针对输入图像的高级语义和低级纹理进行编码。这两种损失函数能够相互补充，协同促进更具辨别性的表征学习。 另外，为了避免低级和高级目标之间的学习冲突，MR-MAE 通过对不同的 tokens 组（25% 可见对 75% 掩蔽）和不同的网络层（编码器和解码器的输出）应用监督。 通过新引入的高级特征模拟损失，MR-MAE 能够显著提升 MAE 的下游性能，并缩短其预训练的 epoch。 图2. MR-MAE 的架构。在 MAE 预训练期间，分别为不同的图像 tokens 和网络层设置高级和低级学习目标：编码器 25% 可见 tokens 的 mimic loss，以及解码器的 75% 掩码 tokens 的重建损失。MR-MAE 中还有些 tricks： Focused Mimicking：MAE 采用随机掩蔽策略来选择可见 tokens，这种方式本质上是对低级信号重构的一种自然选择，不依赖额外的引导。而在预训练模型中，[CLS] token 可以通过其注意力图清晰地标识出重要区域。因此，作者在教师网络的注意力图中挑选出最显著的 tokens，用于模拟可见特征。通过这种方法，MR-MAE 可以更有效地捕获教师网络中编码的高级语义信息，而不是非显著的低级语义。 Multi-layer Fusion：在原始的 MAE 中，只有编码器最后一层的输出 tokens 被送入解码器，用于被掩蔽像素的重建。然而，编码器的不同层可能在不同的抽象级别上描述图像。因此，作者采取策略性地结合来自编码器多个中间层的可见 tokens，然后使用这些组合的 tokens 进行高级特征的模拟和低级像素的重建。通过这种方式，特征模拟的监督可以直接应用到编码器的多个层面，从而优化视觉表征。 Masked Convolution Stages：作者在 transformer 块前添加了额外的掩蔽卷积层，以便有效地捕捉高分辨率的细节，并应用了多尺度的逐块掩蔽，以防止像素重建过程中的信息泄露。这种多尺度编码能够学习到层次化的表示，并在下游任务中实现显著的改进。 结果通过表 1 可以看出： MR-MAE 在重构低级像素和使用各向同性（isotropic）结构上超过了 BeiT, MAE, CAE 等 MIM 方法的性能（85.8% 对 83.0&#x2F;83.6&#x2F;83.6&#x2F;84.0%）； 采用多尺度特征的 SimMIM，MCMAE 和 MixMIM 虽然提高了微调精度，但由于它们仍然重构低级信号，MR-MAE 在预训练周期更少的情况下仍然能够超过它们的微调精度（85.8% vs 84.0&#x2F;85.0&#x2F;85.1%）； 有一些研究尝试用高级语义目标直接替代低级信号的重构，如 MaskFeat，data2vec，MVP 和 MILAN 等。尽管这些模型展现出了一些有前景的结果，但 MR-MAE 在多尺度架构中联合学习低级和高级目标，仍然获得了更好的性能（85.8% vs 85.4&#x2F;85.5%）； 不同于 CMAE 的对比损失和重构损失的联合优化，MR-MAE 使用了一个含有丰富语义知识的预训练教师模型，提高了 Top-1 准确性，并缩短了预训练周期； DMAE 模型尽管采用了与 MR-MAE 类似的方法，但由于其教师模型仍然以低级像素为目标进行预训练，因此其微调精度不如 MR-MAE（84.0% vs 85.8%）。 表1. 对 ImageNet-1K 进行微调。&#39;Ratio&#39; 表示输入编码器的图像 tokens 的可见比率； &#39;P-Epochs&#39; 和 &#39;FT&#39; 表示预训练 epoch 和通过微调获得的 Top-1 精度。从表 2 可以看出： 将 MR-MAE 用作 Mask-RCNN 的预训练 backbone，经过在 COCO train2017 上进行 25 个 epoch 的微调，获得了 53.4% 的 AP^{box} 和 46.9% 的 AP^{mask}。 比于使用了 1600 个 epoch 预训练的 MAE 编码器的基准 ViTDet，MR-MAE 可以提高 AP^{box} 和 AP^{mask} +2.2% 和 +1.4%。 与 CMAE 和 MCMAE 等多尺度主干网络相比，MR-MAE 在预训练周期大大缩短（400 对 1600）的情况下，实现了可比的 AP^{box} 和 AP^{mask}。 表2. 基于 Mask RCNN 框架对 COCO 进行微调表 3 展示了 MR-MAE 的“’mimic before reconstruct”和 bag-of-tricks 的消融研究，可以得出： 基线 MAE 模型使用低级重构损失，在 200 个预训练周期的 ImageNet1K 上达到了 83.0% 的微调精度。通过联合模仿损失学习，分类精度提高了+1.7%。同时优化低级和高级目标可以比仅模仿高级语义获得更好的性能，特别是对于物体检测的 AP^{box} 提升了 0.7%。 在 \\mathcal L_{\\mathcal R} 和 \\mathcal L_{\\mathcal M} 的微调精度为 84.7% 的基础上，Focused mimicking 提升了精度+0.2%，得益于教师网络的注意力图引导的显著 tokens 的关注。Multi-layer Fusion 进一步提高了精度 +0.1%。引入 Masked Convolution Stages 可以将 ImageNet-1K 的微调精度提高 +0.5%，并且提高了 AP^{box} 和 AP^{mask} +1.4% 和 +1.0%，这表明了多尺度架构的重要性。 低级和高级目标包含不同的视觉语义，它们的联合监督可能会产生冲突。低级和高级目标的联合重构使 ImageNet-1K 的微调精度下降1.7%，AP^{box} 下降 1.3%，AP^{mask} 下降 0.6%。Mimic-before-Reconstruct 框架通过在不同的 tokens 组和不同的网络层上应用模仿和重构损失，解决了低级和高级目标之间的冲突。 表3. MR-MAE 的“&#39;mimic before reconstruct”和 bag-of-tricks 的消融研究。表 4 展示了在 ImageNet-1K 上使用高级预训练目标的微调精度的消融研究，可以得出： 由 CLIP 生成的特征比 DINO 的微调精度高 1%，这表明图像-语言对比学习提供的高级语义比图像-图像对比学习更强； 多个高级信号的联合模仿比独立模仿效果差，我们认为这是由于预测不同高级目标的梯度冲突引起的。所以作者分别对 MR-MAE 进行预训练和微调，然后集成两个模型，以避免由重构不同高级目标的冲突引入的性能下降； CLIP&#x2F;DINO（分离）可以比 CLIP&#x2F;DINO（联合）高 1.7%，这验证了与不同目标学习的互补表示。 表4. 在 ImageNet-1K 上使用高级预训练目标的微调精度的消融研究。表 5 展示了预训练 epoch 对 ImageNet-1K 和 COCO 影响的消融研究，可以得出： 200 个周期的预训练可以达到 85.5% 的 ImageNet1K 微调精度和 COCO 的 52.7% 的 AP^{box}。 MR-MAE 在预训练 400 个周期后，ImageNet-1K 的微调精度提高了 0.3%，AP^{box} 提高了 0.7%。 当预训练周期延长到 800 个周期时，性能趋于饱和，这说明引入高级目标可以使 MIM 方法更快地收敛。 在 Mimic-beforere-construct 框架下，原本需要 1600 个预训练周期的计划可以缩短到 400 个周期。 表5. 预训练 epoch 对 ImageNet-1K 和 COCO 影响的消融研究。表 6 测试了 MR-MAE 的可扩展性，可以得出：与单尺度的 baseline MAE 和更强大的多尺度 baseline MCMAE相比，MR-MAE 在所有模型大小上的性能都有显著提升，且预训练周期大大缩短。 表6. 不同模型尺度的 ImageNet-1K 微调精度。同时，作者通过可视化了不同模型的注意力图来解释为何高级目标能提升模型表现。如图 3 所示，MAE 的注意力偏向于纹理模式，因其旨在重构低级像素，这意味着 MAE 在与语义理解无关的低级纹理上浪费了能力。而 DINO 的 [CLS] token 的注意力过于强调突出物体的部分信息。相比之下，MR-MAE 的注意力能够捕获完整的物体信息。 图3. DINO、MAE 和 MR-MAE 最后一个自注意层的注意力权重可视化。与以前的方法相比，MR-MAE 可以更好地捕获显着特征表示。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | 理论：MIM 的数据可扩展性研究","slug":"Masked_Image_Model/论文阅读-MIM-理论MIM-的数据可-Glenn","date":"un33fin33","updated":"un33fin33","comments":true,"path":"2023/06/07/Masked_Image_Model/论文阅读-MIM-理论MIM-的数据可-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/06/07/Masked_Image_Model/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-MIM-%E7%90%86%E8%AE%BAMIM-%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8F%AF-Glenn/","excerpt":"","text":"题目：Delving Deeper into Data Scaling in Masked Image Modeling作者单位：字节跳动论文网址：https://arxiv.org/abs/2305.15248论文代码：未公开首次发布时间：2023 年 5 月 24 日 12345678@misc&#123;lu2023delving, title=&#123;Delving Deeper into Data Scaling in Masked Image Modeling&#125;, author=&#123;Cheng-Ze Lu and Xiaojie Jin and Qibin Hou and Jun Hao Liew and Ming-Ming Cheng and Jiashi Feng&#125;, year=&#123;2023&#125;, eprint=&#123;2305.15248&#125;, archivePrefix=&#123;arXiv&#125;, primaryClass=&#123;cs.CV&#125;&#125; 主要内容使用 Coyo-700M 替代常用的 ImageNet（手动处理和以对象为中心） 数据集来探索 MIM 的数据缩放能力（可扩展性），实验结果表明： 1）当训练数据的规模相对较小时，MIM 可以被视为提高 model capacity 的有效方法； 2）更强的重建目标可以增加模型在下游任务上的性能； 3）MIM 预训练在大多数情况下与数据无关，这意味着采样预训练数据的策略并不重要。 方法与结果如图 1 所示，作者基于 MAE 设计了一个 MIM 基本框架，将 MAE 的重建目标像素值换成目标编码器的输出，此时，重建损失为： {\\mathcal L}_{r}&#x3D;\\frac{\\left(\\overline{y}_{c l s}^{t}-\\overline{y}_{c l s}^{s}\\right)^{2}+\\sum_{i&#x3D;1}^{N}(\\overline{y}_{i}^{t}-\\overline{y}_{i}^{s})^{2}}{N+1} 其中， \\overline{y}^{t} 和 \\overline{y}^{s} 表示目标编码器和解码器的 l_2 归一化输出。 图1. MIM 基本框架 不同的重建目标研究首先使用 ImageNet-1k 和 ImageNet-22k 作为预训练数据集来研究不同重建目标带来的影响。 选择以纯 MIM 方式进行预训练的 MAE、以纯基于增强的对比学习方式进行预训练的 DINO、在 MIM 和对比学习的组合下进行预训练的 CMAE 和使用语言辅助进行预训练的 CLIP 等自监督方法作为目标编码器来生成重建目标。使用 RGB 图像像素值作为目标的原始 MAE 被当作 baseline。结果如表 1 所示。 表1. 使用不同目标编码器的不同下游任务的结果。当预训练数据集从 ImageNet-1k 更改为 ImageNet-22k 时，这导致图像数量从 ~1M 增加到 ~14M，我们可以观察到下游任务的性能显着提高，尤其是在密集预测任务上。例如，当 CMAE 被用作 ImageNet-22k 上的目标编码器时，与在 ImageNet-1k 上预训练的模型相比，在 AP^{box} 和 AP^{mask} 方面可以获得 ~0.6% 和 ~0.6% 的性能增益。 在大多数情况下，使用 CLIP 预训练的模型会产生更好的结果。例如，在 ImageNet1k 上，模型在 ImageNet-22k 数据集上进行预训练时可以达到 84.77% 的 Top-1 准确率，比 baseline 高 1.87%（84.77% 对 82.90%）。 此外，在预训练期间使用 CLIP 作为目标编码器的模型表现出更好的数据缩放能力。具体来说，在 CLIP 的辅助下，在 ImageNet-1k 上的性能提高了 84.40%→84.77%(+0.4%)，在 CityScapes 上的性能提高了80.47%→81.72%(+1.25%)。以上实验结果表明，CLIP 是 MIM 预训练的强目标编码器，因此作者选择CLIP 作为目标编码器进行以下实验。 预训练数据集规模研究ImageNet-1k 数据集是以对象为中心的，这与现实场景不一致。因此，作者选择包含大规模信息图像-文本对的 Coyo-700M 作为预训练数据集进行进一步研究。 图 2 中可视化了不同大小的预训练数据集对不同下游任务的影响，定量结果如表 2、表 3 和表 4 所示。 图2. 使用不同大小的预训练数据集在不同下游任务上的微调性能。下游任务包括 ImageNet-1k 分类、COCO 对象检测、ADE20K 语义分割和 CityScapes 语义分割。从图 2 中，我们可以很容易地观察到，微调性能在五个下游任务上都会饱和。 当预训练数据集的大小从 0.5M 增加到 1M 时，模型的性能突然提高。 当预训练数据集的大小在 1M 到 10M 的范围内时，在大多数情况下仍然可以持续提高性能，但性能饱和的迹象开始出现。 最后，当预训练数据集的大小达到 100M 时，大多数下游任务的性能几乎没有提高，在某些情况下性能下降。 我们可以得出结论，当预训练数据集的大小限制为 10M 时，该模型具有很强的数据可扩展性。然而，随着预训练数据集的大小继续增加，基于 MIM 的预训练很难为模型提供可扩展性。 此外，对于 ViT-H 模型，我们观察到它有时会产生比 ViT-L 模型更差的结果，尤其是当使用小数据集进行预训练时。例如，当使用 0.5M 图像进行预训练时，ViT-H 在 ImageNet-1k 上实现了 82.00% 的 Top-1 精度，这比ViT-L 模型低了近 0.2%。作者推测，庞大的模型仍然需要大规模的预训练数据集来实现更好的性能，但它们仍然无法突破性能饱和的极限。 我们还在 ImageNet-1k 上采用线性探测来评估模型是否可以用更大的预训练数据集进行扩展。线性探测结果如表 2 所示。 表2. ImageNet-1k 上的微调和线性探测结果。模型在从 Coyo-700M 随机采样的不同大小的数据集上预训练模型在线性探测下，当预训练数据域与验证集不同时，预训练数据集的规模起着重要的作用。当预训练数据大小较小时，学习表示和验证集之间存在差距，导致性能不佳（例如，预训练数据集规模为 0.5M 时，ViT-H 达到 36.43%）。随着尺寸扩大到 5M，线性探测的性能急剧增加，达到了 20% 以上的准确率增益。尽管如此，当预训练数据达到 100M 时，MIM 预训练显示出有限的性能改进。 表3. Microsoft COCO 上的对象检测和实例分割结果表4. ADE20K 和 CityScapes 上的语义分割结果 预训练数据集质量研究如表 2 所示，使用 Coyo-1M 进行预训练最终在 ImageNet-1k 上实现了 83.17% 的 Top-1 准确度，远低于使用 ImageNet-1k 预训练的模型（83.17% vs. 84.40%），可以推测使用来自同一域的数据进行预训练和验证会导致性能差距，预训练数据的质量起着重要作用。 为了评估数据缩放能力是否受到预训练数据质量的影响，作者使用 CiT 中提出的采样策略，而不是随机抽样从 Coyo-700M 中选择图像进行预训练。 CiT 测量元数据和原始数据之间的文本嵌入的相似性，并以在线方式选择与感兴趣的任务相关的训练数据。作者采用 EVA 中的文本编码器，并比较了 Coyo-700M 数据集提供的文本描述与来自 ImageNet-1k 的类标签之间的相似性。设置不同的阈值以离线方式对不同尺度的预训练数据集进行采样。 如表 5 所示，CiT 采样策略不会导致任何性能改进。 表5. 微调 ViT-L&#x2F;16 并报告不同数据采样策略的结果。结果以“A&#x2F;B”格式显示，其中“A”表示随机采样，“B”表示 CiT 中提出的策略。 假设 MIM 预训练是数据不可知的，这意味着预训练数据是否简单或复杂不会影响性能。 同时，“更好”的数据采样策略不会改变数据缩放的趋势。随着 CiT 的采样策略，ViT-L 可以获得从 Coyo-1M 到 Coyo-10M（即 84.59% → 86.20%）的 ImageNet-1k 的性能增益，而当大小增长到 100M 时，性能几乎冻结（即 86.20% → 86.22%）。 更难的下游任务研究为了探索下游任务是否限制了大规模预训练模型的容量，作者尝试在“更难”的下游任务上构建和评估 MIM，包括 ImageNet-5k 上的分类、iNaturalist2018 上的长尾分类、LVIS 上的长尾对象检测。结果如表 6 和表 7 所示。 从表 6 中可以看出，在包含比 ImageNet-1k 更多类的 ImageNet-5k 上，可以很容易地发现： 表6. 对“更难”下游任务进行微调或使用更强的目标编码器的结果。 当数据大小扩大到 10M 时，微调性能会迅速提高（59.09% 对 58.01%）； 然而，当数据大小扩大到 100M 时，模型仅实现了约 0.3% 的性能提升（59.09% 对 59.38%）。 这种现象与在 ImageNet-1k 上微调时观察到的趋势一致。 此外，还在 iNaturalist2018 细粒度图像分类上评估 VT-L&#x2F;16，并观察到类似的结论。 请注意，在表 6 中，使用 Coyo-5M 进行预训练的性能甚至超过了使用 Coyo-10M 的性能（81.09% 对 80.61%）。作者认为，主要原因是 Coyo-5M 预训练的模型经过更多的迭代训练。 图 7 报告了 LVIS 的结果，这是一个具有挑战性的数据集，具有大词汇量对象类别以及高质量的实例掩码。与 COCO 不同，LVIS 包含大量稀有类别。 图7. 在 LVIS 验证集上评估的 ViT-L&#x2F;16 的定量结果。“r”、“c”和“f”分别代表“rare”、“common”和“frequency”。MIM 预训练很少在稀有类别上表现出很强的数据可扩展性。 从表 7 中，可以观察到使用更大的数据集进行预训练，该模型可以显着提高性能。具体来说，ViT-L 使用预训练的 Coyo-100M 实现了 47.30% 的最佳 APbox，比使用 Coyo-10M 预训练的模型的性能高 0.98%（47.30% vs. 46.32%）。 此外，性能增益主要来自稀有类别。当数据集的大小从 10M 增加到 100M 时，稀有类的性能提高了 3% 以上（36.51% → 39.83%），而频繁类的性能仅增加了不到 1%（50.93% → 51.59%），这表明大规模数据预训练可能有助于长尾目标检测以及实例分割。 更强的目标编码器研究使用 VT-B&#x2F;16 作为目标编码器是对训练成本的折衷，这可能会限制模型的数据可扩展性。因此，作者还尝试了更大的目标编码器进行重建，包括来自 CLIP 的 VT-L&#x2F;14（650M 参数），以及来自 EVA 的 EVA-G&#x2F;14（1B 参数）。结果列于表6。 当使用 100M 数据进行预训练时，使用 CLIP-L&#x2F;14 实现了 86.66% 的 Top-1 准确度，这比使用 CLIP-B&#x2F;16 预训练的模型高出约 0.4%。但是，这可能是由于 patch 大小的差异造成的。 在这里，作者使用更强的目标编码器 EVA-G&#x2F;14，大约有 1.0B 参数，用于重建，它达到了 86.94% 的 Top-1 准确度，比使用 CLIP-L&#x2F;14 预训练的模型高约 0.3%（86.94% 与 86.66%）。 因此，作者认为使用更强大的目标编码器可能有助于增加模型的容量。 然后，为了调查较长的预训练是否有助于提高性能，作者使用 100M 数据预训练具有 10 个 epoch 的编码器。该模型达到 86.90%，与用 15 个 epoch 预训练的模型相似（86.90% 对 86.94%）。 换句话说，当性能趋于饱和时，即使使用更长的预训练时期，也很难提高性能。 最后，作者使用包含 ~1B 参数的 EVA-G&#x2F;14 作为目标编码器来探索以 MIM 方式预训练的编码器是否在数据上可扩展。 不幸的是，获得了相同的结论，即当模型大小固定时，使用 MIM 的模型很难在更多的预训练数据上进行扩展。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"Make Something Wonderful 全书翻译：Steve 关于童年和青年时期的回忆（1）","slug":"书籍/Steve关于童年和青年时期的回忆","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2023/05/13/书籍/Steve关于童年和青年时期的回忆/","link":"","permalink":"https://chenluda.github.io/2023/05/13/%E4%B9%A6%E7%B1%8D/Steve%E5%85%B3%E4%BA%8E%E7%AB%A5%E5%B9%B4%E5%92%8C%E9%9D%92%E5%B9%B4%E6%97%B6%E6%9C%9F%E7%9A%84%E5%9B%9E%E5%BF%86/","excerpt":"","text":"写在前面如今，大型人工智能模型（如 GPT-4）成为科技界的热门话题。这些强大的算法正在逐渐改变我们的生活方式，不仅在科技领域，还影响到商业、教育、医疗等众多行业。大厂们纷纷投入巨资，研发和优化这些模型，以争夺市场份额。在这个人工智能浪潮中，有时我不禁会思考，如果乔布斯（Steve Jobs）先生还在世，他会如何看待这种现象呢？ 史蒂夫·乔布斯档案馆（Steve Jobs Archive）于 4 月 11 日在线发布了一本基于史蒂夫·乔布斯的电子邮件和对话的新电子书：《Make Something Wonderful: Steve Jobs in his own words》，并免费开放给大众阅读。 这本书包含了大家熟悉的照片，详细介绍了乔布斯生活中的一些知名时刻，以及没有被公众看到过的电子邮件、对话和照片。这包括他对自己童年的回忆，对创建和被排挤出苹果公司的看法，他在皮克斯和 NeXT 的时期发生的趣事，以及他的复出并逐步走向巅峰的故事。 这部巨著由史蒂夫·乔布斯档案馆执行主任 Leslie Berlin 编辑，并由 Laurene Powell Jobs[1] 作为顾问。 原文查看：Make Something Wonderful There’s lots of ways to be, as a person. And some people express their deep appreciation in different ways. But one of the ways that I believe people express their appreciation to the rest of humanity is to make something wonderful and put it out there.And you never meet the people. You never shake their hands. You never hear their story or tell yours. But somehow, in the act of making something with a great deal of care and love, something’s transmitted there. And it’s a way of expressing to the rest of our species our deep appreciation. So we need to be true to who we are and remember what’s really important to us.— Steve, 2007 引言这段文字由 Laurene Powell Jobs 编写 了解一个人最好的方式是直接倾听他的声音。要了解 Steve，最好的方法就是聆听他一生中所说所写的内容。他在演讲、采访和电子邮件中的话语为我们提供了了解他思维方式的窗口。他是一位杰出的思想家。 这些页面中的很多内容都反映了 Steve 一生的指导原则：他对艺术与科技结合所能带来的新世界的洞察；他会严格要求自己，展现出难以置信的严谨；他在组建和领导伟大团队的执着追求；也许，最重要的是，他对人性的深刻见解。 Steve 曾经告诉一群学生：“你们会突然出现，并有机会在天空中绽放光芒，然后就会消失。”他对如何充分利用我们短暂的时间进行了大量的思考。他热衷于成为人类历史长河中的一部分，怀着我们每个人都有可能推动或加速人类进步的信念。 要看清现有的事物已经足够困难，想要获得清晰的视角更为艰难。Steve 在这方面的天赋卓越：他看清了那些不存在的东西，可能存在的东西，必须存在的东西。他的思想从未被现实所束缚。恰恰相反，他设想了现实所缺乏的东西，并着手弥补它。他的想法并非来自争辩，而是直觉，源于真正的内心自由和无尽的可能性。 在这本书中，由 Steve 起草并修改。他跌跌撞撞，不断成长，不断改变。但他始终保持对无限可能的信念。我希望这些选文能激发你对他的理解：构成我们称之为生活的一切都是由和我们一样聪明、一样有能力的人创造的；我们的世界并非一成不变——因此我们可以让它变得更美好。 编辑：Leslie Berlin 出版：史蒂夫·乔布斯档案馆（Steve Jobs Archive） 为了内容清晰和隐私，原文经过了编辑和摘录。 ✂ 表示从原文中删除了几个句子或段落。 两岁的 Steve。他后来称计算机为“思维的自行车” 前言：Steve 关于童年和青年时期的回忆Steve 通常对自己的私人生活保持低调，但他偶尔会谈论在旧金山湾区的成长经历。那时，工程师和程序员开始涌入后来被称为硅谷的地区。 1995年，他在史密森尼博物馆留下了一段口述内容。我很幸运有一位相当了不起的父亲，名叫保罗。他没有完成高中学业，在二战期间加入了海岸警卫队，曾在巴顿将军的指挥下为部队运送兵力。我想他应该是因为总惹麻烦，所以被降级到列兵。他的本职工作是机械师，工作非常努力，手非常灵巧。 他在车库里有一个工作台，当我五六岁的时候，他在工作台上划出一小块区域，告诉我：“Steve，这现在是你的工作台。”他给了我一些较小的工具，教我如何使用锤子和锯子，如何建造东西。这对我来说真的非常有益。他花了很多时间教我如何建造东西，拆卸东西，然后再重新组装东西。 电子产品是其中之一。他自己并没有深入的了解，但在修理汽车和其他东西时接触过很多电子设备。他向我展示了一些电子设备的基本原理，我对此产生了浓厚的兴趣。 我是在硅谷长大的。我五岁的时候，我的父母从旧金山搬到了山景城。我父亲调职了，而那正是硅谷的核心地带，周围都是工程师。那时候，硅谷大部分地区仍然是果园，杏子园和梅子园，真的是天堂。我记得几乎每天的空气都是清澈的，你可以从山谷的一头看到另一头。那真是世界上最美好的成长环境。 有个新住户带着他的妻子搬到了街区的尽头，离我家大约六七户的距离。后来发现，他是惠普公司的一名工程师，也是一名业余无线电操作员，非常热衷于电子产品。为了结识街区的孩子们，他采取了一种相当奇特的方式：在他的车道上放置一个碳麦克风、一块电池和一个扬声器，你可以对着麦克风说话，然后扬声器会放大你的声音。当你搬到一个新社区时，这是一种很奇怪的做法，但他就是这么做的。✂ 后来我认识了这个叫做 Larry Lang 的人，他教了我很多电子知识。他过去常常自己组装 Heathkits[2]。Heathkits 是一种以套件形式出售的产品，会附带详细的手册，告诉你如何将这个东西组装起来，所有的零部件都会以某种方式和颜色编码排列。你需要亲自组装这个东西。实际上购买这些套件要比直接购买成品要贵。 这给我带来了一些收获。首先，它让我们了解了成品内部的构造以及其工作原理，因为它的手册中会包含详细的操作原理。但也许更重要的是，它让我们意识到，我们可以建造周围宇宙中所看到的所有东西。这些东西不再是神秘的了。我的意思是，当你看着一台电视机时，你会想，“我还没造过那个，但我能造。Heathkits 的目录里有一个，我已经组装过两个其他的 Heathkits，所以我能造一台电视机。”事物变得更加清晰，它们是人类创造的结果，而不是那些神奇的东西突然出现在我们身边，而我们对它们的内部一无所知。这给了我们极大的自信，通过探索和学习，我们可以理解周围环境中看似非常复杂的事物。我的童年在这方面是非常幸运的。✂ 一开始上学对我来说是件相当艰难的事。我母亲在我上学前教会了我阅读，所以当来到学校时，我真的只想做两件事：读书，因为我喜欢读书；到外面追蝴蝶。你知道，五岁孩子都喜欢做的事情。我遇到了前所未有的权威，我不喜欢它。他们差点把我的好奇心扼杀掉。 到了三年级，我有一个好哥们，Rick Ferrentino，我们唯一的乐趣就是制造恶作剧。我记得当时学校里有一个大自行车架，大家都把自行车停在那里，可能有一百辆自行车在那个架子上。我们一个一个地把他们的锁组合交换，有一天，我们把每个人的锁放在别人的自行车上，直到晚上大约十点钟他们才把所有的自行车整理好。我们还在老师的桌子上引爆了炸药。所以我们总是被赶出学校。 到了四年级，我遇到了我生命中的另一位圣人： Hill 老师。当他们打算把我和 Rick Ferrentino 安排在同一个四年级的班级里，校长在最后一刻说：“不，不是个好主意，把他们分开。”于是， Hill 女士说：“我来教一个。”她教的是四年级学生的高水平课程，感谢上帝，我被随机放进了这个班里。她观察了我大约两周，然后找到我。她说：“Steve，我跟你说个事，我跟你打个赌。我这有本数学练习册，如果你把它带回家，在没有任何帮助的情况下自己完成，然后把它带回来给我，如果你做对了 80%，我会给你五美元和一根很大的棒棒糖。”她买了一个棒棒糖，把它伸到我面前，对当时的我来说它真的很大。 我看着她，心想：“你疯了吗，女士？从来没有人这么做过！”当然，我做到了。她基本上是用糖果和金钱把我重新引导回学习的道路。真正值得注意的是，没过多久，我就对她产生了如此高的敬意，这在某种程度上重新点燃了我的学习欲望。她还给了我一些制作照相机的套件。我自己磨了一个镜头，制作了一个照相机。那真是太美妙了。我想我在那一年里学到的学术知识可能比我一生中学到的还要多。 1984年，Steve 与记者 David Sheff 聊天，讨论了在年轻时，他和他的那一代人是如何开始形成自己的文化观念的。我的父母从来没有强迫我上大学，但他们总是想确保，如果我想去，他们有足够的资源让我去。他们省吃俭用，真的为我攒了一些钱（供我上里德学院），但六个月后，把他们的积蓄花在让我上大学这件事上似乎真的很荒谬。 我对自己想做什么还不够了解，另外，我觉得我退学后，还可以重新回到学校上课，学到同样多的东西。所以我在六个月后退学了，然后又回去上了一年多的课。 我在那里待了大约一年半时间，也许接近两年。我非常喜欢那里。那是我生命中一段艰难的时期，但我很享受。我不知道我想做什么。里德是一个竞争非常激烈的地方，那里有很多聪明的人，每个人都想改变世界，但都不知道该怎么做。✂ 20世纪70年代初，东方神秘主义风潮席卷了美国。我们在里德学院见到了一大批人，从 Timothy Leary 和 Richard Alpert 到 Gary Schneider 等人。因此，关于生命和存在的真相，那里始终充满了智力的探讨[3]。✂ 60 年代的理想主义之风仍在我们身后，我认识的大多数和我同龄的人都有这种根深蒂固的想法。他们心中有理想主义，但他们也对一些事情持谨慎态度，比如担心在 45 岁时会在一家天然食品店的柜台后工作，这就是他们看到一些年长朋友在做的事情——并不是说这本身就很糟糕，但如果这不是你真正打算做的或你真正想做的，那就很糟糕。 因此，理想主义形成了，但同时也有一种感觉，实现这些理想主义必须有更成功的方式。 Steve 回忆起离开里德学院后在加州和印度的时光。我回到了（旧金山湾区），因为我决定我想去旅行，但我缺乏必要的资金。 这里是加州。你可以从斯坦福大学那里得到新鲜制作的迷幻药。晚上你可以和你的女朋友以及其他有意义的人一起睡在海滩上。你可以……直到我去了那些地方，我才真正意识到加州与美国中部，甚至在某种程度上与东海岸有多么不同。直到二十出头，我才去过这些地方。加州有一种实验精神和开放精神，一种开放和新的可能性，直到我去了其他地方，我才真正意识到这一点。 所以我回到这里找工作，我看了看报纸，上面有一则广告说成为一名工程师，还能同时玩得开心。听起来很有趣，所以我打了电话。这是（电子游戏制造商）Atari。我填了一份申请表，列出了我做过的所有事情，人事部的女士说：“好吧，不要给我们打电话，我们会给你打电话！”但后来，巧合的是，我的申请表被一位名叫 Al Alcorn 的人看到了，他当时是 Atari 的工程副总裁。他第二天给我打了电话，聘请了我，那感觉太棒了。我在那里待了不到一年，他们把一批游戏机运到欧洲，但里面有一些工程缺陷。我找到了修复它们的方法，但需要有人过去亲自修理。 所以我自愿去了；他们问我是否愿意去，我说我非常愿意，但希望在那里休假。他们答应了我的要求，我最后来到了瑞士，从苏黎世飞到新德里。然后我在印度呆了一段时间。 我很难概括（我在印度的旅程）。任何人都很难在一页纸上总结他们一生中有意义的经历。我的意思是，如果我是 William Faulkner[4]，我也许能为你做到，但我不是。 回来的文化冲击比去的时候要大。在回到加州后，我真正想做的就是找一个草地，坐下来。我不想开车。我不想去旧金山或者做其他事情。我不想做。 所以，我在大约三个月里都没有这样做。我只是坐着和阅读。当你在一个地方成为陌生人时，你会注意到那些当你变得熟悉时会迅速停止注意的事情。我人生中第一次成为美国的陌生人，所以我看到了以前从未见过的事物。在那三个月里，我试图关注这些事物，因为我知道，逐渐地，我的熟悉感会再次恢复。 Steve 的护照，1973 年春天，他十八岁1974 年送给朋友的一首诗：“不要浪费你的生命。” 下一节： Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（2.1） | 1976–1985 参考 ^乔布斯先生的遗孀。 ^Heathkit 是一家美国公司，曾经生产和销售电子组装套件，允许用户自行构建电子设备，如收音机，电视机，电台，计算机等。该公司成立于 1926 年，曾是美国电子爱好者和学生的热门选择，但在数字电子技术的兴起和成熟后，该公司的业务逐渐衰退，最终于 1982 年停止经营。 ^20 世纪 70 年代初，东方神秘主义开始在美国流行。当时，里德大学成为了一个重要的停留地，涌现了大量思想家和学者，例如 Timothy Leary，Richard Alpert，Gary Schneider 等等。这些人在里德大学不断地探讨生活和存在的真理。这场思想革命，对当时美国社会产生了深远的影响，推动了许多人对生活和意义的不断探索。 ^美国文学史上最具影响力的作家之一，意识流文学在美国的代表人物，1949年诺贝尔文学奖得主，获奖原因为“因为他对当代美国小说做出了强有力的和艺术上无与伦比的贡献”。","categories":[{"name":"书籍","slug":"书籍","permalink":"https://chenluda.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"Steve Jobs","slug":"Steve-Jobs","permalink":"https://chenluda.github.io/tags/Steve-Jobs/"}]},{"title":"Make Something Wonderful 全书翻译：1976–1985（2.1）","slug":"书籍/msw-2-1-1976-1985","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2023/05/13/书籍/msw-2-1-1976-1985/","link":"","permalink":"https://chenluda.github.io/2023/05/13/%E4%B9%A6%E7%B1%8D/msw-2-1-1976-1985/","excerpt":"","text":"写在前面早在 1983 年，乔布斯就在阿斯彭国际设计会议上提到了他对未来人工智能的畅想。 问题是，你不能向 Aristotle（亚里士多德） 提问。我认为，在我们展望未来五十到一百年时，如果我们真的能制造出这些能够捕捉潜在精神、潜在原则或潜在的看待世界的方式的机器，那么，当下一个 Aristotle 出现时，也许如果他一生都带着这样一台机器，并输入所有的东西，那么也许有一天，在这个人死去后，我们可以问这台机器：“嘿，Aristotle 会怎么说？这个问题怎么样？”也许我们得不到正确的答案，但也许我们会得到。这让我感到非常兴奋。这也是我为什么要做我现在所做的事情的原因之一。— Steve, 1983 回顾： Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（1） | Steve 关于童年和青年时期的回忆 原文： Make Something Wonderful 1976 年，当 Steve 和他的朋友 Steve Wozniak（“Woz”）开始在乔布斯家的车库里组装后来被称为 Apple I 的东西时，那时“计算机”这个词让人想到的是由专业程序员照料的庞大机器，只有 IBM 一家公司独霸着整个行业。然而，Steve 和 Woz 作为新一代创造性思想家、工程师和业余爱好者，他们试图制造出他们自己可以编程的小型、低成本的机器。 苹果公司成立时，Steve 年仅 21 岁，聪明但缺乏经验，未经打磨。在苹果公司的第一次董事会议上，他把双脚放在会议室的桌子上，很快遭到了董事会主席的斥责。公司的突破是随着 Apple II 的推出，这是一台可以开箱即用、带有磁带存储和内置彩色屏幕的机器。不到一年，苹果成为美国增长最快的公司之一，当 Steve 30 岁时，他已经成为《财富》500 强公司的公众人物。 在苹果公司内部，他的想法和激情是鼓舞人心的，但 Steve 的管理风格却令人难以接受。他的职责几乎每年都在变，他被分配到各种项目和团队，然后又被撤职。他开始与自己钦点的首席执行官 John Sculley 发生冲突。1985 年 9 月，苹果董事会解雇了 Steve。 后来，当谈到苹果公司的最初几年时，Steve 介绍了他当时专注的产品：Macintosh（Mac），这是他和他的团队于 1984 年向世界推出的计算机。对于 Steve 来说，Mac 具备了科技产品应该具备的一切。它既简洁实用，又简单精致，既是提高创造力的工具，也是提高生产力的工具。 Steve 认为，在平行空间中，Mac 团队的成员可以成为作家、音乐家或艺术家。他说：“人们投入到这个项目中的感情和激情与诗人或画家完全无差。”他称他们的作品为一种爱，他们的产品是“为大众提供的计算机”，既有鼠标又有箭头键，桌面图标取代了编程命令，而在启动时，代替闪烁光标的是一个微笑。 Mac 还代表了 Steve 首次领导一个团队开发他认为改变了世界的产品。“它引领了一场革命，”23 年后，在推出另一项改变世界的创新产品：iPhone 时，Steve 回忆道。“我记得在我们推出 Mac 的前一周，大家都聚在一起，说，’以后每台计算机都将这样运作。关于这一点你再也无法争辩。你可以争辩这需要多长时间，但你不能再否认这一事实。’” Steve 关于创办苹果的回忆在 1984 年，Steve 回忆起与苹果背后的友谊。我在 13 岁那年在一个朋友的车库里认识了 Woz。我想他应该有十八岁。[…] 他搬到了我一个叫 Bill Fernandez 的朋友家附近，我当时在 Bill 家。一天晚上，我们正在做一个项目，工作到很晚，Woz 顺道过来看看。我们在一起聊了几个小时。我对他印象深刻，觉得他很棒。他有很好的幽默感，[…] 我们对电子设备有着共同兴趣，这在某种程度上将我们联系在一起，尽管我们在其他方面完全不同。 ✂ 我们就像是各自轨道上的两颗行星，时不时会相互交汇。我们之间的纽带将伴随我们的一生。 1996年，苹果迎来了成立20周年，Steve 回忆起如何将一项少年时期的兴趣制作电脑转变成了一项生意。我们（Woz 和我）之所以制作一台电脑，是因为我们想要拥有一台，但当时我们负担不起购买一台电脑的费用。那时一台电脑要花费几千美元。我们只是两个青少年。我们开始尝试制作电脑，四处在硅谷寻找零件。经过几次尝试，我们成功组装出了第一台 Apple I。我们所有的朋友也想要，他们想自己组装。事实证明，手工组装这样一台东西可能需要五十个小时。因为我们的朋友们在组装方面并不擅长，所以 Woz 和我替他们组装，这占用了我们所有的业余时间。 我们想，如果我们能得到所谓的印刷电路板，就可以直接插入零件，而不必手动连接整个电路，这样我们就可以将装配时间从大约五十个小时缩短到一个小时。Woz 卖掉了他的惠普计算器[1]，我卖掉了我的大众微型巴士[2]，我们筹集到足够的钱请人为我们设计了一块印刷电路板。我们的目标是把它们作为原始印刷电路板卖给朋友们，从而赚回计算器和运输的费用。 然后发生了一件意想不到的事。世界上第一家电脑商店（当时位于山景城）说：“好吧，我要订购 50 台这样的电脑，但我希望它们已经完全组装好。”这是一个我们从未想过的转折。 我们购买了足够制作 100 台电脑的零件。我们组装了 50 台，并交付给客户。我们拿到现金支付，然后跑回去支付出售零件的人。接下来我们遇到了经典的马克思主义利润实现危机[3]，即我们的利润并非现金，而是摆在地板上的 50 台电脑。 我们决定必须开始学习销售和分销，这样我们才能卖掉这 50 台电脑并拿回我们的钱。这就是我们进入这个行业的原因。我们将电脑的想法带到了几家公司，其中两家是 Woz 当时工作的公司（惠普）和我当时工作的公司（Atari）。但是，这两家公司都对此不感兴趣，所以我们自己创办了公司。 1976 年，运行中的 Apple I 。Steve 放在办公室里的一台 Apple I（1&#x2F;2）。Steve 放在办公室里的一台 Apple I（2&#x2F;2）。苹果公司的第一封粉丝信，是一张电脑屏幕的宝丽来（一款相机）照片1977 年，Steve 展示 Apple II 的原型机。 《纽约客》杂志访谈“这是一台驯化的电脑。” Steve 在 1977 年的《纽约客》杂志中首次亮相。杂志派了一名记者参加在纽约博览会中心[4]举行的第一届个人计算机博览会。当时，大多数人还从未见过个人电脑。我们在一个标着“苹果电脑公司”的展位前停下来，与负责这个展位的年轻人聊了起来，他自我介绍说是 Steven Jobs，公司的运营副总裁。Steve 对博览会的参展人数感到非常满意：“我希望在我成长的过程中，就能拥有这些个人电脑，”他说，“过去十年里，人们通过媒体了解到了各种关于计算机的事情。据说，计算机在某种程度上已经控制了他们生活的各个方面。然而，尽管如此，大多数成年人对计算机究竟是什么、它能做什么、不能做什么都一无所知。 “现在，人们首次可以用一台好的立体声音响的价格购买一台计算机，与它互动，了解所有关于它的信息。这就像拆解 1955 年的雪佛兰汽车一样。或者说，就像照相机。全国各地有成千上万的人在学习摄影课程。他们永远不会成为专业摄影师。他们只是想了解摄影过程究竟是怎么回事。计算机也是如此。 “我们在 1976 年在洛斯阿尔托斯的一个车库里创立了一个小型个人计算机制造公司。现在我们是世界上最大的个人计算机公司。我们制造我们认为是个人计算机的劳斯莱斯。这是一台驯化的计算机。人们期望看到闪烁的指示灯，但他们发现它看起来像一个便携式打字机，可以连接到合适的显示屏上，显示彩色图像。 “它给人们带来的反馈，以及用户的热情是巨大的。我们总是被问到它能做什么，它确实能做很多事情，但在我看来，它现在真正做的事情是教人们如何编程。” 回忆起 Steve 小时候希望拥有这样的机器，我们问他是否介意告诉我们他的年龄。 “22岁，”Steve 说。 1977年，苹果公司两次搬迁以扩大办公空间。1981 年，也就是 IBM 推出个人电脑的那一年，苹果公司推出了 Apple II。Steve 是一位狂热的摄影师。为了获得设计灵感，他把摩托车放在苹果公司的大厅里 在阿斯彭国际设计会议上的演讲“计算机和社会正在进行初次约会。” 1983 年 6 月 15 日，在苹果公司推出 Lisa 电脑五个月后，Steve 在科罗拉多州阿斯彭的年度聚会上与设计师们交谈。你们中有多少人超过三十六岁？你出生在计算机之前。计算机已有三十六年的历史了。我认为，当我们回顾过去时，历史的时间线会有一段小切片，一个相当有意义的时期。你们中的很多人都是电视时代的产物。我基本上也是电视时代的产物，但在某种程度上，我已经开始成为计算机时代的产物。 然而，现在正在成长的孩子们无疑是计算机时代的产物，在他们的一生中，计算机将成为主导的沟通媒介，就像电视取代了广播，甚至取代了书籍一样。 你们中有多少人拥有苹果电脑？还是拥有任何一种个人电脑？ 哦哦。 你们中有多少人使用过电脑，或者看过电脑，或者类似的东西？很好。 计算机其实非常愚蠢。它们异常简单，但速度非常快。我们必须为这些微型处理器，甚至是这些巨型 Cray-1 超级计算机[5]提供的原始指令是最琐碎的指令。它们从那里获取一些数据，从这里获取一个数字，将两个数字相加，然后测试结果是否大于零。这可能是你能想象到的最平凡的事情。 但关键是：假设我能比在场的任何人快一百倍。在你眨眼的瞬间，我可以跑出去，抓一束新鲜的春花，跑回来，然后打个响指。你们都会认为我是个魔术师。然而，我基本上只是在执行一系列非常简单的指令：跑出去，抓一些花，跑回来，打响指。但我可以做得如此之快，以至于你们会认为有什么神奇的事情发生。 计算机也是如此。它可以在一秒钟内执行大约一百万条指令。因此，我们往往认为计算机里有什么神奇的东西，而实际上，它只是一系列简单的指令。 我来这里的原因之一是因为我需要你们的帮助。如果你们看过计算机，它们看起来就像垃圾。所有伟大的产品设计师都在设计汽车或建筑。但几乎没有人在设计计算机。如果我们看一下，无论它们看起来像一块破烂还是看起来很棒，我们今年都将销售 300 万台计算机，到 1986 年将达到 1000 万台。无论它们看起来如何，人们都会迅速购买这些产品。而让它们看起来很棒并不需要花费更多的钱。它们将成为每个人工作环境、教育环境和家庭环境中的新物件。我们有机会在这里放置一个很棒的物件，如果我们不这样做，我们将在那里放置另一个破烂物件。 到 1986 年、1987 年，随便选一年，人们与这些机器互动的时间将比今天与汽车互动的时间更长。人们每天将花费两三个小时与这些机器互动，比他们在汽车上花费的时间还要长。因此，工业设计、软件设计以及人们如何与这些东西互动肯定应该得到我们现在给汽车的关注，更多。 如果你看一下，我们所面临的情况是，大多数汽车都不是在美国设计的。电视机？音响电子产品？手表、相机、自行车、计算器，你能说出来的：我们生活中的大多数物件都不是在美国设计的。我们搞砸了。从工业角度来看，我们已经失去了市场，被外国竞争对手所替代。我们在设计方面也搞砸了。 我认为，随着新的计算技术在八十年代与人们相遇，计算机与社会在八十年代开始初次约会，我们有机会让这些东西变得美观，我们还有机会通过物件本身的设计传达某种信息。 当我还在上学的时候，我有一些很棒的老师和很多平庸的老师。让我没有落入犯罪的可能是书籍。我可以在没有中间人的情况下直接阅读 Aristotle（亚里士多德） 或 Plato（柏拉图） 的著作。一本书是一件了不起的事情。它可以直接从源头传递到目的地，中间没有任何阻碍。 问题是，你不能向 Aristotle 提问。我认为，在我们展望未来五十到一百年时，如果我们真的能制造出这些能够捕捉潜在精神、潜在原则或潜在的看待世界的方式的机器，那么，当下一个 Aristotle 出现时，也许如果他一生都带着这样一台机器，并输入所有的东西，那么也许有一天，在这个人死去后，我们可以问这台机器：“嘿，Aristotle 会怎么说？这个问题怎么样？”也许我们得不到正确的答案，但也许我们会得到。这让我感到非常兴奋。这也是我为什么要做我现在所做的事情的原因之一。 那么，你们想谈论什么呢？ Steve 在两个会议上回答了问题。这些计算机将如何协同工作？他们可能会像人们一样合作。有时他们会合作得很好，有时他们不会合作得那么好。 有一些场所已经把这些东西连起来了。其中一个最突出的场所是 Xerox（施乐）公司在帕洛阿尔托研究中心（简称 PARC）。他们将约 100 台计算机连接在一起，形成所谓的局域网，这只是一根来回传输所有信息的电缆。 […] 接下来发生了有趣的事情。有 20 个人对排球感兴趣。于是就产生了一个排球分发列表，然后，当下周的排球比赛改变时，你会写一个快速备忘录并发送到排球分发列表中。然后又出现了一个中餐烹饪列表。不久之后，列表的数量超过了人数。 这是一个非常有趣的现象，因为我认为当我们开始将这些东西（计算机）联系在一起时，它们将促进交流，促进将有共同兴趣的人聚集在一起。 我们离真正解决办公室里把这些电脑连接在一起的问题还有大约五年的时间。我们离解决在家里把他们联系在一起的问题还有十到十五年的时间。很多人正在努力解决这个问题，但这是一个相当困难的问题。 现在，苹果的策略非常简单。我们想要做的是把一台非常棒的计算机放在一本书里，你可以随身携带，并在二十分钟内学会如何使用。这就是我们想做的。我们想在这十年里做到这一点。而且我们真的想用无线电链路实现它，这样你就不必连接任何东西，就可以与所有这些更大的数据库和其他计算机通信。我们现在不知道如何做到这一点。从技术上讲，这是不可能的。 我们正试图摆脱编程。我们必须摆脱编程，因为人们不想给计算机编程。人们想要使用计算机。 我们（苹果）认为，出于某种疯狂的原因，我们正处在正确的时间和正确的地点，以回馈社会。我所说的是，我们大多数人都没有制作我们穿着的衣服，没有烹饪或种植我们吃的食物，我们使用的语言是由其他人发展的，我们使用的数学也是由其他人发展的。我们只是在不断地获取。 把一些东西放回到人类经验池中的能力是非常棒的。我认为每个人都知道，在接下来的十年里，我们有机会真正做到这一点。在做这件事的同时，也非常有趣，我们将回顾过去，并说：“天哪，我们曾经是其中的一部分！” 我们一开始什么都没有。所以，每当你一开始什么都没有，你总是可以射向月球[6]。你没有什么可以失去的。当你获得一些东西时，你会进入保护自己的模式，然后变得保守，投票给 Ronnie。所以我们试图做的是意识到我们所处的非常惊人的时代，而不进入那种模式。 我不能告诉你为什么现在需要一台家用计算机。我的意思是，人们问我：“为什么我现在要在家里买一台电脑？” 我说：“好吧，学习它，运行一些有趣的模拟。如果你有孩子，他们应该从识字方面了解它。他们可能会获得一些很好的教育软件，特别是他们年龄较小的时候。 “你可以通过计算机访问数据、信息或其他资源，做任何你想做的事。认识女人，我不知道。但除此之外，现在没有什么好理由在家里买一台计算机。但将来会有。将来一定会有。” 我认为金钱并不是推动苹果员工的动力。我认为不是钱，而是让你感觉拥有这家公司的一部分，这是你自己的公司，如果你看到了什么……我们总是告诉人们：“你首先为苹果工作，其次为你的老板工作。”我们对此感受非常强烈。 当有一百万人使用某个东西时，创造力就开始迅速发挥。[…] 我们需要一些像 Lisa[计算机] 这样的革命，但我们还需要把数百万台计算机放在那里，让世界创新，因为我们发现世界非常善于创新。 MacintoshMacintosh 上市还不到一年，但显然已经摆脱了个人电脑行业的困境。当时，Steve 在接受记者 David Sheff 的采访时谈到了它的重要性。我喜欢的一点是，使用 Macintosh，你可以用 Times Roman 或 Helvetica 字体写备忘录，或者如果你想在派对上玩得开心一点，可以用 Old English 字体，比如一个排球比赛的通知。或者，你可以为一件严肃的事情使用严肃的字体。你可以表达自己。 就像在 1844 年，电报被发明出来，这是通信方面的一项惊人突破。事实上，你可以在一个下午把消息从纽约发到旧金山。有些人谈论着要把一台电报机放在美国每个人的桌子上，以提高生产力。 但这行不通。原因是你得学会一整套奇怪的咒语，比如摩尔斯电码，点和横杠，才能用电报。学会使用摩尔斯电码需要大约 40 个小时。大多数人是无法学会使用摩尔斯电码的。 所以幸运的是，在 19 世纪 70 年代，Alexander Graham Bell 为电话申请了专利，另一个通信领域的突破性发明，基本上实现了相同的功能，但人们已经知道如何使用它。最好的是，除了让你用文字交流外，它还让你唱歌。它让你用言外之意来表达你的话语。 今天我们正处于同样的情况。有人说我们需要在美国每个人的桌子上放一台 IBM PC，以提高生产力。但这不会奏效。这次你需要学会的特殊咒语是 slash-qz 之类的东西。大多数人不会学 slash-qz，就像他们不会学摩尔斯电码一样。 这就是 Macintosh 的意义所在。它是我们行业的第一个“电话”。但对我来说，最棒的是，就像电话对电报一样，Macintosh 让你唱歌。它让你使用特殊的字体。它让你制作图画和图片，或者将其他人的图画或图片纳入你的文件。 即使在商业中，你也会看到五页的备忘录被压缩成一页，因为有一张图片可以表达关键概念。所以我们看到的是纸张减少，沟通质量提高。 更有趣的是。一直以来都有这样一个误解，即在家里非常有趣、有创意的人一旦上班，就会变得枯燥、无聊和严肃，而事实并非如此。所以，如果我们能再次将文理学院精神注入商业这个严肃领域，我认为这将是一项有价值的贡献。 下一节： Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（2.2） | 1976–1985 参考 ^HP计算器（HP Calculator）是惠普公司（Hewlett-Packard，简称HP）生产的一系列科学、工程和金融计算器。HP计算器自20世纪70年代开始生产，并因其卓越的质量、功能和创新而受到专业人士和学生的喜爱。惠普公司是第一个生产可编程计算器和第一个生产基于逆波兰表示法（Reverse Polish Notation，简称RPN）的计算器的公司。 ^大众微型巴士（VW Microbus），又称Volkswagen Type 2 或 Transporter，是大众汽车（Volkswagen）制造的一款轻型商用车。VW Microbus在20世纪50年代至70年代非常受欢迎，尤其在嬉皮士（Hippie）文化中具有标志性地位。它以其简单的设计、实用性和空间利用率而著称，被视为大众甲壳虫（Volkswagen Beetle）的亲戚。 ^利润实现危机是指资本主义体系中的一种危机，其中生产的商品无法在市场上实现其价值。这种危机可能会导致经济萎缩和失业率上升。 ^纽约博览会中心（The New York Coliseum）是一座位于纽约市曼哈顿的展览和会议中心，建于1956年，由著名建筑师Wallace K. Harrison设计。它位于哥伦布圆环（Columbus Circle）附近，靠近中央公园和林肯中心（Lincoln Center）。纽约博览会中心于2000年被拆除，为时代华纳中心（现称为华纳中心，The Warner Center）腾出了地皮。时代华纳中心是一座多功能建筑群，包括购物中心、办公楼、住宅和酒店等设施。此外，时代华纳中心也是 CNN 纽约总部的所在地。如今，纽约博览会中心的遗址已经完全被时代华纳中心取代，成为纽约市曼哈顿地区的一个繁华地段。 ^Cray-1 是一款由 Cray Research 公司设计和生产的超级计算机，该公司由著名计算机科学家和工程师 Seymour Cray 创立。Cray-1 作为世界上第一台商业超级计算机，于 1976 年首次亮相，并在 1977 年交付给了第一个客户——洛斯阿拉莫斯国家实验室（Los Alamos National Laboratory）。 ^“Shoot for the moon”（射向月球）是一句常用的英语俚语，意味着要追求宏伟的目标和梦想，即使无法实现这些目标，也至少可以取得相对较高的成就。这句话鼓励人们勇敢地追求自己的梦想，不畏困难，努力实现自己的目标。这个表达的完整版本是“Shoot for the moon. Even if you miss, you’ll land among the stars.”（射向月球，即使你错过了，你也会降落在繁星之间。）这句话暗示即使我们无法实现最初的目标，我们仍然可能在追求梦想的过程中取得显著的进步和成就。","categories":[{"name":"书籍","slug":"书籍","permalink":"https://chenluda.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"Steve Jobs","slug":"Steve-Jobs","permalink":"https://chenluda.github.io/tags/Steve-Jobs/"}]},{"title":"Make Something Wonderful 全书翻译：1976–1985（2.2）","slug":"书籍/msw-2-2-1976-1985","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2023/05/13/书籍/msw-2-2-1976-1985/","link":"","permalink":"https://chenluda.github.io/2023/05/13/%E4%B9%A6%E7%B1%8D/msw-2-2-1976-1985/","excerpt":"","text":"写在前面 乔布斯在与首席执行官 John Sculley 的权力斗争中失败后，于 1985 年 9 月离开了苹果。这次离开官方上说是辞职，但乔布斯认为这是一种背叛。 回顾： Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（1） | Steve 关于童年和青年时期的回忆 Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（2.1） | 1976–1985 原文： Make Something Wonderful 1983 年在夏威夷举行的苹果销售会议。纽约 IBM 办公室外面。 对苹果员工的演讲“George Orwell 关于1984的预言是否准确？[1]“ 1983 年 10 月的一次苹果销售会议上，史蒂夫·乔布斯介绍了 Macintosh 及其标志性广告，这则广告在 1984 年的超级碗期间播出。嗨，我是 Steve Jobs。 现在是 1958 年。IBM 放弃了收购一家年轻的初创公司的机会，这家公司发明了一项名为静电复印的新技术。两年后，Xerox（施乐）诞生了。从那以后，IBM 一直在后悔。 又过了十年，已经是六十年代末期。Digital Equipment（DEC）等公司发明了小型计算机。IBM 认为小型计算机无法进行严谨的计算，因此对他们的业务无关紧要。在 IBM 进入小型计算机市场之前，DEC 发展成为一家市值数亿美元的公司。 又过了十年，现在是七十年代末。1977 年，美国西海岸的一家年轻初创公司苹果，发明了 Apple II，这是我们今天所知道的第一台个人电脑。IBM 认为个人电脑无法进行严谨的计算，对他们的业务也无关紧要。 八十年代初，1981 年。Apple II 成为世界上最受欢迎的电脑。苹果成为一家市值 30 亿美元的公司，成为美国商业史上增长最快的公司，有 50 多家竞争对手争夺市场份额。1981 年 11 月，IBM 推出了IBM PC，进入个人电脑市场。 1983年。苹果和 IBM 成为行业最强大的竞争对手，两家公司在 1983 年各自销售了约 10 亿美元的个人电脑。两家公司将在 1984 年投入超过 5000 万美元用于研发，另投入 5000 万美元用于电视广告，总计近 2.5 亿美元。 整个行业的洗牌已经全面展开。第一家大型公司破产，其他公司也岌岌可危。1983 年整个行业的损失甚至超过了苹果和 IBM 个人电脑的综合利润。 现在是 1984 年。IBM 似乎想要统治一切。人们普遍认为，苹果是唯一有希望给 IBM 带来竞争压力的公司。经销商最初张开双臂欢迎 IBM，现在担心 IBM 会主导和控制未来。他们越来越拼命地想回归苹果，将其视为确保未来自由的唯一力量。 IBM 想要统治整个计算机行业，它将炮口对准了最后的障碍：苹果。大蓝（IBM 的昵称）会主宰整个计算机行业吗？[观众：不会！] 整个信息时代？[观众：不会！] George Orwell 关于 1984 年的预言是否准确？ [Steve 播放了“1984”广告。这部广告由 Ridley Scott 执导，描绘了一个反乌托邦的 Orwellian 式世界。在一个场景中，一群穿着灰色衣服、剃光头的人面无表情地坐在大屏幕前，屏幕上一个独裁者在喋喋不休。一个穿着鲜红色跑步短裤和 Macintosh T 恤的女人闯入房间。她向屏幕投掷了一把锤子，摧毁了它。广告以一个承诺结束：“1 月 24 日，苹果电脑将推出 Macintosh。你将看到为什么 1984 年不会像 ‘1984’ 那样。”] [观众报以热烈的掌声和欢呼声。当声音平息下来时，Steve 继续发言。]那部广告将在 Macintosh 发布前一周播出。制作这部广告的广告公司 Chiat&#x2F;Day 今天也在场。Jay Chiat 是该公司的创始人，Lee Clow 和 Steve Hayden 负责文案和创意。 或许，我想他们刚刚听到了你们的想法。 在 Macintosh 发布前享受难得的休息。1984 年，在新奥尔良的一次销售会议后筋疲力尽。史蒂夫 1984 年在伍德赛德的家中为《时代》杂志拍摄的照片停留以观看 Macintosh 的运行。 与 Michael Moritz 访谈“在犯错的过程中，你的审美会变得更好。” 1984 年 5 月，Steve 和即将转行成为风险投资家的记者 Michael Moritz 在苹果公司的办公室进行了一次对话。他们谈论了很多话题，包括 Steve 对产品设计的看法。Steve Jobs：在设计 Mac 时，我去看了看 Cuisinart[2]。那是我的 Cuisinart 周。 Michael Moritz：除了 Cuisinart，还有没有其他特别的产品对你产生了影响？比如说，来自 70 年代末期之类的。 Steve Jobs：嗯，我们一直生活在汽车周围。我从来不是个汽车迷，但我一直喜欢大众甲壳虫。事实上，我也一直喜欢大众面包车。 还有一些小东西：葡萄酒标签、画廊里的画作。就是一些简单的东西。没有什么特别深刻的东西，只是很多很多的小东西。我认为我的审美品味跟很多其他人并没有什么不同。唯一的区别是，我可以非常固执地追求让事物达到我们都知道它们可以达到的最好状态。这就是唯一的区别。 Michael Moritz：是的，我觉得你在谦虚。 Steve Jobs：嗯，随着你犯错误，事物会变得越来越精细。我有机会犯很多错误。在犯错的过程中，你的审美会变得更好。但真正重要的是：如果你要制作某样东西，要把它做得真正出色，并不需要更多的精力，也并不需要更多的钱。只是需要多花一点时间。并不是很多。还有一种愿意这样做的意愿，一种坚持到底让事物变得真正出色的意愿。 但审美品味呢？我认为审美品味很像唱歌。Joanie Baez 有一副美妙的嗓音，但她的嗓音之所以美妙，不仅仅是因为她的嗓音很好听。更重要的是她有一双极好的耳朵。她可以听某人说话 30 秒，就几乎完美地模仿他们的声音。她的耳朵非常好。我认为，同样地，好的审美品味只是来源于你的眼睛。一种对你所看到的事物的本能感觉，而不是你所做的事情。 Steve Jobs：我想制造比当今市场上任何产品都更小的产品。当你把东西做得更小时，你就有能力把它们做得更精确。显然，手表就是一个完美的例子。它很漂亮，但精度必须与物体本身的比例相同，因此你要把它做得非常精确。随着我们的产品越来越小，我们有机会做到这一点。所以，显然，我希望所有东西都更小。 我还认为能够随身携带的产品真的很好。即使它们不是便携式的，有一个手柄，让人们可以把它们拿起来，当你想改变它们的位置时。从一个房间搬到另一个房间，或者从一个办公室搬到另一个办公室。Lisa [计算机] 太重了，无法从办公室搬到办公室，或者从房间搬到房间，或者在周末带回家。因此，问题是，“我们如何找到一种方式，将同样的功能打包到我们可以携带的更小的东西中，并能够更精确地表达其形态？”这是我们未来的方向。 Michael Moritz：产品中有哪些丑陋、让人反感的设计，或者说有太多无法列举？ Steve Jobs：是啊，三年前随便选一辆车，你知道吗？今天的大多数汽车也一样。任何东西看看房间就知道了。桌子、椅子：都很丑。你可以问我，那我还在这个办公室里干什么？但是，大多数东西都不太好看。 电话就是一个很好的例子。唯一好的电话是原装那个和 Trimline[3]。Trimline 是唯一不错的。他们做的新产品都是垃圾。 Steve Jobs：（在苹果公司）我们变得越来越简单。 Steve Jobs：你见过惠普在佩奇米尔路的建筑吗？它们非常棒。它们有扇形屋顶，面对北面的玻璃，如果你想的话，你甚至可以在上面安装太阳能集热器。在一栋建筑里，他们做了一整面玻璃墙。所以人们在那里工作，有大量自然光线进来。 （苹果公司）这些建筑物的问题是没有光线。我的意思是，你在外面待五分钟，走进来，这里真的很暗，你什么都看不见。我们都像是生活在这些小小的洞穴里。 我只想要大量的自然光。 Michael Moritz：（在《1984》的商业广告中）关于 “Big Brother”[4]的内容是在暗喻 IBM 吗？或者这只是公众自行臆想的事，而你们并不打算—— Steve Jobs：否认吗？ Michael Moritz：是的，否认。 Steve Jobs：好吧，我认为对此最好的回应，是在《财富》杂志上的回答，“如果在 1984 年看到老大哥，对很多人来说意味着就是 IBM，这说明了 IBM 形象问题的严重性，而不是我们刻意在丑化。” 事实上，当然，我们看到了这个暗喻。我认为这个广告在表达两个意思。第一个意思是，计算机被看做是一种由少数人控制的强大工具，会被用来跟踪我们的行踪，而我们正在评判这种恐惧。 当然，任何人都可以认为 IBM 是这种控制的象征，与 “Big Brother” 的形象相似，这种比喻是不可避免的。 Steve 在他的双排扣西装外套里穿了条牛仔裤 《新闻周刊》的采访“我想要创造东西。” Steve 在与首席执行官 John Sculley 的权力斗争中失败后，于 1985 年 9 月离开了苹果。这次离开官方上说是辞职，但 Steve 认为这是一种背叛。几周后，他接受了新闻周刊的采访。新闻周刊：当你听到（苹果）董事会的决定（起诉你）时，你作何反应？这些都是你认识和共事了很长时间的人。 Steve Jobs：哦，是的。我无法想象这一切会以如此疯狂的方式结束。我曾希望我的生活能像一幅色彩斑斓的挂毯那样丰富多样，在苹果公司与其他领域之间不断地转换：我会在那里呆一段时间，然后也许去做一些其他的贡献，但还是与苹果有关，然后也许再回来并呆上很长一段时间，然后再去做其他的事情。但事情不会这样发展。所以，你知道，我度过了我生命中最美好的十年。我不太后悔任何事情。 个人而言，我想要创造东西。我三十岁，还没有准备好成为业界专家。这个夏天我收到了三个当教授的邀请，但我告诉所有大学，我会成为一名糟糕的教授。我最擅长的是找到一群有才华的人，和他们一起创造东西。我尊重苹果公司的发展方向。但对于我个人来说，你知道，我想创造东西。如果在那里没有我的创造空间，那我就会像之前两次那样自己创建一个地方。你知道，当苹果公司成立时，是我在车库里创造了它，当 Mac 诞生时，是我在象征性的车库里创造了它。 Steve Jobs：虽然外界从数字指标的角度看待成功，但我的衡量标准可能与此截然不同。我的衡量标准可能是从这里开始设计的每台计算机都必须至少和 Macintosh 一样好。 Steve Jobs：我以前上班的时候，我会去那里打一两个电话，看一些邮件。但是，这是在六月和七月，大部分的公司管理报告都不会经过我的桌子。有些人可能会看到我的汽车在停车场里，过来陪我一下。 我会感到沮丧，然后三四个小时后回家，真的很沮丧。我这样做了几次后，觉得这种让我的精神状态变得不健康。所以我就不去了。你知道，没有人真正在意我。 新闻周刊：你觉得他们夺走了你的公司吗？ Steve Jobs：对我来说，苹果存在于那里工作的人们的精神中，以及他们开展业务的各种理念和目标。所以，如果苹果只是一个将电脑视为普通商品的地方，那就失去了浪漫情怀，人们会忘记电脑是人类有史以来最神奇的发明之一，这时我便会觉得我失去了苹果。但是，如果我离开了，而留在那里的人仍然怀有这种情感，并且仍在努力制造下一代伟大的个人电脑，那么我会觉得我的精神仍然存在于其中。 Steve Jobs：其中最困难的五天之一是 John（苹果首席执行官 John Sculley）在分析师会议上说，未来将会没有我的一席之地，一周后的另一次分析师会议上他又重复了这句话。他并没有直接对我说，而是对媒体说的。你可能被人打了一拳，肚子痛得喘不过气来。如果你放松下来，就会重新开始呼吸。这就是我整个夏天的感受。我必须做的就是尽量放松。这很困难。但我在树林里散了很多长时间的步，不和别人交流。 新闻周刊：你曾谈到自己难以相处，性格有点强硬。这是否在某种程度上促成了你的失败？ Steve Jobs：你知道，我并不是一个六十二岁的政治家，一辈子都在世界各地旅行。所以我相信，当我 25 岁的时候，如果能回到过去，用我现在的知识，我肯定能处理得更好。我敢肯定，当我 35 岁的时候，我也会对 1985 年的情况有同样的看法。我非常坚定自己的信念。总的来说，我还挺喜欢自己的，我并不急于改变。 新闻周刊：但这次经历改变了你吗？ Steve Jobs：哦，是的，我认为我从这段经历中成长了，我从中学到了很多。虽然我现在还不确定是怎样的改变或学到了什么。但是，我确实有这种感觉。我并不愤怒，我并不痛苦。 新闻周刊：媒体上有很多关于你对佛教、素食主义感兴趣的报道。 Steve Jobs：当我们深入了解这些观念时。 新闻周刊：观念？现在你对这些东西仍然感兴趣吗？ Steve Jobs：嗯，我不知道该说什么。我的意思是，我不吃肉，但我也不用每周日都去教堂。 新闻周刊：有人说，你曾经想过去日本，在一座寺庙里修行。 Steve Jobs：是的，是的。我很高兴我没有那么做。我知道这听起来很老套。但我觉得我是一个美国人，我出生在这里。现在，世界的命运掌握在美国的手中。我真的这么觉得。你知道，我会在这里度过我的一生，并尽我所能提供帮助。 1985 年左右的 Steve 下一节： Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（3.1） | 1985–1996 参考 ^乔治·奥威尔（George Orwell）是一位英国作家，他在1949年出版了一部反乌托邦小说《1984》。这部小说描述了一个名为奥西尼亚（Oceania）的极权主义国家，其中政府完全控制了人民的生活 ^Cuisinart是一家以生产高质量厨房电器和用具而闻名的美国品牌。该公司创立于1971年，最初是由美国物理学家Carl Sontheimer创立的。Cuisinart最初生产的是食品加工器，因其高质量和性能而得到广泛认可。随着时间的推移，Cuisinart开始生产各种小型厨房电器和烹饪用具，如咖啡机、烤箱、榨汁机、搅拌器、多功能锅等等。 ^Trimline是一种由AT&amp;T公司在20世纪70年代生产的电话型号，它是当时非常受欢迎的一种电话。Trimline电话有经典的外观和轻巧的设计，它的特点是机身外形修长，手柄为细长的直线形状，简单而时尚。Trimline电话具有明亮的彩色外壳，常常用于家庭和办公室。它还有一个方便的拨号盘，使用户可以轻松地拨打电话号码。 ^关于“Big brother”的概念，它最早出现在乔治·奥威尔的小说《1984》，是一个描述政府对人民进行普遍监控和控制的概念。IBM是一个大型的跨国科技公司，也曾经被指控参与过与政府的监控计划相关的工作。这些指控包括向政府提供了技术和软件工具，用于进行广泛的监控和数据收集。然而，IBM公司已经多次否认这些指控，并表示他们不会从事违反道德或法律的活动。至于这些指控的真实性和IBM的立场，这是一个有争议的话题，需要进行深入的研究和讨论。","categories":[{"name":"书籍","slug":"书籍","permalink":"https://chenluda.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"Steve Jobs","slug":"Steve-Jobs","permalink":"https://chenluda.github.io/tags/Steve-Jobs/"}]},{"title":"Make Something Wonderful 全书翻译：1985-1996（3.1）","slug":"书籍/msw-3-1-1985-1996","date":"un66fin66","updated":"un66fin66","comments":true,"path":"2023/05/13/书籍/msw-3-1-1985-1996/","link":"","permalink":"https://chenluda.github.io/2023/05/13/%E4%B9%A6%E7%B1%8D/msw-3-1-1985-1996/","excerpt":"","text":"写在前面“Stay hungry，stay foolish”是 2005 年乔布斯在斯坦福大学的演讲主题，很多人都把这里的 “Stay hungry”翻译为“求知若饥”，但很少有人知道乔布斯在里德学院时真实体验过一段体饥饿的时光。 回顾： Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（1） | Steve 关于童年和青年时期的回忆 Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（2.1） | 1976–1985 Glenn：《Make Something Wonderful: Steve Jobs in his own words》全书翻译（2.2） | 1976–1985 原文： Make Something Wonderful Steve 离开苹果后的几年是他职业生涯中最艰难、也最具成长性的几年。 他决心创建一家新的伟大的计算机公司，与 Macintosh 团队的几名成员一起创办了 NeXT。“我们会犯很多错误，但至少它们是新的和创造性的。”他预测道。 大约在同一时间，Steve 向一家名为 Pixar 的小公司投资了 1000 万美元。这家小型的计算机图形操作公司，是从电影制作人 George Lucas 的帝国中新剥离出来的。Pixar 的技术专长吸引了 Steve；其最初的产品是一台售价超过 10 万美元的高端图形计算机。 NeXT 和 Pixar 很快遇到了麻烦。1988 年推出的 NeXT 计算机系统，功能强大且充满了 Steve 喜欢的人性化设计。它在视觉上引人注目，使用起来直观，内置了高质量的音频和莎士比亚的全部作品。但它市场推出太晚，价格昂贵，销售不佳。在 NeXT 推出后的六年后，除了 Steve 外，整个创始团队都已经辞职。 与此同时，Pixar 仅靠销售计算机和软件以及制作广告动画勉强维持生计。公司还制作了一些获奖的短片， Steve 很喜欢这些片子。这种以技术服务于卓越故事讲述的方式体现了他最喜欢的一件事情：在技术和人文艺术的交汇点工作。这些短片激发了 Steve 的热情，让他不断地向 Pixar 写支票，最终投资了约 6000 万美元。 然而，正如 Steve 所说，这些电影只是“背景”，而不是公司的重点。他将 Pixar 的早期商业战略描述为“找到一种支付账单的方式”，后来他推测，当时公司没有崩溃的唯一原因是领导团队“都会感到沮丧……但不是我们所有人都会同时沮丧。” 尽管有时他似乎对技术的可能性感到失望——“这些东西并没有改变世界。它真的没有，”他以一种罕见的悲观情绪告诉记者，但他的世界也在超越工作而扩展。他珍惜自己的隐私，谈到自己的公众形象时说，“我把它看作是我的众所周知的孪生兄弟。那不是我。” Steve 学会了如何磨练一家公司的本质，即使这个过程很痛苦。他将 NeXT 的重点转向销售软件。这个转变意味着关闭工厂并解雇了 NeXT 五百三十名员工中的二百多人。与此同时，Pixar 放弃了广告和硬件业务，并与 Disney 达成协议，所有这些都是为了追求一个似乎不可能实现的梦想：制作全电脑动画的长片。 经过将近十年的艰难，精简后的 NeXT 和 Pixar 都变成了不可思议的成功故事。1995 年底，Pixar 首次公开募股的同一个月首映了《玩具总动员》。一年后，需要操作系统软件的苹果公司以 4.27 亿美元的价格收购了 NeXT。“如果你仔细观察，”Steve 喜欢说，“大多数一夜之间的成功都需要很长时间。” Steve 创办 NeXT几周后，Steve 在 NeXT 公司创立后接受了《新闻周刊》的采访，谈到了成立这家公司的初衷。我一直在阅读一些生物化学、重组 DNA 的文献，最近还见到了 Paul Berg，他是一些重组技术的发明者。我给他打电话，说：“你还记得我吗？我对这些东西一无所知，但我有很多关于它如何运作的问题，我很想和你一起吃午饭。”于是我们在斯坦福大学吃了午餐。他向我展示了他们是如何进行基因修复的。实际上，这很简单，很有意思。它听起来很像一些计算机科学中的概念。他解释了他是如何在潮湿的实验室里做实验的，这使得实验需要一两到三周的时间来完成。我问他：“为什么不在计算机上模拟这些实验呢？这不仅可以让你更快地完成实验，而且有一天，这个国家的每个微生物学新生都可以玩 Paul Berg 的重组软件。” 于是，他的眼睛亮了起来。那次午餐可以说是一个里程碑，因为那时我开始真正思考这些东西并重新启动了我的想象力。 成为 Pixar 的大股东1996 年，也就是 Pixar 发行《玩具总动员》并首次公开募股的第二年，Steve 回顾起最初吸引他加入该公司的工作和想法。我在 1985 年遇见了 Ed Catmull，当时他正在卢卡斯影业的计算机部门工作。[…] 我一生中大部分时间都在从事图形工作。尽管大多数人不记得，但是 Apple II 是你可以使用的第一台真正的彩色计算机。显然，Macintosh 是图形界面。LaserWriter[1]是用于图形的。但是这些都是 2D 的。我们在苹果公司已经做了一些 3D 的工作，我当然也意识到了这个领域。但是 Ed 和他的团队所做的东西远远领先于我见过的任何人。 Pixar 的起步2003 年，Disney 和 Pixar 就工作室的未来进行了深入的谈判，Steve 向电影制作人 Leslie Iwerks 讲述了 Pixar 的起步。在 Pixar 早期的策略是：找到一种支付账单的方式。在这个背景下，我们正在开发动画软件，而 John Laseter 则在制作一系列短片，为之后的《玩具总动员》做准备。我们正在尝试支付账单和争取时间。事实证明，这种策略并不奏效。如果可以回到过去，我们最好只资助动画制作，而不是试图通过其他产品（例如 Pixar 图像计算机和软件）来支付账单，但那是我们试图让公司继续前行的最佳尝试。最终，我只能不断写支票来维持公司运营，这基本上持续了十年。 从一部 Pixar 动画短片中，你可以看到其中有着魔力。对于 Pixar 的其他技术，你必须成为一名专家才能理解它。[…但是]你不需要知道任何专业知识就可以欣赏这部电影。这令人耳目一新，真正指明了我们想要去的地方。我们不想说服人们我们的技术有多么出色 - 我们知道它是很棒的。我们想利用我们的技术创造一些不需要拥有任何专业知识就可以喜欢它的东西。这就是我们最终所做的。 在成为 Pixar 的大股东之前不久。NeXT 计划占领教育市场。NeXT 对周四会议的禁令是短暂的。1987 年，NeXT 计算机的泡沫塑料模型。在 Menlo 公园的 NeXT 公司野餐会上休息。 给 NeXT 员工的电子邮件“Pixar 引领潮流！” 发件人：Steve Jobs收件人：NeXT主题：美好的一天半日期：1989 年 3 月 30 日，晚上 8:08。 好了，我们又做到了。恭喜所有人取得了真正的团队合作成果。而且，Pixar 也做到了。 对于那些昨晚没有看到奥斯卡颁奖典礼的人，Pixar 凭借其使用计算机制作的电影《玩具总动员》获得了动画短片类奖项。 《玩具总动员》是有史以来第一部获得奖项的使用计算机制作的电影，并且在与几部非计算机动画的优秀作品竞争中胜利！ 计算机图形行业刚刚达到了一个重要的里程碑，而 Pixar 引领了这一潮流！ 在里德学院的演讲“性格不是在美好的时光中建立的，而是在困难的时刻中建立的。” 1991 年 8 月 27 日，当 Steve 在母校里德学院迎接新生时，NeXT 电脑销售情况不佳，Pixar 公司也进行了一轮裁员。非常感谢大家的欢迎。对我来说意义非凡。正如你们所知道的，我是一位与众不同的里德校友。我没有从里德毕业——虽然这在某种程度上并不是特别罕见。 但也许更不寻常的是：我在里德只学了一个学期就没钱了，于是我就退学了。但是后来我又“复学”了一年半。所以，我其实是出于自愿来到这里的，这可能更为不寻常。我在这里有过一些经历，我相信你们中的许多人作为大一新生和在校生都会有这些经历，这些经历一直伴随着我一生。我在想一些经历，想和你们分享一下。 请记住，现在我比你们大得多了。我一直认为人们的自我意识在 15 或 16 岁时开始觉醒。因此，如果我们把年龄归一化到 15、16 岁，那么你们中的大多数人作为大一新生，可能只有两三岁或四岁。而我则大约是 20 岁。这或许可以让你们更好地理解我多年后重返里德的感受。但是有几件事情让我一直难以忘怀，我希望能够传递给你们，也许对你们有所帮助。第一件事是，正如你们即将经历的那样，我被迫参加了人文讲座——好像每天都有。我和Svitavsky教授一起学习莎士比亚。当时，我认为这些讲座毫无意义，甚至有些残忍，但现在我可以保证，随着时间的推移，我感谢上帝我在这里有这些经历。它对我以后做的每一件事都有帮助，尽管当时我从未想到过这一点。 我在里德的第二个经历是饥饿，一直都很饥饿。这里的餐厅很快就让我成为了素食主义者。我没有那么多钱，所以我会收集可乐瓶，然后拿到商店去换吃的。我发现最便宜的吃法是 Roman Meal[2]。你们听说过这个吗？它是一种麦片，由一位哈佛大学历史教授发明，他想知道罗马军团征服和掠夺村庄时带了什么食物，通过他的研究，他发现是 Roman Meal。你可以在当地商店买到它，这是最便宜的生活方式。所以我在 Roman Meal 上生活了很多个月。 但也有几个人跟我一样，在好几天没吃东西后，会搭便车到镇上的H are Krishna 寺庙，周日他们会给所有人提供食物。通过实践，我们发现抵达的时机非常重要——在他们的特定宗教活动之后，食物到来之前。由于几天没有吃东西，我们会吃很多，有几次因为实在无法动弹，所以在那里过夜。 第二天早上，他们会在早上四点钟叫醒我们，因为这是他们去收集鲜花来纪念 Krishna[3] 的时间。所以他们会在天亮之前带我们到附近的社区——他们会开始从邻居那里偷花。而住在 Hare Krishna 寺庙附近的邻居很快就会发现他们的掠夺行为，并会早起在花坛旁站岗。所以他们不得不在寺庙周围扩大他们的行动半径。在花一点时间与这些人在一起时，我注意到了他们的其他行为。他们曾经向当地百货商店出售香薰，然后再偷回来，这样百货商店就会购买更多，他们的业务就会蓬勃发展。而他们的伦理告诉他们这是可以接受的，因为一切都是为了 Krishna 服务的。在与他们的互动中，我想我比在校园里学到了更多关于情境伦理的知识。 最后一个经历我想为你们描述的是关于一个人，他可能今天也在这里，他的名字叫 Jack Dudman，他曾经是学院的院长。即使我没有交学费也没有注册学生，他还是放任我留在校园里上课。当我走投无路的时候，他常常会和我散步，走完后我会在破旧的外套口袋里发现一张 20 美元的钞票，他从来不在走之前、期间或之后提起这件事。 在这里，我从杰克·达德曼和这所学校的人身上学到了更多的慷慨和大爱。因此，我想感谢你们，因为我在这里学到的东西一直伴随着我。性格并非在好时光中塑造，而是在困难时期中；不是在丰裕时期，而是在逆境中塑造出来——这所学校似乎能够培养出这种逆境精神，并在某种程度上锤炼了我的性格。因此，我感谢你们教会了我如何饥饿，并如何将这种饥饿保持在我整个生命中。 Steve 有时与克林顿总统通电话1995 年，《玩具总动员》上映，Pixar 首次公开募股1995 年，大约在 Pixar 与 Disney 的交易重新谈判的时候 下一节： 参考 ^LaserWriter是一款早期的激光打印机，由苹果公司于1985年推出。LaserWriter是第一款在个人电脑上使用激光技术的打印机，它的推出引领了桌面出版的革命。LaserWriter在当时是一款非常先进的打印机，具备了高分辨率打印和PostScript语言支持等特性，使得它可以打印出质量非常高的文字和图形。此外，它还采用了AppleTalk网络协议，可以通过网络连接到其他苹果电脑，这也为它的广泛应用提供了可能。尽管现在激光打印机已经普及，但LaserWriter在当时被认为是一款革命性的打印机，为现代的打印技术发展奠定了基础。 ^Roman Meal是一个美国品牌，专门生产全麦面包和其他全麦食品。该品牌成立于1912年，总部位于华盛顿州的卡斯尔洛克。Roman Meal的全麦面包以其含有大量纤维和营养物质而闻名。该公司还生产其他全麦食品，如燕麦片、麦片、饼干和糕点等。Roman Meal致力于提供健康的食品选择，以帮助人们改善他们的饮食和健康状况。 ^Krishna 是印度教中非常重要的神祇之一，他被认为是智慧、爱、勇气和正义的象征。克里希纳被描述为一位英俊、快乐和充满智慧的男子，他的教诲和行为对印度文化和哲学产生了深远的影响。","categories":[{"name":"书籍","slug":"书籍","permalink":"https://chenluda.github.io/categories/%E4%B9%A6%E7%B1%8D/"}],"tags":[{"name":"Steve Jobs","slug":"Steve-Jobs","permalink":"https://chenluda.github.io/tags/Steve-Jobs/"}]},{"title":"MICCAI 2023 挑战赛速览","slug":"速览/MICCAI-2023-挑战赛速览-Glenn","date":"un66fin66","updated":"un33fin33","comments":true,"path":"2023/05/13/速览/MICCAI-2023-挑战赛速览-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/13/%E9%80%9F%E8%A7%88/MICCAI-2023-%E6%8C%91%E6%88%98%E8%B5%9B%E9%80%9F%E8%A7%88-Glenn/","excerpt":"","text":"前言MICCAI 是医学图像计算与计算机辅助干预国际会议（International Conference on Medical Image Computing and Computer Assisted Intervention）的缩写。该会议自 1998 年开始，每年举办一次，是医学图像处理领域中最具有权威性和影响力的会议之一，被广泛认为是医学图像处理领域内的顶级会议之一。 而 MICCAI CHALLENGES 是 MICCAI 的一个重要组成部分。它是一个国际性的竞赛平台，面向医学图像计算和计算机辅助干预领域的研究人员和开发者，旨在鼓励和推动该领域的技术发展和应用。 对于我们来说，这是个提升自己（获取数据集）的机会。今年的 MICCAI 将会在 10 月 8 日到 12 日在加拿大的温哥华举办，每年这个时候都是各种竞赛云集的时刻。 截止 2023-05-09 16:45:17，部分在 MICCAI 2023 上注册的挑战赛还没有发布任务和介绍，持续更新。 图像分割-肾脏（KiTS2023） 主页：The 2023 Kidney Tumor Segmentation Challenge任务：图像分割目标：腹部 CT 中肾脏、肾脏肿瘤、肾脏囊肿（如图 1 所示）分割 图1. 紫色为肾脏，绿色为肾脏肿瘤，蓝色为肾脏囊肿。 数据集：共 599 例，489 例训练集，110 例测试集。时间线：如下表所示 4 月 14 日 训练数据集发布 7 月 14 日 论文提交截止时间 7 月 21 日 - 7 月 28 日 结果提交 7 月 31 日 公布结果 10 月 8 日 - 10 月 12 日 参加 MICCAI 2023 会议 主办单位：明尼苏达大学、德国癌症研究中心、克利夫兰诊所 图像合成-MRI to sCT &amp; CBCT to sCT（SynthRAD2023） 主页：SynthRAD2023 - Grand Challenge任务：图像合成目标：1）使用磁共振影像（MRI）生成伪计算机断层成像（sCT）； 2）锥形束计算机断层扫描（CBCT）生成伪计算机断层成像（sCT）。 图2. 数据集：540 例配对的 MRI-CT；540 例配对的 CBCT-CT 。时间线：如下表所示 4 月 1 日 发布训练数据 4 月 1 日 - 5 月 31 日 训练阶段 5 月 1 日 - 8 月 15 日 初步测试阶段 5 月 13 日 在 ESTRO23 上介绍挑战赛 6 月 1 日 - 7 月 15 日 验证阶段 7 月 16 日 - 8 月 15 日 测试阶段 8 月 15 日 提交方法论论文的截止日期 9 月 20 日 公告和邀请介绍 10 月 8 日 - 10 月 12 日 在 MICCAI 大会上介绍挑战赛结果的第一部分 4 月 &#x2F; 5 月 在 ESTRO 大会上介绍挑战赛结果的第二部分 主办单位：如下图所示 图3. SynthRAD2023 主办单位和成员 图像配准-乳腺癌病理 WSI 的 IHC to H&amp;E（ACROBAT2023） 主页：ACROBAT 2023 - Grand Challenge任务：图像配准目标：将使用常规 IHC 染色剂（ER, PGR, HER2, KI67 四种之一）的乳腺癌组织切片的 WSI，配准到用 H&amp;E 染色的对应 WSI 上。数据集：共 750 例，每例包含 1 组 H&amp;E 染色的组织切片的 WSI 和 1-4 组对应的常规 IHC（ER、PGR、HER2、KI67）WSI。100 例验证集，200 例测试集。时间线：如下表所示 6 月 1 日 数据集发布 8 月 19 日 结果提交 8 月 26 日 获奖者代码提交 主办单位：卡罗林斯卡学院、图尔库大学、坦佩雷大学 图像分割-道内和道外前庭神经鞘瘤与耳蜗（crossMoDA2023） 主页：crossMoDA任务：跨模态图像分割目标：道内和道外前庭神经鞘瘤与耳蜗跨模态分割（从 ceT1 到 hrT2，如图 4 所示） 图4. 跨模态分割 数据集：训练集是 227 例 contrast-enhanced T1 和 295 例 high resolution T2 头部影像，验证集是 96 例 high resolution T2 头部影像，测试集是 365 例 high resolution T2 头部影像。如图 5 所示，所有数据都是多机构、异构扫描的。 图5. 数据集构成 时间线：如下表所示 4 月 15 日 训练和验证数据发布 4 月 27 日 验证阶段开始 7 月 1 日 评估阶段开始 7 月 10 日 评估阶段结束 10 月 8 日 挑战结果将在 MICCAI 2023 上公布 11 月 30 日 在 MICCAI 2023 BrainLes 研讨会上介绍方法 12 月 向该领域的高影响力期刊提交合作论文，总结挑战赛的结果 主办单位：如下图所示 图6. crossMoDA 主办单位和成员 图像分割-腹部 CT 器官与肿瘤（FLARPEACT2023） 主页：CodaLab - Competition任务：图像分割目标：在腹部 CT 扫描中分割 13 个器官（肝、脾、胰腺、右肾、左肾、胃、胆囊、食道、主动脉、下腔静脉、右肾上腺、左肾上腺和十二指肠）和一个包括各种癌症类型的肿瘤（如癌症、肾癌、胃癌、胰腺癌症、结肠癌），如图 7 所示。 图7. 分割目标总览 数据集：共 4000 例，2200 例有部分标签，1800 例没有标签。时间线：如下表所示 4 月 1 日 - 7 月 31 日 加入挑战赛并下载数据 4 月 15 日 - 7 月 31 日 结果提交 8 月 1 日 - 8 月 15 日 测试结果 主办单位：多伦多大学 跨领域少样本学习（L2L2023） 主页：Learn2Learn Challenge MICCAI任务：跨领域少样本学习（Cross-Domain Few-shot Learning, CD-FSL）目标：在提供的 MIMeta 数据集上做 CD-FSL，如图 8 所示。 图8. L2L2023 流程介绍 数据集：17 个公开数据集，共包含 28 个任务。时间线：如下表所示 5 月 2 日 数据集发布 5 月 25 日 提交入口打开 6 月 29 日 报名截止日期 8 月 10 日 提交截止日期 8 月 31 日 Top 团队联系 10 月 8 日 - 10 月 12 日 MICCAI 2023 公布结果 主办单位：麻省理工学院、MEVIS、吕贝克大学 异常检测-脑部 &amp; 腹部（MOOD2023） 主页：Medical Out-of-Distribution Analysis Challenge任务：异常检测目标：1）分析每个病例是否存在异常，并给出异常概率。分数必须在 [0-1] 中，其中 0 表示没有异常，1表示最异常的输入； 2）分析每个病例中每个体素是否存在异常，并给出异常概率。分数必须在 [0-1] 中，其中 0 表示没有异常，1 表示最异常的输入。数据集：脑部MRI 数据集，共 800 例；腹部 CT 数据集，共 550 例。时间线：如下表所示 4 月 1 日 数据集发布 8 月 30 日 注册截止日期 9 月 1 日 结果提交截止日期 9 月 7 日 摘要提交截止日期 10 月 8 日 - 10 月 12 日 结果公布 主办单位：德国癌症研究中心 图像分割-多发性神经纤维瘤（WBMRI-NF2023） 主页：WBMRI-NF 2023: Neurofibromatosis Tumor Segmentation on Whole-body MRI Challenge任务：图像分割目标：在全身 MRI（Whole-Body MRI, WB-MRI）上分割多发性神经纤维瘤（Neurofibromatosis, NF）肿瘤。数据集：共 400 例全身 MRI，200 例训练集，50 例验证集，150 例不可见测试集。 图9. 全身肿瘤分布 时间线：如下表所示 3 月 28 日 挑战赛页面开放 4 月 1 日 训练数据发布 5 月 14 日 验证数据发布及结果提交 7 月 22 日 开放验证阶段截止日期 8 月 1 日 论文提交截止日期 8 月 15 日 代码提交截止日期 9 月 1 日 结果公布日期 主办单位：麻省总医院、美国国家癌症研究所、Sage Bionetworks、威斯康星大学麦迪逊分校、英特尔 图像分割-神经母细胞瘤（SPPIN2023） 主页：Surgical Planning in Pediatric Neuroblastoma - Grand Challenge任务：图像分割目标：1）神经母细胞瘤的全监督分割； 2）肿瘤相关腹部器官和血管的半监督分割。 图10. 神经母细胞瘤及其相关腹部器官和血管的分割 数据集：共 39 例 MRI。时间线：如下表所示 4 月 14 日 训练数据发布 5 月 1 日 示例数据发布 5 月 22 日 初步测试阶段开始 7 月 15 日 提交入口开启 8 月 15 日 提交入口关闭 10 月 8 日 - 10 月 12 日 结果公布 主办单位：Princess Máxima 儿童肿瘤医院、乌得勒支大学 图像分割、图像分类、目标检测-乳腺肿瘤（TSCD-ABUS2023） 主页：TDSC-ABUS2023 - Grand Challenge任务：图像分割、图像分类、目标检测目标：在三维乳腺超声影像中检测、分割和分类（良恶性）肿瘤。 图11. 三维乳腺肿瘤 数据集：共 200 例超声影像，100 例训练集，30 例验证集，70例测试集时间线：如下表所示 3 月 28 日 注册 4 月 6 日 训练数据发布 7 月 15 日 验证数据发布，结果提交入口开放 7 月 20 日 结果提交截止时间 7 月 20 日 测试数据发布，论文提交入口开放 7 月 30 日 论文提交截止时间 9 月 18 日 结果公布 主办单位：哈尔滨工业大学、哈尔滨医科大学 图像重建-多器官超声影像（Ultrasound Image Enhancement Challenge 2023） 主页：Ultrasound Image Enhancement challenge 2023 - Grand Challenge任务：图像重建目标：从低质量的超声图像中重建出高质量的超声图像。数据集：来自 109 名患者的 3000 张超声图像（1500 对低质量和高质量图像），包括甲状腺、颈动脉、肝脏、乳房和肾脏五个器官。时间线：如下表所示 5 月 10 日 训练数据发布 6 月 20 日 - 7 月 30 日 验证阶段 7 月 20 日 - 8 月 20 日 测试阶段 9 月 10 日 结果公布 主办单位：复旦大学、上海大学、复旦大学附属肿瘤医院、斯坦福","categories":[{"name":"推荐","slug":"推荐","permalink":"https://chenluda.github.io/categories/%E6%8E%A8%E8%8D%90/"}],"tags":[{"name":"Competition","slug":"Competition","permalink":"https://chenluda.github.io/tags/Competition/"}]},{"title":"AI 学术产品速览（一）","slug":"速览/AI学术产品产品一","date":"un66fin66","updated":"un33fin33","comments":true,"path":"2023/05/13/速览/AI学术产品产品一/","link":"","permalink":"https://chenluda.github.io/2023/05/13/%E9%80%9F%E8%A7%88/AI%E5%AD%A6%E6%9C%AF%E4%BA%A7%E5%93%81%E4%BA%A7%E5%93%81%E4%B8%80/","excerpt":"","text":"在大模型时代，人工智能学术产品已经成为科学研究领域的一大助力。这些产品通过提高科研人员的工作效率，实现了对科研过程的优化，从而推动科学进步和技术创新。随着不断更新的算法和技术，AI 学术产品已经逐渐渗透到各个学科领域，为科研人员在数据分析、文献检索、实验设计等方面提供了极大的便利。 开源产品学术优化：chatgpt_academic源代码：https://github.com/binary-husky/chatgpt_academic 在线网址：academic-chatgpt - a Hugging Face Space by qingxu98 科研工作专用ChatGPT&#x2F;GLM拓展，特别优化学术Paper润色体验，模块化设计支持自定义快捷按钮&amp;函数插件，支持代码块表格显示，Tex公式双显示，新增Python和C++项目剖析&amp;自译解功能，PDF&#x2F;LaTex论文翻译&amp;总结功能，支持并行问询多种LLM模型，支持gpt-3.5&#x2F;gpt-4&#x2F;chatglm chatgpt_academic功能模块： 论文总结 &amp; 辅助阅读：ChatPaper源代码：GitHub - kaixindelele&#x2F;ChatPaper: Use ChatGPT to summarize the arXiv papers. 全流程加速科研，利用chatgpt进行论文总结+润色+审稿+审稿回复 在线网址：ChatPaper （润色，审稿，审稿回复和题目生成，应该是没上线网站，在源代码中可以找到源码） 全流程加速科研，利用chatgpt进行论文总结+润色+审稿+审稿回复 ChatPaper功能模块： 论文评审 &amp; 评审回复：ChatReviewer &amp; ChatResponse源代码：GitHub - nishiwen1214&#x2F;ChatReviewer: ChatReviewer: 使用ChatGPT分析论文优缺点，提出改进建议 在线网址：ChatReviewer - a Hugging Face Space by ShiwenNiChatResponse - a Hugging Face Space by ShiwenNi ChatReviewer 是一款基于ChatGPT-3.5的API开发的智能论文分析与建议助手。其用途如下1）对论文的优缺点进行快速总结和分析，提高科研人员的文献阅读和理解的效率，紧跟研究前沿；2）对自己的论文进行分析，根据ChatReviewer生成的改进建议进行查漏补缺，进一步提高自己的论文质量。ChatResponse 是一款根据审稿人的评论自动生成作者回复的AI助手。其用途为：根据输入的审稿意见，ChatResponse会自动提取其中各个审稿人的问题和担忧，并生成点对点的回复。 ChatReviewerChatResponse 题目生成：ChatGenTitle源代码：GitHub - WangRongsheng&#x2F;ChatGenTitle: ChatGenTitle：使用百万arXiv论文信息在LLaMA模型上进行微调的论文题目生成模型 使用百万arXiv论文信息在LLaMA模型上进行微调的论文题目生成模型 ChatGenTitle 商业产品学术写作：writefull X网址：AI Writing Tools for Students and Researchers 使用人工智能来帮助学术写作 writefull X浅试了一下，质量挺高的 AI 学术产品，应该正在孵化，后续会更新评测。 目前该网站有以下功能： 论文标题生成。根据摘要内容生成论文题目。 根据 Segment Anything 论文的摘要生成的标题 论文摘要生成。根据论文内容生成论文摘要。 根据 Segment Anything 论文的 Introduction 生成的摘要 降重？ 改写 Segment Anything 论文 Introduction 的第一句 普通语句学术化。将普通语句学术化。 GPT 检测。根据输入的文本，判断其是否为 GPT 生成。 论文总结 &amp; 辅助阅读：SCISPACE网址：https://typeset.io/ 你的 AI 研究助理在几分钟内完成数小时的阅读和理解突出显示令人困惑的文本、数学和表格，以获得简单的解释提出后续问题并立即得到答案一种无需指定关键字就能搜索和查找相关论文的新方法 论文总结 &amp; 辅助阅读：PandaGPT网址：PandaGPT, file reading made easy 厌倦了阅读？让PandaGPT为您处理所有这些！上传一个文件，询问任何问题。 和 SCISPACE 比，还有很长路要走… PandaGPT 论文总结：briefgpt网址：BriefGPT AI 论文速递 briefgpt 论文总结 &amp; 辅助阅读：AMiner网址：AMiner - AI赋能科技情报挖掘-学术搜索-论文检索-论文专利-文献追踪-学者画像 AI 帮你理解科学 AMiner","categories":[{"name":"推荐","slug":"推荐","permalink":"https://chenluda.github.io/categories/%E6%8E%A8%E8%8D%90/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"}]},{"title":"代码 | 获取 arXiv 论文作者单位","slug":"代码/代码-获取-arXiv-论文作者单位-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/代码/代码-获取-arXiv-论文作者单位-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E8%8E%B7%E5%8F%96-arXiv-%E8%AE%BA%E6%96%87%E4%BD%9C%E8%80%85%E5%8D%95%E4%BD%8D-Glenn/","excerpt":"","text":"前几天写了些代码，实现了每天定点向微信推送 arXiv 的最新文章，推送内容包括论文标题、论文网址、首次发布时间、摘要。现在需要提供作者单位，代码已集成在上一篇文章的代码中。 论文标题、论文网址、首次发布时间、摘要等内容都可以通过 arXiv 的接口直接获取。但它没有提供作者单位。 作者单位在论文中的位置随期刊或会议格式而变动，但通常都在第一页。 所以我们需要先将论文 pdf 的第一页解析为可识别的文本格式。 这里可以使用 pdfplumber 库。它可以按页处理 pdf ，获取页面文字，提取表格等操作。 获取到第一页的文本后，就可以识别机构名称了。 这里有三种思路： 一是设置正则化规则去匹配机构名称，这种方式运算量太大，程序很容易卡死，所以直接被淘汰； 二是使用命名实体识别的模型，我这里只使用了两个，一个是 spacy 的 en_core_web_sm，另一个是 huggingface spaces 的 dslim&#x2F;bert-base-NER。效果都不太好，大家可以尝试使用更具有针对性的 NER 模型。 三是使用 chatgpt，因为需要 api key，这种方式本来是不想用的，但无奈效果太好了。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121&#x27;&#x27;&#x27;Description: 获取 arXiv 论文作者单位Version: 1.0Author: GlennEmail: chenluda01@outlook.comDate: 2023-04-24 15:32:39FilePath: \\12-arxivdownload\\getAffiliation.pyCopyright (c) 2023 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import osimport refrom io import BytesIOimport openaiimport pdfplumberimport requestsimport spacyfrom transformers import pipeline# 设置 OpenAI 的 API keyos.environ[&#x27;OPENAI\\_API\\_KEY&#x27;] = &#x27;your\\_openai\\_api\\_key&#x27;openai.api\\_key = os.getenv(&#x27;OPENAI\\_API\\_KEY&#x27;)def get\\_affiliation\\_by\\_bert(text): &quot;&quot;&quot; 使用 bert 模型获取作者单位 &quot;&quot;&quot; # 初始化 BERT NER nlp = pipeline(&quot;ner&quot;, model=&quot;dslim/bert-base-NER&quot;, tokenizer=&quot;dslim/bert-base-NER&quot;) entities = nlp(text) institutions = set() # 提取组织实体 for i, entity in enumerate(entities): if entity[&#x27;entity&#x27;].startswith(&#x27;B-ORG&#x27;): org\\_name = entity[&#x27;word&#x27;] j = i + 1 while j &lt; len(entities) and entities[j][&#x27;entity&#x27;].startswith(&#x27;I-ORG&#x27;): org\\_name += &#x27;&#x27; + entities[j][&#x27;word&#x27;][2:] j += 1 institutions.add(org\\_name) author\\_affiliation = list(institutions) return author\\_affiliationdef get\\_affiliation\\_by\\_spacy(text): &quot;&quot;&quot; 使用 spacy 模型获取作者单位 &quot;&quot;&quot; # 加载预训练模型 nlp = spacy.load(&#x27;en\\_core\\_web\\_sm&#x27;) doc = nlp(text) institutions = set() # 提取组织实体 for ent in doc.ents: if ent.label\\_ == &quot;ORG&quot;: institutions.add(ent.text) author\\_affiliation = list(institutions) return author\\_affiliationdef get\\_affiliation\\_by\\_openai(text): &quot;&quot;&quot; 使用 OpenAI API 模型获取作者单位 &quot;&quot;&quot; prompt = &#x27;Extract the organization names from the given text: &#x27; + text # 调用 GPT-3.5 的接口 response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a very powerful named entity recognition model.&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125; ], ) output = response.choices[0].message.content.strip() institutions = [o.strip() for o in output.split(&quot;\\n&quot;)] author\\_affiliation = list(set(institutions)) return author\\_affiliationdef get\\_text\\_by\\_pdfplumber(pdf\\_url, methodType=&quot;openai&quot;): &quot;&quot;&quot; 使用 pdfplumber 提取 PDF 文本并获取作者单位 &quot;&quot;&quot; # 从 URL 获取 PDF 文件内容 response = requests.get(pdf\\_url) pdf\\_content = BytesIO(response.content) # 使用 pdfplumber 提取 PDF 文本 with pdfplumber.open(pdf\\_content) as pdf: first\\_page = pdf.pages[0] text = first\\_page.extract\\_text() # 根据方法类型选择实体识别方法 if methodType == &#x27;openai&#x27;: author\\_affiliation = get\\_affiliation\\_by\\_openai(text) if methodType == &#x27;spacy\\_model&#x27;: author\\_affiliation = get\\_affiliation\\_by\\_spacy(text) if methodType == &#x27;bert\\_model&#x27;: author\\_affiliation = get\\_affiliation\\_by\\_bert(text) return author\\_affiliationif \\_\\_name\\_\\_==&quot;\\_\\_main\\_\\_&quot;: pdf\\_url = &#x27;https://arxiv.org/pdf/2304.10864.pdf&#x27; methodType = &quot;bert\\_model&quot; author\\_affiliation = get\\_text\\_by\\_pdfplumber(pdf\\_url, methodType) print(&#x27;Author affiliation are: &#x27;, author\\_affiliation) 以这篇文章[1]为例： 题目：FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation单位：北京科技大学、中佛罗里达大学、伯明翰大学、理海大学论文网址：https://arxiv.org/abs/2304.10864论文代码：未公开首次发布时间：2023 年 4 月 21 日 金标准： Author affiliation are: [‘School of Automation and Electrical Engineering, University of Science and Technology Beijing‘, ‘Center for Research in Computer Vision, University of Central Florida’, ‘University of Birmingham‘, ‘Lehigh University‘] 使用 bert 模型获取作者单位： Author affiliation are: [‘Shan‘, ‘Originalage‘, ‘LichaS‘, ‘Jiang‘, ‘UniversityofCentralFida‘, ‘UniversityofScienceT‘] 使用 spacy 模型获取作者单位： Author affiliation are: [‘MIM‘, ‘Masked Image Modeling‘, ‘Original Image‘, ‘NLP‘, ‘MLM‘, ‘Fourier Spectrum‘, ‘Medical Image Segmentation‘, ‘MAE‘] 使用 OpenAI API 模型获取作者单位： Author affiliation are: [‘- University of Science and Technology Beijing‘, ‘- University of Birmingham‘, ‘- Center for Research in Computer Vision, University of Central Florida‘, ‘- FreMAE‘, ‘- Lehigh University‘] 参考 ^Wang, W. et al. FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation. arXiv preprint arXiv:2304.10864 (2023).","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"}]},{"title":"代码 | 每天定点向微信推送 arXiv 最新文章","slug":"代码/代码-每天定点向微信推送-arXiv-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/代码/代码-每天定点向微信推送-arXiv-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E6%AF%8F%E5%A4%A9%E5%AE%9A%E7%82%B9%E5%90%91%E5%BE%AE%E4%BF%A1%E6%8E%A8%E9%80%81-arXiv-Glenn/","excerpt":"","text":"作为科研人，每天早起的第一件事， 就是打开 arXiv 查找自己 follow 的领域有没有最新的文章。 现在我想把这件事自动化： 类似部署自动打卡函数，在阿里云函数计算 FC 中设置一个带有定时触发器的云函数， 再将每天定点向微信推送 arXiv 最新文章的脚本程序部署上去， 实现： 根据关键词搜索在 arXiv 发布的文章； 判断文章的发布日期是否是前一天（或指定日期），如果是，则保留； 使用 Server 酱将前一天文章的标题、网址、摘要推送至微信。 效果展示： 这里我为了展示效果，将条件“前一天发布”改为了特定条件“2023 年 4 月 12 日发布”。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153&#x27;&#x27;&#x27;Description: 每天定点向微信推送 arXiv 最新文章Version: 1.0Author: GlennEmail: chenluda01@outlook.comDate: 2023-04-19 08:35:13FilePath: \\12-arxivdownload\\index.pyCopyright (c) 2023 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import datetimeimport osimport timefrom io import BytesIOimport openaiimport pdfplumberimport requestsdef get\\_author\\_affiliation(pdf\\_url): &quot;&quot;&quot; 从 PDF 文件中提取作者单位信息 &quot;&quot;&quot; response = requests.get(pdf\\_url) pdf\\_content = BytesIO(response.content) author\\_affiliation = [] with pdfplumber.open(pdf\\_content) as pdf: first\\_page = pdf.pages[0] text = first\\_page.extract\\_text() # 设置提示词 prompt = &#x27;Extract the organization names from the given text: &#x27; + text # 调用 GPT-3.5 的接口 response = openai.ChatCompletion.create( model=&quot;gpt-3.5-turbo&quot;, messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a very powerful named entity recognition model.&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125; ], ) output = response.choices[0].message.content.strip() institutions = [o.strip() for o in output.split(&quot;\\n&quot;)] author\\_affiliation = list(set(institutions)) return author\\_affiliationdef get\\_yesterday(): &quot;&quot;&quot; 获取前一天的日期 &quot;&quot;&quot; today = datetime.datetime.now() yesterday = today - datetime.timedelta(days=1) return yesterday.strftime(&#x27;%Y-%m-%d&#x27;)def search\\_arxiv\\_papers(search\\_term, yester\\_date, max\\_results=10): &quot;&quot;&quot; 在 arxiv 按照关键词查找前一天的论文 &quot;&quot;&quot; papers = [] base\\_url = &#x27;http://export.arxiv.org/api/query?&#x27; search\\_query = f&#x27;search\\_query=all:&#123;search\\_term&#125;&amp;start=0&amp;max\\_results=&#123;max\\_results&#125;&amp;sortBy=submittedDate&amp;sortOrder=descending&#x27; response = requests.get(base\\_url + search\\_query) if response.status\\_code != 200: print(&quot;请求失败，请检查你的查询参数。&quot;) return feed = response.text entries = feed.split(&#x27;&lt;entry&gt;&#x27;)[1:] if not entries: print(&quot;没有找到与搜索词匹配的论文。&quot;) return for entry in entries: # 获取标题、摘要、链接、首次发布日期 title = entry.split(&#x27;&lt;title&gt;&#x27;)[1].split(&#x27;&lt;/title&gt;&#x27;)[0].strip() summary = entry.split(&#x27;&lt;summary&gt;&#x27;)[1].split(&#x27;&lt;/summary&gt;&#x27;)[0].strip() url = entry.split(&#x27;&lt;id&gt;&#x27;)[1].split(&#x27;&lt;/id&gt;&#x27;)[0].strip() pub\\_date = entry.split(&#x27;&lt;published&gt;&#x27;)[1].split(&#x27;&lt;/published&gt;&#x27;)[0] pub\\_date = datetime.datetime.strptime(pub\\_date, &quot;%Y-%m-%dT%H:%M:%SZ&quot;).strftime(&quot;%Y-%m-%d&quot;) # 获取 PDF 链接 pdf\\_url = entry.split(&#x27;&lt;link href=&quot;&#x27;)[1].split(&#x27;&quot; rel=&quot;alternate&#x27;)[0].strip() pdf\\_url = pdf\\_url.replace(&quot;abs&quot;, &quot;pdf&quot;) # 获取作者单位 author\\_affiliation = get\\_author\\_affiliation(pdf\\_url) if pub\\_date == yester\\_date: papers.append(&#123; &#x27;title&#x27;: title, &#x27;url&#x27;: url, &#x27;pub\\_date&#x27;: pub\\_date, &#x27;summary&#x27;: summary, &#x27;author\\_affiliation&#x27;: author\\_affiliation &#125;) return papersdef send\\_wechat\\_message(title, content, SERVERCHAN\\_API\\_KEY): &quot;&quot;&quot; 使用 Serve 酱向微信推送论文信息 &quot;&quot;&quot; url = f&#x27;https://sctapi.ftqq.com/&#123;SERVERCHAN\\_API\\_KEY&#125;.send&#x27; params = &#123; &#x27;title&#x27;: title, &#x27;desp&#x27;: content, &#125; requests.post(url, params=params)# def handler(event, context):if \\_\\_name\\_\\_ == &#x27;\\_\\_main\\_\\_&#x27;: # 设置 OPENAI 的 API\\_KEY os.environ[&#x27;OPENAI\\_API\\_KEY&#x27;] = &#x27;your\\_openai\\_api\\_key&#x27; openai.api\\_key = os.getenv(&#x27;OPENAI\\_API\\_KEY&#x27;) # 修改为自己 Serve 酱 API SERVERCHAN\\_API\\_KEY = &#x27;SCT206421TeQFPxkyqpZQFegFELJaKCW6d&#x27; # 关键词 search\\_term = &#x27;&quot;masked image model&quot;&#x27; # 获取的最大论文数 max\\_results = 10 # 获取前一天的日期 yester\\_date = get\\_yesterday() # 在 arxiv 按照关键词查找前一天的论文 papers = search\\_arxiv\\_papers(search\\_term, yester\\_date, max\\_results) for paper in papers: title = paper[&#x27;title&#x27;] url = paper[&#x27;url&#x27;] pub\\_date = paper[&#x27;pub\\_date&#x27;] summary = paper[&#x27;summary&#x27;] msg\\_title = f&#x27;标题：&#123;title&#125;&#x27; msg\\_url = f&#x27;论文网址：&#123;url&#125;&#x27; msg\\_pub\\_date = f&#x27;首次发布时间：&#123;pub\\_date&#125;&#x27; msg\\_summary = f&#x27;摘要：&#123;summary&#125;&#x27; msg\\_content = f&#x27;&#123;msg\\_title&#125;\\n\\n&#123;msg\\_url&#125;\\n\\n&#123;msg\\_pub\\_date&#125;\\n\\n&#123;msg\\_summary&#125;&#x27; send\\_wechat\\_message(title, msg\\_content, SERVERCHAN\\_API\\_KEY) # 为避免触发微信推送服务的限制，等待一段时间后再发送下一篇论文的信息 time.sleep(5) 使用 gpt3.5 识别论文的作者单位，如果不需要的话，可以注释掉相关代码。 其中，handler 函数中的 SERVERCHAN_API_KEY、search_term 、max_results 是根据需求修改的变量。 SERVERCHAN_API_KEY：自己 Serve 酱的 API（下方会解释）。 search_term：搜索论文的关键词，如果使用双引号将词包裹起来，表明论文中必须出现这个词，例如，’Masked Image Model’ 和 ‘“Masked Image Model”‘ 搜索结果不同，具体请查看 arxiv 的文献检索说明。 max_results：检索论文的最大数量。 如果想直接本地运行，则可以将 def handler(event, context): 改为 if __name__ &#x3D;&#x3D; ‘__main__‘: 。 1. Server 酱我们需要使用到Server 酱，这是一款从服务器、路由器等设备上推消息到手机的工具。我们需要使用她实现向微信推送消息的功能。 打开官网网址：https://sct.ftqq.com/； 微信扫码登陆后，进入 Key&amp;API 模块； 将 SendKey 复制替换代码中的 Your-Server-API。 2. 阿里云函数计算 FC函数计算（Function Compute）是一个事件驱动的全托管 Serverless 计算服务，无需管理服务器等基础设施，只需编写代码并上传，函数计算会准备好计算资源，并以弹性、可靠的方式运行代码。 打开官网网址：国内唯一入选Forrester领导者象限，登陆后，进入管理控制台； 进入服务及函数模块，点击创建服务按钮； 登陆后，进入管理控制台，再进入服务及函数模块，点击创建服务按钮 将弹窗中的名称和描述填完后，点击左下角确定按钮； 将弹窗中的名称和描述填完后，点击左下角确定按钮 进入函数管理模块，点击创建函数按钮； 进入函数管理模块，点击创建函数按钮 填写函数名称； 将上面给出的脚本代码保存为 index.py 放入一个名为 arxiv_push_code 的空文件夹中； 压缩该文件夹，将压缩包拖至代码包处； 点击页面下方创建按钮； 将 arxiv_push_code 文件夹中的 index.py 拖至上级文件夹 CODE 处； 将 arxiv_push_code 文件夹删除； 进入触发器管理模块，点击创建触发器按钮； 进入触发器管理模块，点击创建触发器按钮 在弹窗中填写相关内容，其中“指定时间”就是在设置每天几点向微信推送文章； 填写完后点击左下方确定按钮，完成触发器创建； 回到函数代码页面，点击部署代码按钮，出现部署成功提示后，表示部署成功； 点击测试函数，可以直接运行。","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"}]},{"title":"代码 | 将知乎专栏文章转换为 Markdown 文件保存到本地","slug":"代码/代码-将知乎专栏文章转换为-Markd-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/代码/代码-将知乎专栏文章转换为-Markd-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E5%B0%86%E7%9F%A5%E4%B9%8E%E4%B8%93%E6%A0%8F%E6%96%87%E7%AB%A0%E8%BD%AC%E6%8D%A2%E4%B8%BA-Markd-Glenn/","excerpt":"","text":"最近想构建一个本地知识库。 需要从知乎下载文章、专栏、回答，并以 Markdown 格式保存到本地。 自己写了个脚本， 可以判断给定 url 的类型，是文章、专栏还是回答，三种类型的处理方式不同； 然后将图片保存至本地，并将转换的 Markdown 中图片 url 更换为本地路径，使图片可以在本地显示。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205&#x27;&#x27;&#x27;Description: 将知乎专栏文章转换为 Markdown 文件保存到本地Version: 1.0Author: 陈路达Email: chenluda01@outlook.comDate: 2023-04-18 10:48:38FilePath: \\11-zhihuToMd\\main.pyCopyright (c) 2022 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import osimport reimport urllib.parseimport requestsfrom bs4 import BeautifulSoupfrom markdownify import markdownify as mddef download\\_image(url, save\\_path): &quot;&quot;&quot; 从指定url下载图片并保存到本地 &quot;&quot;&quot; if url.startswith(&quot;data:image/&quot;): # 如果链接以 &quot;data:&quot; 开头，则直接写入数据到文件 with open(save\\_path, &quot;wb&quot;) as f: f.write(url.split(&quot;,&quot;, 1)[1].encode(&quot;utf-8&quot;)) else: response = requests.get(url) with open(save\\_path, &quot;wb&quot;) as f: f.write(response.content)def get\\_valid\\_filename(s): &quot;&quot;&quot; 将字符串转换为有效的文件名 &quot;&quot;&quot; s = str(s).strip().replace(&#x27; &#x27;, &#x27;\\_&#x27;) return re.sub(r&#x27;(?u)[^-\\w\\_]&#x27;, &#x27;&#x27;, s)def judge\\_zhihu\\_type(url): &quot;&quot;&quot; 判断url类型 &quot;&quot;&quot; response = requests.get(url) soup = BeautifulSoup(response.content, &quot;html.parser&quot;) if soup.text.find(&quot;知乎专栏&quot;) != -1: # 如果是专栏，将所有文章放在一个以专栏标题命名的文件夹中 title = soup.text.split(&#x27;-&#x27;)[0].strip() folder\\_name = get\\_valid\\_filename(title) os.makedirs(folder\\_name, exist\\_ok=True) os.chdir(folder\\_name) parse\\_zhihu\\_column(url) elif url.find(&quot;answer&quot;) != -1: # 如果是回答 parse\\_zhihu\\_answer(url) else: # 如果是单篇文章 parse\\_zhihu\\_article(url)def save\\_and\\_transform(title\\_element, content\\_element, author, url): &quot;&quot;&quot; 转化并保存为 Markdown 格式文件 &quot;&quot;&quot; # 获取标题和内容 if title\\_element is not None: title = title\\_element.text.strip() else: title = &quot;Untitled&quot; # 防止文件名称太长，加载不出图像 markdown\\_title = get\\_valid\\_filename(title)[0:20] markdown\\_title = f&quot;&#123;markdown\\_title&#125;\\_&#123;author&#125;&quot; if content\\_element is not None: # 将 css 样式移除 for style\\_tag in content\\_element.find\\_all(&quot;style&quot;): style\\_tag.decompose() for img\\_lazy in content\\_element.find\\_all(&quot;img&quot;, class\\_=lambda x: &#x27;lazy&#x27; in x if x else True): img\\_lazy.decompose() # 处理回答中的图片 for img in content\\_element.find\\_all(&quot;img&quot;): # 将图片链接替换为本地路径 img\\_url = img.get(&quot;data-original&quot;, img.get(&quot;src&quot;, None)) if img\\_url is None: continue img\\_name = urllib.parse.quote(os.path.basename(img\\_url)) img\\_path = f&quot;&#123;markdown\\_title&#125;\\_files/&#123;img\\_name&#125;&quot; # 如果图片链接中 .jpg 后面还有字符串则直接截停 if img\\_path.find(&#x27;.jpg&#x27;) + 3 != len(img\\_path) - 1: img\\_path = img\\_path[0: img\\_path.find(&#x27;.jpg&#x27;) + 4] img[&quot;src&quot;] = img\\_path # 下载图片并保存到本地 os.makedirs(os.path.dirname(img\\_path), exist\\_ok=True) download\\_image(img\\_url, img\\_path) # 在图片后插入换行符 img.insert\\_after(&#x27;\\n\\n&#x27;) # 在 &lt;/figcaption&gt; 后面加上换行符 for figcaption in content\\_element.find\\_all(&quot;figcaption&quot;): figcaption.insert\\_after(&#x27;\\n\\n&#x27;) # 获取回答文本内容 content = content\\_element.decode\\_contents().strip() # 转换为 markdown content = md(content) else: content = &quot;&quot; # 转化为 Markdown 格式 if content: markdown = f&quot;# &#123;title&#125;\\n\\n \\*\\*Author:\\*\\* [&#123;author&#125;]\\n\\n \\*\\*Link:\\*\\* [&#123;url&#125;]\\n\\n&#123;content&#125;&quot; else: markdown = f&quot;# &#123;title&#125;\\n\\nContent is empty.&quot; # 保存 Markdown 文件 with open(f&quot;&#123;markdown\\_title&#125;.md&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(markdown)def parse\\_zhihu\\_article(url): &quot;&quot;&quot; 解析知乎文章并保存为Markdown格式文件 &quot;&quot;&quot; # 发送GET请求获取网页内容 response = requests.get(url) # 解析HTML soup = BeautifulSoup(response.content, &quot;html.parser&quot;) # 找到文章标题和内容所在的元素 title\\_element = soup.select\\_one(&quot;h1.Post-Title&quot;) content\\_element = soup.select\\_one(&quot;div.Post-RichText&quot;) author = soup.select\\_one(&#x27;div.AuthorInfo &gt; div.AuthorInfo-content &gt; div.AuthorInfo-head&#x27;).text.strip() # 解析知乎文章并保存为Markdown格式文件 save\\_and\\_transform(title\\_element, content\\_element, author, url)def parse\\_zhihu\\_answer(url): &quot;&quot;&quot; 解析知乎回答并保存为 Markdown 格式文件 &quot;&quot;&quot; # 发送 GET 请求获取网页内容 response = requests.get(url) # 解析 HTML soup = BeautifulSoup(response.content, &quot;html.parser&quot;) # 找到回答标题、内容、作者所在的元素 title\\_element = soup.select\\_one(&quot;h1.QuestionHeader-title&quot;) content\\_element = soup.select\\_one(&quot;div.RichContent-inner&quot;) author = soup.select\\_one(&#x27;div.AuthorInfo &gt; div.AuthorInfo-content &gt; div.AuthorInfo-head&#x27;).text.strip() # 解析知乎文章并保存为Markdown格式文件 save\\_and\\_transform(title\\_element, content\\_element, author, url)def parse\\_zhihu\\_column(url): &quot;&quot;&quot; 解析知乎专栏并获取所有文章链接 &quot;&quot;&quot; # 获取所有文章链接 items = [] url\\_template = &quot;https://zhuanlan.zhihu.com/p/&#123;id&#125;&quot; offset = 0 while True: api\\_url = f&quot;/api/v4/columns/&#123;url.split(&#x27;/&#x27;)[-1]&#125;/items?limit=10&amp;offset=&#123;offset&#125;&quot; response = requests.get(f&quot;https://www.zhihu.com&#123;api\\_url&#125;&quot;) data = response.json() items += data[&quot;data&quot;] if data[&quot;paging&quot;][&quot;is\\_end&quot;]: break offset += 10 article\\_links = [url\\_template.format(id=item[&quot;id&quot;]) for item in items] # 遍历所有文章链接，转换为Markdown并保存到本地 for article\\_link in article\\_links: parse\\_zhihu\\_article(article\\_link)if \\_\\_name\\_\\_==&quot;\\_\\_main\\_\\_&quot;: # 回答 # url = &quot;https://www.zhihu.com/question/593914819/answer/2971671307&quot; # 文章 # url = &quot;https://zhuanlan.zhihu.com/p/626703154&quot; # 专栏 url = &quot;https://www.zhihu.com/column/c\\_1620937636624703488&quot; judge\\_zhihu\\_type(url) 1. 先扔个回答的 urlMeta 发布图像分割论文 Segment Anything，将给 CV 研究带来什么影响？ - Glenn的回答 - 知乎 https://www.zhihu.com/question/593914819/answer/2971671307 将会生成一个以“问题标题 + ‘_‘ + 作者名称 + ‘_files’”命名的文件夹和以“问题标题 + ‘_‘ + 作者名称”命名的 .md 文件。 文件夹中保存了本回答的所有图片。 为防止问题标题太长，导致图片加载不出来，在第 20 个字符处做了截断。 2. 单个文章的 urlGlenn：MIM | HPM：引入蒸馏概念自动生成掩蔽策略 将会生成一个以“文章标题 + ‘_‘ + 作者名称 + ‘_files’”命名的文件夹和以“文章标题 + ‘_‘ + 作者名称”命名的 .md 文件。 3. 专栏文章 urlMIM 将会生成一个以“专栏标题”命名的文件夹，里面包含了所有专栏文章的图像文件夹和 .md 文件。","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"}]},{"title":"代码 | 从 OpenReview 获取顶会接收论文集并保存至本地数据库","slug":"代码/代码-从-OpenReview-获取顶-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/代码/代码-从-OpenReview-获取顶-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/%E4%BB%A3%E7%A0%81/%E4%BB%A3%E7%A0%81-%E4%BB%8E-OpenReview-%E8%8E%B7%E5%8F%96%E9%A1%B6-Glenn/","excerpt":"","text":"最近在调整自己的论文追踪网站，发现从 arXiv 的 Comments 扒论文的中稿会议或期刊效率太低，且准确率也不高，想着还不如直接爬取会议接受的论文。 AI 论文追踪 by GlennOpenReview 是一个旨在促进同行评审过程中透明度的平台。通常，OpenReview 公布接收结果的时间比会议官方网站要早。实际上，大部分论文推送网站也是从 OpenReview 上获取会议论文信息的。 OpenReviewGithub 上也有从该网站爬取会议论文集的项目，如 paper_downloader。但是，这个项目是通过直接请求页面元素并根据布局来查找相关论文信息的。由于不同会议在 OpenReview 上的页面布局各不相同，特别是在不同年份之间，这就导致了一个问题：代码变得极其冗余（因为每个会议在不同的年份都需要有自己独特的爬取代码）。 paper_downloader要知道 OpenReview 是有自己的官方 API 文档的 :) 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&#x27;&#x27;&#x27;Description: 从 openreview 获取被接收的顶会论文保存至数据库Version: 1.0Author: GlennEmail: chenluda01@outlook.comDate: 2023-05-04 13:45:40FilePath: \\12-arxivdownload\\main.pyCopyright (c) 2023 by Kust-BME, All Rights Reserved. &#x27;&#x27;&#x27;import openreviewimport mysql.connectorimport timedef get\\_papers\\_from\\_openreview(conference\\_id): &quot;&quot;&quot; 从 openreview 获取被接收的顶会论文 &quot;&quot;&quot; def get\\_accepted\\_forum\\_ids(blind\\_notes): &quot;&quot;&quot; 从提交的论文中获取被接收的论文 &quot;&quot;&quot; forum\\_ids = set() for note in blind\\_notes: for reply in note.details[&quot;directReplies&quot;]: if reply[&quot;invitation&quot;].endswith(&quot;Decision&quot;) and &#x27;Accept&#x27; in reply[&quot;content&quot;][&#x27;decision&#x27;]: forum\\_ids.add(reply[&#x27;forum&#x27;]) return forum\\_ids def format\\_note(note, conference\\_name): &quot;&quot;&quot; 获取需要存储的论文信息 &quot;&quot;&quot; authors\\_string = &#x27;,&#x27;.join(note.content[&#x27;authors&#x27;]) tags\\_string = &#x27;,&#x27;.join(note.content[&#x27;keywords&#x27;]) localTime = time.localtime(note.pdate/1000) strTime = time.strftime(&#x27;%Y-%m-%d&#x27;, localTime) return &#123; &#x27;title&#x27;: note.content[&#x27;title&#x27;], &#x27;url&#x27;: &#x27;https://openreview.net/forum?id=&#x27; + note.forum, &#x27;pub\\_date&#x27;: strTime, &#x27;summary&#x27;: note.content[&#x27;abstract&#x27;], &#x27;authors&#x27;: authors\\_string, &#x27;tags&#x27;: tags\\_string, &#x27;read\\_num&#x27;: 0, &#x27;conference&#x27;: conference\\_name, &#x27;venue&#x27;: note.content[&#x27;venue&#x27;] &#125; # 获取该会议的所有提交论文 submissions = client.get\\_all\\_notes( invitation=conference\\_id + &#x27;/Conference/-/Blind\\_Submission&#x27;, details=&#x27;directReplies&#x27;) # 从提交的论文中获取被接收论文的 id accepted\\_forum\\_ids = get\\_accepted\\_forum\\_ids(submissions) # 通过 id 获取需要存储的论文信息 notes\\_list = [format\\_note(note, conference\\_name) for note in submissions if note.forum in accepted\\_forum\\_ids] return notes\\_listdef insert\\_papers\\_to\\_db(papers): &quot;&quot;&quot; 将获取的论文信息保存至数据库 &quot;&quot;&quot; connection = None try: connection = mysql.connector.connect( host=&#x27;your-host-address&#x27;, user=&#x27;your-username&#x27;, password=&#x27;your-password&#x27;, database=&#x27;your-database&#x27; ) cursor = connection.cursor() for paper in papers: query = &#x27;&#x27;&#x27; INSERT INTO card\\_data (title, abstracts, tags, authors, date, url, read\\_num, conference, venue) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) &#x27;&#x27;&#x27; cursor.execute(query, ( paper[&#x27;title&#x27;], paper[&#x27;summary&#x27;], paper[&#x27;tags&#x27;], paper[&#x27;authors&#x27;], paper[&#x27;pub\\_date&#x27;], paper[&#x27;url&#x27;], paper[&#x27;read\\_num&#x27;], paper[&#x27;conference&#x27;], paper[&#x27;venue&#x27;] )) connection.commit() except mysql.connector.Error as error: print(f&quot;Failed to insert paper: &#123;error&#125;&quot;) finally: if connection is not None and connection.is\\_connected(): cursor.close() connection.close()if \\_\\_name\\_\\_ == &#x27;\\_\\_main\\_\\_&#x27;: client = openreview.Client(baseurl=&#x27;https://api.openreview.net&#x27;) # 会议名称，建议先在官网上查看下收录会议该年的论文是否已经放出 conference\\_name = &#x27;ICLR&#x27; # 会议年份 conference\\_year = &#x27;2023&#x27; # conference\\_name = &#x27;NeurIPS&#x27; # conference\\_year = &#x27;2022&#x27; paper\\_list = get\\_papers\\_from\\_openreview( conference\\_name + &#x27;.cc/&#x27; + conference\\_year) pass if paper\\_list: insert\\_papers\\_to\\_db(paper\\_list) else: print(&quot;No papers found to insert into the database.&quot;) 注：在填写会议名称和会议年份时，建议先去网站上查看下该年该会议的论文是否已经公开。 本地数据库","categories":[{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"}]},{"title":"MIM 的小总结（一）","slug":"Masked_Image_Model/MIM-的小总结-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-的小总结-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-%E7%9A%84%E5%B0%8F%E6%80%BB%E7%BB%93-Glenn/","excerpt":"","text":"距离放出的最新 Masked Image Model（MIM）文章（非特定领域）已经过去 8 天了。 趁这段时间，赶紧对 MIM 做下小总结。 MIM 作为上游任务，其目标应该是为下游任务（例如分类、分割、目标检测）提供一个有利的初始化。 直观上来看，当模型面临具有挑战性的重建任务时，它需要学会识别图像中更复杂、更抽象的特征，以便在缺失数据的情况下进行有效的预测。这意味着编码器需要捕捉更高层次的图像特征和结构信息。通过这种方式，编码器可以学到更丰富的特征表示，从而为下游任务提供更有利的初始化。 通常增加重建任务难度的方式有很多，例如： 改进掩蔽策略。可以通过改进掩蔽策略来增加重建任务的难度。例如，HPM[1] 主张掩蔽富含语义的前景对象，从而迫使模型学习更复杂的特征表示。 HPM 改变重建目标。 1）传统的像素级别重建任务可能会限制模型的表达能力。为了提高模型的性能，可以将重建目标替换为更高级的语义表示，如 MaskFeat[2] 使用 HOG 特征，BEiT-v2[3] 使用 CLIP 特征等。这将迫使模型学习更丰富的特征表示，从而有助于下游任务的性能； 2）深度监督是一种在建模过程中引入不同监督信号的方法。通过让模型关注不同尺度和抽象级别的重建任务，可以促使模型学习更丰富的特征表示。例如，LocalMIM[4] 对不同尺度的图像特征进行监督，DeepMIM[5] 在模型的不同层次引入监督信号，从而强制模型在多个抽象层次上学习有效的特征表示。 LocalMIM 引入额外的代理任务。CMAE[6] 除了重建任务以外，引入了一个对比学习的任务，迫使模型学习更具区分性和局部敏感性的特征表示。MixedAE[7] 引入了同源识别（homologous recognition） 的辅助代理任务，不仅通过明确要求每个 patch 识别同源 patches 来缓解互信息的增加，而且还可以执行 object-aware 的自监督预训练，以获得更好的下游密集感知性能。 MixedAE但当重建任务变得过度困难时，也会导致一系列问题： 如果过度掩蔽前景对象，模型可能会遇到错位（misalignment）问题，即模型在重建图像时，可能无法正确对齐原始图像中的局部结构和细节。这可能会导致生成的图像与原始图像的语义有所偏差，从而影响模型在下游任务中的性能。 MIM 的错位（misalignment）问题 针对复杂的重建目标，可能导致模型过于关注重建任务中的细节，而忽略了下游任务中更为重要的全局信息，从而影响模型性能。 随着重建任务变得越来越困难，模型可能需要更多的计算资源和时间才能收敛。这可能会导致训练成本上升，对硬件资源要求更高，并且需要更多的数据来避免过拟合。 为了解决这些问题，我们可以将思路转变如下： 如何量化重建任务的难度？HPM 使用重建损失作为重建任务难度的指标，而 MixedAE 使用模型输入和重建目标之间的互信息作为量化标准。 如何平衡重建任务的难度和下游任务的需求？可以通过在训练过程中逐渐增加重建任务的难度，或者在训练过程中引入多个不同难度的重建任务，以逐步提高模型的表征能力。 如何选择合适的重建目标？这涉及到对具体下游任务的理解和需求。例如，对于目标检测任务，重建任务应该更注重保留物体的形状和位置信息，而对于图像分类任务，重建任务则应更注重提取图像的语义特征。 那么对应的解决方法： 权衡掩蔽策略。我们把掩蔽策略的选择看作 tradeoff 问题，即前景掩蔽率越高，通过 MIM 获得的信息越多，但同时解码器的重建难度也越高。对此 AutoMAE[8] 将掩模生成和图像重建任务集成到一个完全可微的框架中，用于寻找具有较高信息密度的区域内找到重建难度较低的 patch 来掩蔽。DPPMask[9] 基于行列式点过程 (determinantal point process, DPP)，逐步选择更具有代表性 patches 来保留。特别的，这些方法以及 SemeMAE[10]、HPM 都可以轻易地构建 easy-to-hard 任务。 MAE、SemMAE、AutoMAE 的对比 改变重建任务。使用更简单的任务。例如，MixMIM[11] 和 MixedAE 使用另一幅图像的可见 patch 替换 MIM 的 patch masking&#x2F;zero（可以理解为用 0 来填充 patch），这种创建一个混合图像，然后再进行双重重建的任务，比单纯使用 patch masking&#x2F;zero 再重建的任务要简单。 MixMIM 解耦重建任务。MIM 中由于解码器中也同时输入了编码器输出的编码特征，那么在完成重建任务的时候，就会对这部分也进行优化，这就限制了编码器的表征学习能力。CAE[12] 提出 Latent Contextual Regressor 模块，用于解耦编码和解码的角色，前者专心做内容理解，后者专心去预测目标，从而充分激发编码器表征学习的潜能，进而提升表征质量[13]。 CAE参考 ^Wang, H. et al. Hard Patches Mining for Masked Image Modeling. arXiv preprint arXiv:2304.05919 (2023). ^Wei, C. et al. Masked feature prediction for self-supervised visual pre-training. in Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition 14668–14678 (2022). ^Peng, Z., Dong, L., Bao, H., Ye, Q. &amp; Wei, F. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366 (2022). ^Wang, H. et al. Masked Image Modeling with Local Multi-Scale Reconstruction. arXiv preprint arXiv:2303.05251 (2023). ^Ren, S., Wei, F., Albanie, S., Zhang, Z. &amp; Hu, H. DeepMIM: Deep Supervision for Masked Image Modeling. arXiv preprint arXiv:2303.08817 (2023). ^Huang, Z. et al. Contrastive masked autoencoders are stronger vision learners. arXiv preprint arXiv:2207.13532 (2022). ^Chen, K. et al. Mixed Autoencoder for Self-supervised Visual Representation Learning. arXiv preprint arXiv:2303.17152 (2023). ^Chen, H., Zhang, W., Wang, Y. &amp; Yang, X. Improving Masked Autoencoders by Learning Where to Mask. arXiv preprint arXiv:2303.06583 (2023). ^Xu, J. et al. DPPMask: Masked Image Modeling with Determinantal Point Processes. arXiv preprint arXiv:2303.12736 (2023). ^Li, G. et al. Semmae: Semantic-guided masking for learning masked autoencoders. arXiv preprint arXiv:2206.10207 (2022). ^Liu, J., Huang, X., Liu, Y. &amp; Li, H. Mixmim: Mixed and masked image modeling for efficient visual representation learning. arXiv preprint arXiv:2205.13137 (2022). ^Chen, X. et al. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026 (2022). ^来看看新一代 MIM 青年 CAE(Context AutoEncoder) 如何克服 MAE 中表征学习不充分的问题 - CW不要無聊的風格的文章 - 知乎 https://zhuanlan.zhihu.com/p/519425855","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | 理论：掩蔽图像建模本质上在学习遮挡不变特征","slug":"Masked_Image_Model/MIM-理论掩蔽图像建模本质上在学习遮-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-理论掩蔽图像建模本质上在学习遮-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-%E7%90%86%E8%AE%BA%E6%8E%A9%E8%94%BD%E5%9B%BE%E5%83%8F%E5%BB%BA%E6%A8%A1%E6%9C%AC%E8%B4%A8%E4%B8%8A%E5%9C%A8%E5%AD%A6%E4%B9%A0%E9%81%AE-Glenn/","excerpt":"","text":"题目：Understanding Masked Image Modeling via Learning Occlusion Invariant Feature单位：旷视论文网址：https://arxiv.org/abs/2208.04164论文代码：未公开首次发布时间：2022 年 8 月 8 日 123456@article&#123;kong2022understanding, title=&#123;Understanding masked image modeling via learning occlusion invariant feature&#125;, author=&#123;Kong, Xiangwen and Zhang, Xiangyu&#125;, journal=&#123;arXiv preprint arXiv:2208.04164&#125;, year=&#123;2022&#125;&#125; 前言PixMIM、AutoMAE 都提到了改变掩蔽策略可以提升 MIM 模型性能，而 DeepMIM、LocalMIM 的改进策略都放在编码器和损失函数上，那么究竟哪一种方式才是 MIM 成功的关键？ 主要内容 提出了一种新的 RelaxMIM 框架来近似原始的重建 MIM 方法。根据 RelaxMIM 的观点，MIM 可以解释为对比学习的一种特殊情况：数据增强方式是 patch masking，相似性度量与解码器有关。换句话说，MIM 模型本质上学习遮挡不变特征。 基于 RelaxMIM，作者用更简单的信息损失代替相似度度量。令人惊讶的是，性能与原始模型保持相同。这表明，MIM 框架中的重构解码器并不重要，其他测量也可以很好地工作。patch masking 可能是成功的关键。 为了理解为什么 patch masking 很重要，作者使用很少的图像（例如只有 1 张图像）进行 MIM 预训练，然后在 ImageNet 上对编码器进行微调。虽然经过预训练后，学习到的表征缺乏语义信息，但精细化模型的性能仍然显著优于从头开始的训练模型。 符号公式描述对比学习的表述如下： \\underset{\\theta}{\\min}\\underset{x～\\mathcal D}{\\mathbb{E}}\\mathcal M(z_1,z_2), z_1&#x3D;f_\\theta(\\mathcal T_1(x)),z_2&#x3D;f_\\theta(\\mathcal T_2(x))（1） 其中 \\mathcal D 为数据分布； f_θ(\\cdot) 表示由 θ 参数化的编码器网络； \\mathcal T_1(\\cdot) 和 \\mathcal T_2(\\cdot) 是输入数据上的两个数据增强方式，它定义了需要学习的不变性； \\mathcal M(\\cdot, \\cdot) 是距离函数（或相似性度量），用来度量两个特征图 z_1 和 z_2 之间的相似性。 MIM 的表述如下： \\underset{\\theta}{\\min}\\underset{x～\\mathcal D}{\\mathbb{E}}\\mathcal M(d_\\phi(z),x\\odot(1-M)), z&#x3D;f_\\theta(x\\odot M)（2） 其中 \\odot 表示 element-wise product；M 是 patch mask（如图 1）； f_θ(\\cdot) 和 d_\\phi(\\cdot) 分别是编码器和解码器； z 是可学习表征； \\mathcal M(\\cdot, \\cdot) 是相似性度量方法。 图1. patch mask 解释 假设一：MIM 本质上学习遮挡不变特征【背景】学习不变性特征是对比学习这类自监督方法成功的关键：通过最小化各类增强图像的特征差异，可以鼓励模型学习原始图像的增强不变性特征。 那么我们是否可以将 MIM 看作对比学习的一种特殊情况呢？ 【方法】为了验证该假设，首先需要将 MIM 公式 2 改写为近似对比学习公式 1 的形式。 以 MAE 举例，相似性度量方法设置为 l2−distance。 则 MAE 的损失函数公式可以写成： \\begin{equation}L(x,M)&#x3D;||d_\\phi(f_θ(x\\odot M))\\odot(1−M)−x\\odot(1 − M)||^2\\end{equation}（3） 在原始的 MAE 中，编码器只生成未被掩蔽 patch 的 token，而解码器只在训练过程中预测被掩蔽 patch。在此实验中，为了简单起见，假设编码器生成整个输入 patch 的 token，而解码器也是预测整张图像。 在 MAE 中，特征嵌入的维度是远远大于输入图像维度的，也就是说编码器有足够的空间来捕捉输入图像中的所有信息。因此编码器理论上可以实现在不丢失任何信息的情况下对输入图像进行特征提取。 因此，可以假设对于编码器 f_θ(\\cdot) ，存在一个参数为 \\phi’ 的网络 d’_{\\phi’}(\\cdot) ，满足 d’{\\phi’}(f_θ(x\\odot(1-M)))\\odot(1-M)\\approx x\\odot(1-M) 。 据此，可以将公式 3 重写为以下等效形式： L(x,M)&#x3D;||d_\\phi(f_θ(x\\odot M))\\odot(1−M)−d’_{\\phi’}(f_θ(x\\odot(1-M)))\\odot(1-M)||^2\\ s.t.\\quad\\phi’&#x3D;\\underset{\\phi’}{argmim}\\underset{x’～\\mathcal D}{\\mathbb{E}}||d’_{\\phi’}(f_θ(x’\\odot(1-M)))\\odot(1-M)-x’\\odot(1-M)||^2（4） 其中 d’_{\\phi’}(\\cdot) 近似于 f_θ(\\cdot) 的“逆”表示，因此可以让 d’_{\\phi’}(\\cdot) 使用 d_\\phi(\\cdot) 相同的结构。令 d’ &#x3D; d ，可以构建一个新的相似性度量方法： \\begin{equation} \\overline{\\mathcal{M}{\\phi, \\phi^{\\prime}}}\\left(z_1, z_2\\right) \\triangleq\\left|\\left(d\\phi\\left(z_1\\right)-d_{\\phi^{\\prime}}\\left(z_2\\right)\\right) \\odot(1-M)\\right|^2 \\end{equation}（5） 数据增强方法： \\begin{equation} \\mathcal T_1(x)&#x3D;x\\odot M,\\mathcal T_2(x)&#x3D;x\\odot(1-M) \\end{equation}（6） 则可将 MAE 的损失函数公式 4 改为以下对比学习形式： \\begin{equation} \\begin{aligned}L(x, M ; \\theta, \\phi) &amp; &#x3D;\\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(f_\\theta\\left(\\mathcal{T}_1(x)\\right), f_\\theta\\left(\\mathcal{T}_2(x)\\right)\\right) \\\\text { s.t. } \\quad \\phi^{\\prime} &amp; &#x3D;\\underset{\\phi^{\\prime}}{\\arg \\min } \\underset{x^{\\prime} \\sim \\mathcal{D}}{\\mathbb{E}}\\left|\\left(d_{\\phi^{\\prime}}\\left(f_\\theta\\left(\\mathcal{T}_2\\left(x^{\\prime}\\right)\\right)\\right)-\\mathcal{T}_2\\left(x^{\\prime}\\right)\\right) \\odot(1-M)\\right|^2\\end{aligned} \\end{equation}（7） 【结果与讨论】对于公式 7 ，我们可以把他看作是对比学习的一种特殊情况：损失函数用来最小化来自两个掩蔽变换产生的表征之间的差异。因此，可以推测出 MIM 是在鼓励模型学习原始图像的遮挡不变特征。 虽然公式 7 在理论上明确揭示了 MIM 鼓励学习遮挡不变特征，但公式 7 涉及嵌套优化，这是一个缺点，很难计算。 因此，作者将提出了公式 7 的联合优化形式，命名为 R-MAE（或 RelaxMIM）： \\underset{\\theta, \\phi, \\phi^{\\prime}}{\\min} \\underset{x \\sim \\mathcal{D}}{\\mathbb{E}} \\overline{\\mathcal{M}{\\phi, \\phi^{\\prime}}}\\left(f_\\theta\\left(\\mathcal{T}1(x)\\right), f\\theta\\left(\\mathcal{T}2(x)\\right)\\right)+\\lambda\\left|\\left(d{\\phi^{\\prime}}\\left(f_\\theta\\left(\\mathcal{T}_2(x)\\right)\\right)-\\mathcal{T}_2(x)\\right) \\odot(1-M)\\right|^2（8） 假设二：MIM 中的相似性度量是可替换的【背景】比较公式 7 与公式 2 ，我们可以发现，与对比学习相比，MIM 有点不同： 1）数据转换 \\mathcal T(\\cdot) ：传统的对比学习方法通常采用 random crop，而 MIM 方法采用 patch masking； 2）相似性度量 \\mathcal M(\\cdot，\\cdot) ：对比学习通常使用 InfoNCE loss，而 MIM 使用一个相对复杂的公式 \\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(\\cdot, \\cdot\\right) 。 那么 MIM 中的复杂相似性度量方法 \\mathcal M(\\cdot, \\cdot) 是否真的很重要呢？ 【方法】为了解答这一问题，作者使用 InfoNCE loss 取代原有的 \\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(\\cdot, \\cdot\\right) 方法，构建了一个新的孪生 MIM 模型命名为 contrastive MAE（C-MAE）。 使用新的相似性度量方法： \\widetilde{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(z_1, z_2\\right) \\triangleq L_{\\mathrm{NCE}}&#x3D;-\\log \\frac{\\exp \\left(s\\left(z_1, z_2\\right) &#x2F; \\tau\\right)}{\\sum_j \\exp \\left(s\\left(z_1, z_j^{\\prime}\\right) &#x2F; \\tau\\right)}（9） s\\left(z, z^{\\prime}\\right)&#x3D;\\frac{q_{\\phi^{\\prime}}\\left(p_\\phi(z)\\right) \\cdot p_\\phi\\left(z^{\\prime}\\right)}{\\left|q_{\\phi^{\\prime}}\\left(p_\\phi(z)\\right)\\right| \\cdot\\left|p_\\phi\\left(z^{\\prime}\\right)\\right|}（10） 其中， p_\\phi(\\cdot) 和 q_{\\phi’}(\\cdot) 是来自 BYOL（ Bootstrap your own latent - A new approach to selfsupervised learning） 的 project head 和 predict head，均使用 MLPs 实现。 τ 是 softmax 的温度（参考 An empirical study of training self-supervised vision transformers）。 C-MAE 的损失函数： L\\left(x, M ; \\theta, \\phi, \\phi^{\\prime}\\right)&#x3D;\\widetilde{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(f_\\theta\\left(\\mathcal{T}_1(x)\\right), f_\\theta\\left(\\mathcal{T}_2(x)\\right)\\right) C-MAE 的其他配置信息可以查看原文。 【结果与讨论】表 1 展示了 C-MAE 和其他一些自监督方法的微调结果。可以看出： C-MAE 与 MAE 的微调结果相差不多，说明 MIM 中常用的相似性度量方法 \\overline{\\mathcal{M}_{\\phi, \\phi^{\\prime}}}\\left(\\cdot, \\cdot\\right) （编码器&#x2F;损失函数设计）在 MIM 中并不重要； C-MAE 与 DINO 等方法的微调结果相差不多，即使前者主要采用随机 patch 掩蔽而后者涉及复杂的数据增强方法，说明数据增强方法在 MIM 中并不重要。 表1：ImageNet 微调中 C-MAE 与 ImageNet 微调预训练方法的比较。所有的模型都是基于 ViT-B 的。 假设三： MIM 可以学习一种 data-agnostic 的初始化【背景】如上所述，学习遮挡不变特征是 MIM 方法成功的关键。那么 MIM 如何对不变性进行建模呢？有两种假设： 假设 3.1：遮挡不变性以 data-agnostic（在这里应该指模型性能与数据规模、类型无关） 的方式表示，只有在最重要的输入部分没有被掩盖的情况下，输出特征才是鲁棒的。（√） 假设 3.2：不变性需要从大量数据中获得知识。 【方法】为了验证假设，作者减少了 MAE 预训练的图像数量，只使用从 ImageNet 训练集中随机采样的 1000 张图像中的 1 张，因此在预训练阶段，来自训练数据的语义信息应该非常有限。 【结果与讨论】从表 2 的实验结果可以看出： 使用 MAE 预训练迭代次数为 5 次时，其微调结果比从头开始训练 100 次好得多，与从头开始训练 300 次不相上下。 当预训练图像的数量增加到 1000 时，微调结果没有改善。 由于不太可能只有一张图像包含整个数据集的大部分语义信息，因此实验提供了强有力的证据，证明 MIM 可以学习到一种有利的初始化，更重要的是，这是 data-agnostic 的。 表2. 用不同图像数量预训练的 MAE 微调结果比较。表 3 表明，图像采样策略的选择并不影响微调的精度，进一步表明 MIM 预训练带来的优势中可能不包括类别信息。 表3. 不同图像采样策略的比较。对于 MAE 预训练，分别采用不同的策略从 ImageNet 中采样 1000 张图像。 （本篇博文内容并没有包含论文所有的验证实验，详情请查看原文）","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | 理论：对比学习和掩蔽图像建模究竟有何不同？","slug":"Masked_Image_Model/MIM-理论对比学习和掩蔽图像建模究竟-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-理论对比学习和掩蔽图像建模究竟-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-%E7%90%86%E8%AE%BA%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%8E%A9%E8%94%BD%E5%9B%BE%E5%83%8F%E5%BB%BA%E6%A8%A1%E7%A9%B6%E7%AB%9F-Glenn/","excerpt":"","text":"题目： WHAT DO SELF-SUPERVISED VISION TRANSFORMERS LEARN?单位：基因泰克、NAVER AI Lab论文网址：https://arxiv.org/abs/2305.00729论文代码：未公开首次发布时间：2023 年 5 月 1 日（收录于 ICLR 2023） 123456@article&#123;park2023self, title=&#123;What Do Self-Supervised Vision Transformers Learn?&#125;, author=&#123;Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo&#125;, journal=&#123;arXiv preprint arXiv:2305.00729&#125;, year=&#123;2023&#125;&#125; 主要内容对比学习（CL）和掩蔽图像建模（MIM）是目前主流的自监督架构。由图 1 可得，CL 在线性探测和小模型方面优于 MIM，而 MIM 在微调精度、大模型和密集预测方面表现出色。 图1. CL 和 MIM 的对比实验作者对 CL 和 MIM 在表示和下游任务的性能方面的差异进行了比较研究。结果表明，基于 ViT 的这两种自监督架构具有以下特性： 1）CL 相较于 MIM 能够更好地捕捉全局依赖，例如物体的形状。这主要体现在 ViT 的后期层。对比学习通过将不同的图像表示引入到相同的空间中，并让模型学习区分它们，从而帮助 ViT 在图像表示空间中实现线性分离。 然而，CL 的这种特性也有一些缺点。它导致所有 query tokens 和 heads 的自注意力变得同质化（homogenized），这意味着所有 tokens 和 heads 的注意力权重变得相似。这种自注意力的同质性降低了表示的多样性，使得模型在应对不同任务和场景时的扩展性和密集预测性能受到影响。 2）CL 利用低频信号，这些信号通常代表了图像中的全局信息和形状。因此，CL 倾向于关注形状特征。相反，MIM 则关注高频信号，这些信号通常代表了图像中的局部信息和纹理。因此，MIM 更倾向于关注纹理特征。 3）CL 在后期层中起着至关重要的作用，而 MIM 主要关注早期层。 这也从理论角度验证了最近一些将 MIM 和 CL 结合工作的有效性。 方法与结果讨论从自注意力角度来解释图 1 的现象CL 主要捕捉全局依赖： 通过注意力距离（attention distance）来衡量自注意力（self-attention）的范围。注意力距离被定义为 query tokens 和 key tokens 之间的平均距离，同时考虑它们的自注意力权重。因此，注意力距离在概念上类似于 CNNs 中感受野的大小。 图3显示，CL（MoCo）的注意力距离显著高于 MIM（SimMIM），尤其是在后面的层中。结合图 2 的定性可视化分析，这意味着 CL 的表示包含全局信息和形状特征，因此它可以帮助 ViT 模型在图像中区分不同的物体。 相反，MIM的自注意力主要捕捉局部关系。这表明 MIM 在识别整个物体及其形状方面可能存在困难。 图2. CL 和 MIM 注意力距离的定性可视化分析图3. CL 和 MIM 注意力距离的定量可视化分析CL 的自注意力会变得同质化： 图 2 还展示了来自两个不同空间位置的 query tokens 的注意力图。 与 MIM 相比，CL 的自注意力对于这两个 query tokens 显示出几乎相同的物体形状。作者将这一现象称为注意力坍缩成同质性（attention collapse into homogeneity）。这种坍缩趋势在 CL 的所有 heads 和 query tokens 中都可以观察到。 相比之下，MIM 的自注意力能更好地捕捉到这两个 query tokens 之间的差异，而 CL 的自注意力则更倾向于将它们视为相同的物体形状。 作者还使用了归一化互信息（normalized mutual information，NMI）来测量注意力坍缩。 假设 p(q) 是 query tokens 的分布，且这些 query tokens 在空间坐标上是均匀分布的，即 p(q) &#x3D; 1&#x2F;N ，其中 N 是 tokens 的数量。那么 query tokens 和 key tokens 的联合分布为 p(q, k) &#x3D; π(k|q)p(q) ，其中 π(k|q) 是 softmax 归一化的自注意力矩阵。因此，归一化互信息可以表示为 I(q, k) &#x2F; √(H(q)H(k)) ，其中 I(·, ·) 表示互信息， H(·) 表示边缘熵。 归一化互信息的值较低意味着注意力图对 query tokens 的依赖较弱，这暗示了注意力坍缩成同质性。相反，归一化互信息的值较高表示注意力图对 query tokens 的依赖很强。 如图 4 所示，在后期层中，CL 的互信息显著低于 MIM ，这意味着 CL 的自注意力更容易坍缩成同质化分布。 图4. 关于 NMI 的注意力坍缩程度注意力坍缩会降低表征多样性： 作者猜想，自注意力坍缩成同质性最终会导致同质化的 token 表示。 为了验证猜想，作者使用三种相似性： 不同自注意力 heads 之间（heads）。衡量不同 heads 之间的表示相似性，用于评估不同 heads 是否捕获了多样性的特征。 自注意力前期层和后期层之间（depths）。衡量自注意力前后层表示的相似性，用于评估表示在模型层中的变化程度。 不同 tokens 之间（tokens）。衡量不同 tokens 之间表示的相似性，用于评估模型对不同输入 tokens 的响应程度。 通过计算这三种余弦相似性，可以评估表示在不同自注意力 heads、depths 和 tokens 之间的多样性。如果观察到表示的余弦相似性较高，那么这可能意味着表示具有同质性；反之，则表明表示具有较高的多样性。 如图 5 所示，CL 的相似性在后期层明显高于 MIM，表明 CL 的表示具有显著的同质性。即使增加模型的大小也不能解决 CL 所面临的问题，反而可能使其恶化： 增加 heads 的数量（ViT-S 到 ViT-B；图 5a）提高了 MIM 的表示多样性，但对 CL 的多样性改善甚微。 增加 CL 的深度（ViT-B 到 ViT-L；图 5b）降低了表示的多样性。 这些结果进一步支持了前面提到的猜想，即自注意力坍缩成同质性最终导致同质化的 token 表示。对比学习 CL 的表示在后期层次中的同质性可能限制了模型的泛化能力和性能。 同时，这些发现表明，通过调整模型参数（如 heads 的数量和层数）来改善表示多样性可能对 MIM 更有效，而对 CL 的影响有限。 图5. 不同相似度的对比研究此时，可以解释图 1 的现象： 在线性探测任务中，CL 优于 MIM，因为它能捕捉形状信息，有助于识别物体并区分图像。尽管 MIM 保留了纹理和表示的多样性，但它们与物体或内容的关联可能不如形状那么强。 注意力坍缩阻止了 CL 充分利用 ViTs 的 heads、depths 和 tokens。由于同质化的表示对于改善 token 表示没有太大帮助，使用 CL 训练的 ViTs 浪费了大部分网络能力。因此，在大型模型中，MIM 的微调准确率明显高于 CL。 对于密集预测任务，CL 不太适用，因为 token 特征在空间坐标上呈同质化分布。 表征如何被转换？CL 将所有 tokens 统一地进行转换，而 MIM 在处理 tokens 时更具个性化： 为了展示 CL 和 MIM 如何转换 token 表示，作者将它们可视化在表示空间中。 图 6 展示了 ImageNet 验证集中单个图像样本在自注意力模块前后（第一层和最后一层）的196个（14×14 patches）tokens。 图 6a 表明 CL 的自注意力将所有 tokens 统一地进行平移。这种现象的发生是因为其同质性。这意味着 CL 关心的是整体而非个别 tokens。然而，如图 6b 所示，CL 通过将表示分布的“中心”相互分离来帮助区分图像。简而言之，尽管 CL 失去了区分 tokens 的能力，但它使得图像在线性表示空间中可分离。 与此相反，如图 6c 所示，MIM 在处理 tokens 时更具个性化，MIM 改变了单个图像中 tokens 之间的距离以及表示分布的体积。 图6. CL 和 MIM 表征转换方式可视化CL 利用低频信号，MIM 则关注高频信号： 可以假设 CL 捕获空间维度上的低频信息，而 MIM 捕获高频信息。 为了从频率的角度支持这个观点，作者对表示进行了傅立叶分析。 图 7 显示了 CL 和 MIM 的相对振幅（表示的最高频率和最低频率之间的振幅差）。 结果表明，CL 的高频振幅明显小于 MIM，说明 CL 主要利用了全局结构和形状等低频空间信息。 相反，MIM 通常使用高频的空间信息，如局部结构和精细的纹理。 图7. CL 和 MIM 的相对振幅CL 是形状偏置的，而 MIM 是纹理偏置的： 基于傅里叶分析的结果，可以假设 CL 和 MIM 各自对形状和纹理都具有偏置。 为了证明这一说法，作者使用 Stylized ImageNet，这是一个使用 AdaIN 进行纹理改变的数据集。 图 8a 报告了在 Stylized ImageNet 上的线性探测结果，以评估预训练模型对形状和纹理的偏好。 与使用监督学习预训练的模型相比，CL 更依赖于图像的形状，而 MIM 更依赖于图像的纹理来进行图像分类。换句话说，CL 对纹理变化具有鲁棒性，而 MIM 对它们较为敏感。 这种差异表明，根据任务需求，可以选择适当的预训练方法以获得对形状或纹理更敏感的模型。例如，在需要关注物体形状的任务中，CL 可能表现更好；而在需要关注纹理和细节的任务中，MIM 可能表现更优。 另外，在 ImageNet 数据集上测量基于频率的随机噪声对准确率影响的实验也显示了同样的结果。 如图 8b 所示，CL 对高频噪声具有鲁棒性，而 MIM 明显更易受影响。这也可以解释 CL 在对抗扰动下的鲁棒性。 图7. 探究 CL 和 MIM 的形状和纹理偏置 哪些组件在 CL 和 MIM 种起到了重要作用？CL 的后期层和 MIM 的早期层是重要的： 在图 8 中，可以发现： MIM 在模型开始阶段的线性探测准确率高于 CL。而在模型结束时，CL 的表现优于 MIM。这个结果表明，CL 的后期层和 MIM 的早期层在产生线性可分离表示方面起着重要作用。 CL 的准确率随着深度的增加而增加，但 MIM 在模型结束时的准确率却在降低，即 MIM 的后期层在分离表示方面并不是很有帮助。作者将这个现象解释为：具有浅预测 head 的 MIM 方法，如 SimMIM，将主干网络的后期层用作解码器。而具有深度自注意解码器的 MIM，例如 MAE，可以在线性探测性能方面发挥作用。此外，这也解释了为什么 SimMIM 在后期层中的高频分量会下降，如图 7 所示。 即使是 MIM 的最高线性探测准确率也低于 CL。 图8. CL 和 MIM 的线性探测随深度而改变显式解码器有助于 ViTs 进一步利用 MIM 的优势： 就如上面分析的那样，具有浅预测 head 的 MIM 的隐式解码器（如 SimMIM）可能会影响性能。 MAE 通过引入深度显式 ViT 解码器并仅在单独的解码器中重构掩蔽 tokens 来解决这个问题。 图 9a 的结果表明，MAE 在编码器的后几层中的互信息比 SimMIM 低，但在解码器中的互信息较高，这意味着解码器根据相邻的 tokens 重建被掩盖的 tokens。 这种现象表明，MAE 在编码器部分与 SimMIM 具有类似的自注意力特性，但在解码器部分的自注意力表现有所不同。这种高互信息的存在表明，解码器在重建过程中更加关注局部信息，从而有助于在预训练阶段学习丰富的特征表示。 图 9b 展示了 SimMIM 的后四层降低了高频分量。而 MAE 的后几层（除了最后一层）并未减少高频分量。与编码器相比，MAE 的解码器优先考虑低频信息，从而使主干网络能够更有效地利用高频信息。 这种现象表明，通过引入显式解码器，MAE 能够在不损失高频信息的情况下关注低频信息。这使得 MAE 在捕捉图像中的局部纹理和细节方面表现更出色，进而在预训练和下游任务中取得更好的性能。 图9. 每个图的左侧代表编码器，右侧代表解码器。 结论 上述结果表明，将 CL 和 MIM 结合起来训练主干网络可能有助于发挥两种方法的优势。 为了证明 CL 和 MIM 是互补的，作者采用了最简单的方法来调和 CL 和 MIM，即线性组合两种损失，如 \\mathcal L &#x3D; (1 − λ)\\mathcal L_{MIM} + λ\\mathcal L_{CL} ，其中 \\mathcal L_{MIM} 和 \\mathcal L_{CL} 分别表示 MIM 和 CL 的损失， λ 是 CL 的重要性权重。作者发现，这种简单的混合模型通过组合损失训练可以有效地利用两种方法的优势。 图 10a 展示了在 ImageNet 上随着 λ 变化的线性探测和微调准确率。可以看出： 混合模型在这两个方面都优于 MIM（ λ &#x3D; 0 ）和 CL（ λ &#x3D; 1 ）。 图 10b 和图 10c 分别分析了 λ &#x3D; 0.2 的模型在互信息和频域的行为，可以看出： 两个结果都显示混合模型在前几层利用 MIM 特性，在后几层利用 CL 特性。特别是，图 12b 表明，早期层的自注意力根据 query tokens 而改变，但后期层的自注意力则没有改变。同样，图 12c 显示，早期层利用高频信息，而后期层试图利用低频信息。 图10. CL 和 MIM 是互补的 更多实验请查看原文。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | PixMIM：重新思考掩蔽图像建模中的像素重建","slug":"Masked_Image_Model/MIM-PixMIM重新思考掩蔽图像建-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-PixMIM重新思考掩蔽图像建-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-PixMIM%E9%87%8D%E6%96%B0%E6%80%9D%E8%80%83%E6%8E%A9%E8%94%BD%E5%9B%BE%E5%83%8F%E5%BB%BA-Glenn/","excerpt":"","text":"题目：PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling作者单位：上海人工智能实验室、西蒙菲莎大学、香港中文大学论文网址：https://arxiv.org/abs/2303.02416论文代码：https://github.com/open-mmlab/mmselfsup首次发布时间：2023 年 3 月 4 日 前言前面介绍过微软亚研院的新 MIM 架构 DeepMIM，与 CVPR23 的 LocalMIM 一样都是主张将 Deep Supervision 引入 MIM 中，后者提到，从底层到高层特征图的尺度由粗到细对各种视觉任务（检测，分割和分类）都有增益。然而今天介绍的 PixMIM 认为，在 MIM 任务中模型应该更关注浅层特征，即形状偏置，而不用浪费建模能力来关注短期依赖和高频细节。 主要内容早期的 MIM 方法主要从三个方面来改进建模： 1）重建目标，将 Pixel 替换为 Frequency、HOG 等来提升表征质量； 2）额外任务，例如，CMAE 除了重建任务以外，引入了一个对比学习的任务； 3）知识蒸馏，利用强大的预训练模型来蒸馏学习，例如，MASKDISTILL 使用教师网络输出的表征作为 target。 但这些方法要么会使整个框架复杂化，要么不可避免地引入不可忽视的训练成本（证明？）。 本篇论文从输入 patch（保留更多的前景） 和重建目标（使模型更关注低频信号）两个方面入手，提出了一个简单而有效的方法 PixMIM。 分析本篇论文基于 MAE 实验分析得出两个重要的观察结论： 1）当 MAE 使用的 Random Resized Crop 图像增强方式与 75% 的掩蔽策略结合时，输入的可见 patch 中就只剩平均 17.1% 的前景。这种较低的前景覆盖范围阻碍了模型有效捕获形状和语义先验的能力，从而限制了表征的质量，如图1所示。 图1. MAE 的输入 patch 的可视化。对于每个例子，从左到右展示了原始图像、Random Resized Crop 之后的图像，以及由 MAE 的掩蔽策略产生的可见 patch。Random Resized Crop 与 75% 掩蔽策略的耦合导致输入中前景对象的覆盖率低，损害了表征质量。分析方法如下： 使用 PSSL（Distilling ensemble of explanations for weakly-supervised pre-training of image segmentation models 提出的方法）对 ImageNet-1K 取二值掩模，并提出了一个前景覆盖率百分比度量 \\mathcal J(\\mathcal F) 来评估图像处理操作 \\mathcal F(\\cdot) ，用表示。如图 2 所示， \\mathcal J(\\mathcal F) 定义为 A_2 和 A_1区域之间的比值。A_1 和 A_2 分别为原始图像 I 和处理后的图像 \\mathcal F(I) 中的前景对象的面积。 图2. 目标覆盖率的计算方法。在上例中，A1 是前景区域。A2 是黄色区域的面积。蓝色的矩形是由数据增强产生的裁剪图像。目标覆盖率由 A2 和 A1 之间的比率得到。然后，利用该度量来研究 MAE 对增强函数 \\mathcal A(\\cdot) 和腐败函数 \\mathcal M(·) 的选择是如何影响对象的覆盖范围的。如表 1 所示，MAE 对 \\mathcal A(\\cdot) 使用了常用的 Random Resized Crop，对 \\mathcal M(·) 使用了 75% 的掩蔽操作。最终发现 \\mathcal J(\\mathcal A) &#x3D; 68.3 %，而 \\mathcal J(\\mathcal M◦\\mathcal A)&#x3D; 17.1 %，这表明在 MAE 的输入中可能缺乏前景信息。 表1. 各种 MIM 方法。aug：数据增强。mask：掩码比率。MSA：多头自注意力。RRC：Random Resized Crop。前景对象通常比背景编码更多的语义概念。DeiT III（Deit III: Revenge of the vit）认为，缺少前景可能会导致监督学习中的 sub-optimal optimization。在 MIM 的背景下，Random Resized Crop 和掩蔽操作的耦合可能会阻碍表征学习。 2）自 MAE 出现以来，大多数 MIM 方法都采用原始像素作为重建目标。训练目标需要对掩蔽 patch 进行完美的重建，包括复杂的细节，例如纹理，浪费建模能力来关注短期依赖和高频细节。根据最近对形状和纹理偏置的研究（Partial success in closing the gap between human and machine vision，Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness），更依赖于形状偏置的模型往往在迁移学习中表现更好，对领域变化更鲁棒。然而，重建细粒度的细节不可避免地会引入对纹理的偏置，降低表征质量。 方法根据上面提到的两个方面：输入 patch 和 重建目标。PixMIM 使用了更为保守的图像增强方法，并构建了一个低频目标生成器。 图3. PixMIM 的架构。首先使用低通滤波器从重构目标中去除高频分量，从而降低了纹理主导细节的重构，并降低学习低频模式的优先级。然后，使用 Simple Resized Crop 替换常用的 Random Resized Crop，以缓解输入 patch 中的前景缺失的问题。 更为保守的图像增强方法基于上述分析，我们希望在模型的输入 patch 中保留更多的前景信息。由于高掩蔽比对于 MIM 学习有效表示至关重要，最直接的策略是保持腐败 \\mathcal M(\\cdot) 不变，但使增强函数 \\mathcal A(\\cdot) 更加保守。 Simple Resized Crop 是 AlexNet 中使用的图像增强方法，先将短边 resize 到指定分辨率，然后再中心裁剪到指定分辨率（例如，358×558 → 224×558 → 224×224）。 Center Crop 总是从图像的中心截取固定大小的裁剪。 由图 4 可以看出 Simple Resized Crop + 75% 掩蔽策略可以保留最大的前景覆盖率。 图4. 前景覆盖率分析。即使在 MAE 的 75% 掩蔽策略下，SRC 保留比例也高于 RRC 和 CC。请注意，当应用 MAE 的掩蔽策略时，前景覆盖率的上限为25%。(RRC：Random Resized Crop，SRC：Simple Resized Crop，CC：Center Crop）需要注意的是，当没有图像掩蔽时，Simple Resized Crop 将 \\mathcal J (\\mathcal F) 从 Random Resized Crop 的 68.3% 提高到 88.2%，表明它提供的多样性不如 Random Resized Crop，这解释了 DeiT III 观察到的监督图像分类的性能退化。但与监督学习不同的是，MIM 中的 75% 图像掩蔽已经提供了足够的随机性，并且 Simple Resized Crop 的使用不会像在监督学习中那样损害多样性（证明？）。 低频目标生成器1）从空间到频率的域转换，使用二维离散傅里叶变换（DFT） \\mathcal F_{DFT} 。 2）低频分量提取，使用低通滤波器 \\mathcal F_{LPF} ： \\mathcal F_{LPF}(u,v)&#x3D;\\begin{cases} 1, \\sqrt{((u-u_c)^2+(v-v_c)^2)}\\leq r,\\ 0, otherwise. \\end{cases} 其中 v_c 和 u_c 为频谱的中心坐标。 r 是圆形理想低通滤波器的带宽，用于控制有多少高频分量将从频谱中被滤掉， r ∈[0，min(\\frac{H}{2},\\frac{W}{2})] 。 提取过程用 \\mathcal F_{LPF}(u,v)⊗\\mathcal F_{DFT}(I_i)(u,v) 表示。 3）重建目标的生成，使用离散傅里叶反变换（IDFT） \\mathcal F_{IDFT} 重建 RGB 图像： Y&#x3D;\\mathcal F_{IDFT}(\\mathcal F_{LPF}(u,v)⊗\\mathcal F_{DFT}(I_i)(u,v)) 结果图5. MAE 和 PixMIM 的频率分析。计算了不同频率间隔下的重建图像的信噪比（PSNR），并从 ImageNet-1K 中获取 50,000 张图像的平均值。PixMIM 将模型的关注点转向低频分量，以减少纹理偏置。表2. MIM 方法对各种下游任务的性能比较。报告了对 ImageNet-1K 的微调（ft）和线性探测（lin）实验、COCO 和 ADE20K 的结果。所有实验的主干网络都是 ViT-B。∗：结果来自于运行官网代码。†：由于 MAE、LSMAE 和 ConvMAE 用于目标检测时没有统一的微调 Epoch，因此使用相同数量的 Epoch 对 PixMIM图6. 性能与 PixMIM 的 epoch 图。在不同的训练 epoch，PixMIM 在不同的评估方法中始终为基线 MIM 方法带来显著的收益。左： ImageNet 微调。中间：ImageNet 线性探测。右图：ADE20K 语义分割。具体结果查看原文。 使模型更关注低频信号的观点是否与 CVPR23 的 LocalMIM（Masked Image Modeling with Local Multi-Scale Reconstruction）和 DeepMIM（DeepMIM: Deep Supervision for Masked Image Modeling）观点矛盾呢？ 且保留更多的前景是否会使得自监督训练任务可能过于简单，模型无法学习有效的表征呢？","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | MixedAE：Patch Mix 的调教手册","slug":"Masked_Image_Model/MIM-MixedAEPatch-Mi-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-MixedAEPatch-Mi-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-MixedAEPatch-Mi-Glenn/","excerpt":"","text":"题目：Mixed Autoencoder for Self-supervised Visual Representation Learning单位：香港科技大学、华为诺亚方舟实验室论文网址：https://arxiv.org/abs/2303.17152论文代码：未公开首次发布时间：2023 年 3 月 30 日（收录于 CVPR 2023） 123456@article&#123;chen2023mixed, title=&#123;Mixed Autoencoder for Self-supervised Visual Representation Learning&#125;, author=&#123;Chen, Kai and Liu, Zhili and Hong, Lanqing and Xu, Hang and Li, Zhenguo and Yeung, Dit-Yan&#125;, journal=&#123;arXiv preprint arXiv:2303.17152&#125;, year=&#123;2023&#125;&#125; 前言前面介绍的论文（Understanding Masked Image Modeling via Learning Occlusion Invariant Feature）提到， MIM 中的 patch masking&#x2F;zero（可以理解为用 0 来填充 patch） 可以看作是一种数据增强方式。从这个角度来看，可以再衍生出一种 MIM 的改进方向：Input augmentation。 MAE 原文中做过这样的实验，输入图像加入 color jittering 后，迁移的效果反而降低，这说明相比于对比学习， MIM 对数据增强的方法可能有不同的偏好。 MixMIM 提出使用另一幅图像的可见 patch 替换该图像的 patch masking&#x2F;zero，即创建一个混合图像，然后再进行双重重建。其间，也尝试使用了其他增强方式，比如 patch shuffle、patch zoomin 等等，最终的实验表明，patch mix 是个简单有效的方法。 （PS：我认为 Input augmentation 与 Masking strategy 的区别在于前者是针对 patch masking&#x2F;zero 这种方法的，而后者是针对如何进行或者对哪个部分使用 patch masking&#x2F;zero 的） 然而 MixedAE 证明了如果直接在 MIM 框架中引入 patch mix，会导致模型性能下降，因此提出了 homologous recognition（同源识别） 的辅助代理任务。图 1 展示了 MixedAE 的有效性。 图1. 在 ImageNet-1K 上的微调精度。ID 表示 instance discrimination。 主要内容 使用 patch mix 替换 MAE 中的 patch masking&#x2F;zero，构建了一个简单的 baseline 结构。 利用该 baseline 分析了直接引入 patch mix 会导致 MIM 模型性能下降的原因：互信息的增加。 为了解决这个问题，作者提出了 homologous recognition（同源识别） 的辅助代理任务，不仅通过明确要求每个 patch 识别同源 patches 来缓解互信息的增加，而且还可以执行 object-aware 的自监督预训练，以获得更好的下游密集感知性能。 分析构建一个 mix baseline使用 patch mix 替换 MAE 中的 patch masking&#x2F;zero，我们可以构建一个简单的 baseline 结构。 给定 Batch size 为 B ，则输入图像可以表示为 \\left{x_i\\right}^B_{i&#x3D;1} 。 将输入图像分为 B×r 个组，每个组会生成一张混合图像，其中 r\\in (0, 0.5] 是混合比。 每组将会不重叠、顺序地从 Batch 中取 1&#x2F;r 张图像，按下面方式进行 patch 混合： $$ \\hat x^j&#x3D;\\sum_{i&#x3D;1}^{1&#x2F;r}\\mathbb I(M^j&#x3D;i)x_i^j $$ 其中， j\\in [1, Br] 表示第 j 组， \\mathbb I(\\cdot) 表示指示器， \\mathbb I(M^1&#x3D;1) 表示第 1 组第 1 张图像的掩蔽策略。 将 \\hat x^j 喂入 encoder 后得到 \\hat z^j 。 然后通过插入 [MASK] token 来实现 unmix： $$ z^j_i&#x3D;\\mathbb I(M^j&#x3D;i)\\hat z^j+[1-\\mathbb I(M^j&#x3D;i)][MASK] $$ 再将 unmix 后的 \\left{z^j_i\\right}^{1&#x2F;r}_{i&#x3D;1} 送入 decoder 中，得到 y^j_i 。 重建损失函数可以写为： $$ \\mathcal L_{recon}&#x3D;\\sum^{1&#x2F;r}_{i&#x3D;1}[1-\\mathbb I(M^j&#x3D;i)(y_i^j-x_i^j)^2 $$ 实验 ImageNet ADE20K Top-1 Acc. mIoU MAE 82.7 46.1 mix baseline 82.4 45.0 由表格可以看出，我们构建的 mix baseline 性能甚至比 MAE 更差。 分析原因以 r&#x3D;0.5 为例， X_1 ， X_2 表示为两个输入图像， M 表示为随机掩模。则混合输入可以表示为： $$ \\sigma_{mix}(\\left{X_1,X_2\\right},M)&#x3D;\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)X_2 $$ 而 MAE 输入可以表示为： $$ \\sigma_{mix}(X_1,M)&#x3D;\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0 $$ 因此，以 X_1 作为重建目标，可以将混合输入与重建目标 X_1 之间的互信息（MI）表示为： $$ \\begin{aligned} I(\\sigma_{mix}(\\left{X_1,X_2\\right},M);X_1) &amp;&#x3D; I(\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)X_2;X_1)\\ &amp; &#x3D; H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)X_2)\\ &amp; &#x3D; H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0+\\mathbb{I}(M&#x3D;1)\\vec 0+\\mathbb{I}(M&#x3D;2)X_2)\\ &amp; &#x3D; H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0,\\mathbb{I}(M&#x3D;1)\\vec 0+\\mathbb{I}(M&#x3D;2)X_2)\\ &amp; \\geqslant H(X_1)-H(X_1|\\mathbb{I}(M&#x3D;1)X_1+\\mathbb{I}(M&#x3D;2)\\vec 0)\\ &amp; &#x3D; I(\\sigma_{MAE}(X_1,M);X_1) \\end{aligned} $$ 公式推理证明混合输入 \\sigma_{mix}(\\left{X_1,X_2\\right},M) 与重建目标 X_1 之间的 MI，不小于 MAE 输入 \\sigma_{MAE}(\\left{X_1\\right},M) 与 X_1 之间的 MI。 当 x_2 为重建目标时，上述结论也成立。 因此，与减少 MI 的 patch mask&#x2F;zero 不同，直接混合反而会增加模型输入和重建目标之间的 MI，从而简化重建任务。 混合带来的 MI 增加是 target-invariant 的，这表明当目标是有监督学习的标签或对比学习的正样本时，上述结论也成立。这可能解释了为什么直接混合有利于有监督学习和对比学习，而不利于 MIM。 方法导致 MI 增加的一个因素是，MAE 在 ViT 中使用了 global self-attention，通过这种方式，每个查询 patches 将不可避免地关注来自其他图像的异源 patches。由于生成式建模的不确定性，异源 patches 可能提供了完成重建的捷径。如图 2 所示，黄瓜的绿色是重建图中狐狸身后森林的“有价值”线索。 图2. 生成式建模的不确定性为了解决这个问题，作者提出了一种新的代理任务，称为同源识别，以强制每个查询明确识别并只关注同源 patch。 图3. Mixed Autoencoder（MixedAE）的网络结构1）同源 attention：通过使用 TopK(\\cdot) 的采样操作，强制每个查询 patches 只关注注意分数最高的关键 patches，从而动态识别同源 patches。具体来说，同源 attention 可以表述为： $$ A_{HomoAtt}&#x3D;softmax(TopK(qk^T&#x2F;\\sqrt D_h)) $$ 默认情况下，除第一层外，ViT 中的所有 self-attention 操作都被同源 attention 替换。 2）同源对比：旨在通过鼓励编码器提取同源 patches 的特征相似，而异源 patches 特征不同。对比损失为： $$ \\mathcal{L}_{HomoCon}&#x3D;-\\sum^L_{l&#x3D;1}\\sum_{l+}log\\frac{exp(cos(\\hat z^j_l,\\hat z^j_{l+})&#x2F;\\tau)}{\\sum_{l’\\neq l}^{L}exp(cos(\\hat z^j_l,\\hat z^j_{l’})&#x2F;\\tau)} $$ 其中， \\tau 为温度， cos(\\cdot, \\cdot) 为余弦相似度。 3）Segment embedding：类似 MixMIM，为了降低预训练难度，除了位置嵌入之外，MixedAE 在混合输入\\hat x^j 中还添加了 Segment 嵌入。 Segment 嵌入对于来自同一图像的 patches 是共享的，而对于来自不同图像的 patches 则是不同的，如图 4 所示。 图4. 不同的 Segment 嵌入代表不同的图像。4）混合模式：为了在不同的训练开销下进行公平的比较，MixedAE 采用了两种混合模式，如图 5 所示。 Compose：每一组生成一个单一的混合样本。 Full：每组独立地进行 1&#x2F;r 次采样来生成 1&#x2F;r 个混合样本。 如果未另行指定，则默认采用 Compose。 图5. 当混合比 r&#x3D;0.5 时，两种混合模式的可视化。最终的损失函数为： $$ \\mathcal L_{MixedAE}&#x3D;\\mathcal L_{recon}+\\lambda \\mathcal L_{HomoCon} $$ 其中平衡权重 λ 默认设置为 0.1。 结果图 6 可以看出，MAE 关注更具鉴别性的 patches ，如边界，而 MixedAE 更关注前景 patches。因此，作者在前面强调 MixedAE 具有 object-aware。 图6. 从 ImageNet-1K（第 1-3 列）、COCO（第 4-6 列）和 ADE20K（第 7-9 列）的图像的注意力热图可视化。 MAE 和 MixedAE 都在 ImageNet-1K 上进行了 300 个 epoch 的预训练。由表 1 可以得出： 有效性： MixedAE 在不同的预训练时间和额外开销下取得了最先进的性能。 效率： MixedAE 始终超过强 iBOT 基线，而且只需要 53.4% 的预训练开销。 object-aware：当迁移到下游的密集感知任务时，取得了更显著的改进。 表 1. 在 ImageNet-1K 上预训练方法之间的迁移性能比较。∗：部署了一个轻量级解码器来与 BEiT 保持类似的预训练开销。†：在 Tesla V100 GPUs 上估计的 GPU-days。††：iBOT 的有效 epoch。由表 2 可以得出： 1600 个 epoch 预训练的 MixedAE 在所有 11 个分类数据集上均实现了比 MAE 更好的结果，平均准确率为 86.9%，超过了所有同类方法。 表2. 11 个下游分类任务的迁移性能比较。更多实验结果请查看原文。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | Img2Vec：基于 token 多样性原则选择教师模型","slug":"Masked_Image_Model/MIM-Img2Vec基于-token-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-Img2Vec基于-token-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-Img2Vec%E5%9F%BA%E4%BA%8E-token-Glenn/","excerpt":"","text":"题目：Img2Vec: A Teacher of High Token-Diversity Helps Masked AutoEncoders单位：腾讯、浙江大学、北京大学论文网址：https://arxiv.org/abs/2304.12535论文代码： 未公开首次发布时间：2023 年 4 月 25 日 123456@article&#123;pan2023img2vec, title=&#123;Img2Vec: A Teacher of High Token-Diversity Helps Masked AutoEncoders&#125;, author=&#123;Heng Pan and Chenyang Liu and Wenxiao Wang and Li Yuan and Hongfa Wang and Zhifeng Li and Wei Liu&#125;, journal=&#123;arXiv preprint arXiv:2304.12535&#125;, year=&#123;2023&#125;,&#125; 前言前面对 MIM 的小总结中提到，传统的像素级别重建任务可能会限制模型的表达能力。为了提高模型的性能，可以将重建目标替换为更高级的语义表示，如 MaskFeat 使用 HOG 特征，BEiT-v2 使用 CLIP 特征等。这将迫使模型学习更丰富的特征表示，从而有助于下游任务的性能。 究竟哪些高级特征对预训练最有效？ 我们知道这些高级语义特征可以由另一个预训练模型或预训练模型（教师模型）的指数移动平均（EMA）生成。 那么这个问题就可以转化为：在 MIM 中如何选择教师模型对预训练最有效？ 对此，Img2Vec 提出了基于 token 多样性的原则选取教师模型。 主要内容作者进行了初步分析，探讨了不同高级语义特征作为重建目标的影响以及对下游任务的影响，得出结论：在MIM中，具有出色表现的教师模型未必能带来更好的学生模型。 为解决这一问题，作者提出了一个基于多样性原则的方法来选择 MIM 中的教师模型。基于这一原则，具有高 Token 多样性的较小模型可以表现更优。 此外，作者还提出了一种新的自监督预训练框架，即 Img2Vec，使用小型 ResNet-50 模型作为教师模型，同时引入多特征学习和全局语义学习，以增强 Img2Vec。 方法教师模型选择的标准： token 多样性作者采用 ViT-B 作为学生模型，并对其进行 1600 个 epoch 的预训练。采用在不同自监督模型（MAE、SimCLR、DINO，为了公平对比，不使用 CLIP）预训练下的不同教师模型的输出特征作为重建目标。实验结果如表 1 所示，可视化如图 1 所示。 表1. 不同教师模型比较图1. 不同教师模型比较可视化 最后一列显示了学生模型的 top-1 的微调精度。与 MAE-ViT-B 获得的 84.3% 的准确率相比，使用参数量更大的 MAE-ViT-L 作为教师模型可以将性能略微提高 0.1%。这可能归功于 ViT-L 更强的特征提取能力。 相比之下，在相同的教师模型下，与 MAE ViT-B（84.6% 对 84.3%）相比，DINO ViT-B 可以带来显著的改善。这一显著结果表明，教师模型的选择比模型大小和性能影响力更大。 因此，问题来了：影响教师模型选择最重要的因素是什么？ 作者通过计算目标 token 的余弦相似度来可视化它们的关系，如图 2 所示。第一列中带有红色框的区域是 query patch。 图2. 来自不同教师模型的目标相似性的可视化。颜色越亮，值就越大，反之亦然。在图 2 中，可以发现 MAE ViT-B 中的 tokens 之间更为相似，而 DINO ViT-B 中的相似的 tokens 要少得多。 为了对这一现象进行定量分析，作者提出了一个名为“token 多样性”的指标。定义如下： 对于具有 K 个 tokens \\left{y_i\\right}^K_{i&#x3D;1} 的教师特征样本，计算所有 tokens 之间的余弦相似性，并使用最大最小归一化将值映射到 [0，1] 中。然后再将这些相似性平均为样本值。 N 个样本的多样性为：diver&#x3D;1-\\frac{1}{N}\\sum^N_{n&#x3D;1}sim_n其中， sim_n&#x3D;\\frac{1}{K(K-1)}\\sum^K_{i\\neq j}norm\\left{cosine(y_i,y_j)\\right} 是第 n 个样本的相似性。更大的 diver 说明了 tokens 越多样化。 表 1 中还报告了 MAE ViT-B、MAE ViT-L 和 DINO ViT-B 的 token 多样性。这一比较验证了假设，即具有更高 token 多样性的 DINO ViT-B 训练出的学生模型性能更好。 在 MIM 预训练中，掩蔽 ptaches 是通过可见 patches 重建的。如果掩蔽目标和可见目标高度相似，这就意味着，模型不需要学习到区分掩蔽目标和可见目标之间的特征，而只需要记住像素之间的具体匹配关系。这种情况下，模型的泛化能力会受到限制，因为它没有学习到更高级别的抽象特征。这会降低模型的表示学习能力，并导致模型在下游任务中的性能下降。 举例来说，如果一个狐狸的耳朵被遮住了，那么一个基于 MAE 的教师模型可能会容忍学生模型将它预测为狐狸的可见身体特征，因为它们都属于狐狸的一部分。然而，一个基于 DINO 的教师模型则要求学生更准确地预测，因为它需要学生区分不同的视觉区域，而不是只依赖像素级别的匹配关系。 表征的多样性反映了模型的不同辨别能力，即对于不同的视觉区域，模型学到的特征不同，这有助于提高模型的表示学习能力和泛化能力，从而在下游任务中表现更好。 考虑到 ViT 在全局注意力机制中会处理所有的 tokens，它们的输出 tokens 往往彼此相似。而 ConvNets 具有局部性的归纳偏差，它提取邻居特征。因此，ConvNets 可能是生成 MIM 目标的良好替代方案。 从表 1 和图 2 可以看出，SimCLR 和 DINO 预训练的 ResNet-50 比 ViT 获取了更高的 token 多样性。 通过以上定性和定量分析，可以假设 token 多样性是衡量 MIM 重建目标的一个有价值和必要的视角。 Img2Vec 架构根据以上讨论，作者提出了 MIM 新架构：Img2Vec。 Img2Vec 采用高标记多样性模型，DINO ResNet-50 作为教师模型。此外，还包含两个新的模块，多块特征学习和全局语义学习。整个框架如图 3 所示。 图3. Img2Vec 架构多块特征学习和全局语义学习是 MIM 的常见手段了。 多块特征学习是指将编码器中所有 block 的输出平均后作为编码器的总输出。 全局语义学习是指将编码器最后一个 block 的输出特征经过 mlp 和平均池化，与教师模型输出的重建目标经过平均池化后做损失计算。 结果如表 2 所示，为了进行公平的比较，所有方法都只在 ImageNet 训练集上进行预训练。 ViT-B、ViT-L、ViT-H 表示各个自监督模型使用这三个模型在 ImageNet-1K 上的 top-1 微调精度； Linear 表示在 ImageNet-1K 上的线性探测结果； AP^{bbox} 和 AP^{mask} 表示各个自监督模型使用 Mask R-CNN 作为检测器在 COCO 上的对象检测和实例分割性能； mIoU 表示各个自监督模型使用 UperNet 在 ADE20k 上的语义分割结果。 表2. 不同模型在不同下游任务上的比较如表 3 所示，多块特征学习模块的在不同教师模型上的消融实验。 表3. 多块特征学习模块的在不同教师模型上的消融实验","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | HPM：引入蒸馏概念自动生成掩蔽策略","slug":"Masked_Image_Model/MIM-HPM引入蒸馏概念自动生成掩蔽-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-HPM引入蒸馏概念自动生成掩蔽-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-HPM%E5%BC%95%E5%85%A5%E8%92%B8%E9%A6%8F%E6%A6%82%E5%BF%B5%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E6%8E%A9%E8%94%BD-Glenn/","excerpt":"","text":"题目：Hard Patches Mining for Masked Image Modeling单位：中国科学院、旷视、中国科学院大学论文网址：https://arxiv.org/abs/2304.05919论文代码： https://github.com/Haochen-Wang409/HPM首次发布时间：2023 年 4 月 12 日（收录于 CVPR 2023） 123456@article&#123;wang2023hard, title=&#123;Hard Patches Mining for Masked Image Modeling&#125;, author=&#123;Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang&#125;, journal=&#123;arXiv preprint arXiv:2304.05919&#125;, year=&#123;2023&#125;&#125; 前言“Learning Where to Mask”目前是 MIM 中改进掩蔽策略的主流方向之一，它分为人工选定和自动生成。 人工选定：事先就决定好掩蔽策略，例如： PixMIM 认为，掩蔽较多的前景 patches 会阻碍模型有效捕获形状和语义先验的能力，从而限制了表征的质量，因此 PixMIM 使用了可以保留最大的前景覆盖率的 Simple Resized Crop + 75% 掩蔽策略。 自动生成：由模型训练来得到合适的掩蔽策略，例如： 而 AutoMAE 认为，掩蔽策略是一个 trade-off 的问题，前景掩蔽率越高，通过掩蔽图像建模获得的信息越多，但同时解码器的重建难度也越高。对此，AutoMAE 将生成对抗网络引入 MIM 来搭建一个完全可微的框架。 而本文介绍的 HPM 从改进掩蔽策略的角度，引入蒸馏概念来自动生成合适的掩蔽策略。 主要内容传统的 MIM 方法通常需要先人工给定掩蔽策略，比如 MAE 的随机掩蔽，BeiT 的逐块掩蔽等，目的是构造具有挑战性的重建任务。 这个过程我们可以认为是在训练学生（即模型）解决给定任务（即预测掩蔽 patches），如图 1(a) 所示。 但如果预先给定了掩蔽策略，就无法控制重建任务的困难程度。 为此，本文提出了 Hard Patches Mining（HPM），一个全新的 MIM 框架，通过让模型学习自己“出题”的能力，使其可以同时站在学生和教师的立场上，被迫对图像内容有更全面的理解，从而通过生成更理想的重建任务来引导自己，如图 1(b) 所示。 具体来说，就是将 MIM 中人工给定掩蔽策略改为自动生成掩蔽策略，而生成的原则是让重建任务更具备挑战性（困难）。 那么，如何让模型自己选取需要掩蔽的 patches，以使得重建任务更困难呢？ 我们可以用重建损失来量化重建任务的困难程度，一般来说，重建损失越大，表示重建任务越困难，则对应的掩蔽策略就是我们需要的。 因此，作者引入了一个辅助的损失 predictor，用以预测每个 patch 的重建损失。并提出了一个 Easy-to-Hard 的掩蔽策略生成方法。 图1. 传统的 MIM 预训练范式与本文提出的 HPM 之间的比较。(a) 传统的方法可以理解为训练学生（即模型）解决给定任务（即预测掩蔽 patches）；b) 本文提出的 HPM 预训练范式使该模型既是教师又是学生，通过自适应调整掩蔽策略来产生一个具有挑战性的重建任务。 分析1600 个 epoch 预训练的 MAE 作为 MIM 模型，使用超过 10 种不同掩蔽策略产生的平均 patch 级重建损失来可视化每个 patch 重建的困难程度，如图 2 每组的前两张图片所示。 我们可以发现，通常图像的前景区域更难重建（拥有更大的重建损失）。 作者提出一个重建损失预测器，用以预测每个 patch 的重建损失。如图 2 每组的后两张图片，使用 200 个 epoch 预训练过的 ViT-B 来验证该预测器的有效性。 可以看出，具有较大预测损失的 patches 往往是有判别性的（前景几乎被屏蔽），因此屏蔽这些 patches 会使得重建任务更具有挑战性。 图2. 验证实验。每组第一张：输入图像；第二张：超过 10 种不同掩蔽策略的平均 Patch 级重建损失可视化；第三张：重建损失预测器预测的重建损失可视化；第四张：根据预测的重建损失选择掩蔽的 patches（例如，预测的重建损失达到 top-75% 的 patches 被掩蔽） 方法（参考原文作者对本文的介绍）HPM 由一个学生模型（ f_{θ_s} 、 d_{\\phi_s} 和 d_{ψ_s} ）和一个教师模型（ f_{θ_t} 、 d_{\\phi_t} 和 d_{ψ_t} ）组成。两个模型使用相同的网络结构。 其中 f_θ(\\cdot) 、 f_\\phi(\\cdot) 和 d_ψ(\\cdot) 分别是编码器、解码器和预测器。 为了使得结果尽可能一致，教师模型的参数将由学生模型指数平滑更新而来。 在每次预训练迭代中，将划分成 patches 的图像 x \\in \\mathbb R^{H×(P^2C)} 输入至教师网络，得到每个 patch 预测的重建损失 \\hat {\\mathcal L}^t&#x3D;d_{\\psi_t}(f_{\\theta_t}(x)) 。 基于 \\hat {\\mathcal L}^t ，使用 Easy-to-Hard 的掩蔽策略生成方法来产生掩蔽图像 M\\in \\left{0,1\\right}^N ，用于学生模型的重建任务。 学生模型的损失函数为真实重建任务和预测任务的损失之和： \\mathcal L &#x3D; \\mathcal L_{rec} +\\mathcal L_{pred} 其中， \\mathcal L_{rec} 就是传统的 MIM 重建损失： \\mathcal L_{rec}&#x3D;\\mathcal M(d_{\\phi_s}(f_{\\theta_s}(x\\odot M)),(x\\odot(1-M)))) \\mathcal M(\\cdot,\\cdot) 表示某种度量函数。 图3. HPM 框架 重建损失预测器为了使得重建损失预测器得到的损失预测值，与真实的重建损失尽可能一致，一种简单的方法就是，直接最小化真实重建损失 \\mathcal{L}_{\\mathrm{rec}} 和损失预测值 \\hat{\\mathcal L}&#x3D;d_{\\psi_s}(f_{\\theta_s} (\\mathbf{x} \\odot \\mathbf{M})) 之间的均方误差（MSE），即： \\mathcal{L}_{\\mathrm{pred}} &#x3D; \\left( d_{\\psi_s}(f_{\\theta_s} (\\mathbf{x} \\odot \\mathbf{M})) - \\mathcal{L}_{\\mathrm{rec}} \\right)^2 \\odot (1 - \\mathbf{M}) 这里的 \\mathcal{L}_{\\mathrm{rec}} 梯度已停止更新，作为预测损失 \\hat{\\mathcal L} 的金标准。 然而，我们的目标是确定图像中的困难 patches，因此我们需要学习 patches 之间的相对关系。而 MSE 关注的是预测损失值和真实损失值之间的绝对差异。在训练过程中，真实重建损失 \\mathcal{L}_{\\mathrm{rec}} 会随着训练的进行而减少，损失预测器可能更关注于损失值的准确性，而不是提取不同patches之间的相对困难程度。 为此，作者提出了一种基于二元交叉熵的相对损失作为替代方案。 具体来说，给定一张含有 N 个 patch 的图片，其真实的重建损失为 \\mathcal{L}_{\\mathrm{rec}} \\in \\mathbb{R}^N，我们的目的是预测这 N 个 patch 之间重建损失的相对大小，即 \\texttt{argsort}(\\mathcal{L}_{\\mathrm{rec}}) 。然而， \\texttt{argsort}(\\cdot) 并不可导，作者将问题转化为了预测两两 patches 之间的大小关系。 \\begin{aligned} \\mathcal{L}_{\\mathrm{pred}} &#x3D; -\\sum_{i&#x3D;1}^N \\sum_{j&#x3D;1 \\atop j\\neq i}^N \\mathbb{I}^{+}_{ij} \\log \\left( \\sigma(\\hat{\\mathcal{L}}^s_i - \\hat{\\mathcal{L}}^s_j) \\right) -\\sum_{i&#x3D;1}^N \\sum_{j&#x3D;1 \\atop j\\neq i}^N \\mathbb{I}^{-}_{ij} \\log \\left( 1 - \\sigma(\\hat{\\mathcal{L}}^s_i - \\hat{\\mathcal{L}}^s_j) \\right), \\end{aligned} 其中 \\hat{\\mathcal{L}}^s &#x3D; d_{\\psi_s}(f_{\\theta_s}(\\mathbf{x} \\odot \\mathbf{M})) \\in \\mathbb{R}^N 表示的是学生模型输出的损失预测值，而 i, j&#x3D;1,2,\\dots,N 是 patch indexes。 \\sigma(\\cdot) 是 \\texttt{sigmoid} 函数，即 \\sigma(z) &#x3D; e^z &#x2F; (e^z + 1) 。 \\mathbb{I}^{+}_{ij} 和 \\mathbb{I}^{-}_{ij} 是两个指示函数，表示 patch i 和 patch j 的真实重建损失大小，定义如下： \\mathbb{I}^{+}_{ij} &#x3D; \\left{ \\begin{aligned} &amp;1, &amp;&amp;\\mathcal{L}_{\\mathrm{rec}}(i) &gt; \\mathcal{L}_{\\mathrm{rec}}(j) \\mathrm{\\ and\\ } \\mathbf{M}_i&#x3D;\\mathbf{M}_j&#x3D;0, \\ &amp;0, &amp;&amp;\\mathrm{otherwise}, \\end{aligned} \\right. \\mathbb{I}^{-}_{ij} &#x3D; \\left{ \\begin{aligned} &amp;1, &amp;&amp;\\mathcal{L}_{\\mathrm{rec}}(i) &lt; \\mathcal{L}_{\\mathrm{rec}}(j) \\mathrm{\\ and\\ } \\mathbf{M}_i&#x3D;\\mathbf{M}_j&#x3D;0, \\ &amp;0, &amp;&amp;\\mathrm{otherwise}, \\end{aligned} \\right. 其中 \\mathbf{M}_i&#x3D;\\mathbf{M}_j&#x3D;0 表示的是 patch i 和 patch j 都应当是被掩蔽的。 Easy-to-Hard 掩蔽策略生成方法最简单的方式就是图 2 介绍的，直接将预测的重建损失 \\texttt{argsort}(\\hat{\\mathcal L}^t)&#x3D;\\texttt{argsort}[d_{\\psi_t}(f_{\\theta_t} (\\mathbf{x}))] 达到 top-75% 的 patches 掩蔽掉。 然而，在早期训练阶段，学习到的特征表示容易被丰富的纹理所淹没，这意味着，此时，大的预测重建损失不等同于该 patch 就难以判别。 为此，作者提出了一种 Easy-to-Hard的掩蔽策略生成方法，提供一些合理的提示，引导模型逐步重建困难 patches。 具体来说，对于第 t 轮 epoch，由 \\texttt{argsort}(\\hat{\\mathcal L}^t) 决定的 \\alpha_t \\cdot \\gamma N 个 patches 被掩蔽掉，然后剩余的 1-\\alpha_t \\cdot \\gamma N 个 patches 被保留， \\gamma 表示 mask radio， N 表示 patches 总数。而 \\alpha_t 被定义为： \\alpha_t&#x3D;\\alpha_0+\\frac{t}{T}(\\alpha_T-\\alpha_0) 其中， T 表示 epoch 的总轮数， \\alpha_0,\\alpha_T\\in[0,1] 是可调的超参数。 也就是说， \\alpha_t 随着 epoch 轮数的增加，会以线性方式从 \\alpha_0 增加到 \\alpha_T 。 这就相当于，随着 epoch 轮数的增加，重建任务由简单到困难。 整个训练过程的伪代码如图 4 所示： 图4. HPM 预训练伪代码 结果表1. 对不同重建目标的消融实验。表2. 不同掩蔽策略的消融实验。研究了不同的 α_0、α_T 和 γ 的影响。α_T 较大表示重建任务难度越大。表3. 不同掩蔽策略的消融研究。研究了 argmax(·) 对重建损失预测值和“easy-to-hard”方式的有效性。argmin(·) 表示掩蔽简单 patches。表4. 损失预测公式的消融实验。表5. 在下游任务上的性能表6. 与 ImageNet-1K 分类任务上 SOTA 模型的比较。表7. 与 ADE20k 语义分割任务上 SOTA 模型的比较。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | FreMAE：基于傅里叶变换的 MIM 用于医学图像分割","slug":"Masked_Image_Model/MIM-FreMAE基于傅里叶变换的-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-FreMAE基于傅里叶变换的-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-FreMAE%E5%9F%BA%E4%BA%8E%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E7%9A%84-Glenn/","excerpt":"","text":"题目： FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation单位：北京科技大学、中佛罗里达大学、伯明翰大学、理海大学论文网址：https://arxiv.org/abs/2304.10864论文代码：未公开首次发布时间：2023 年 4 月 21 日 123456@article&#123;wang2023fremae, title=&#123;FreMAE: Fourier Transform Meets Masked Autoencoders for Medical Image Segmentation&#125;, author=&#123;Wang, Wenxuan and Wang, Jing and Chen, Chen and Jiao, Jianbo and Sun, Lichao and Cai, Yuanxiu and Song, Shanshan and Li, Jiangyun&#125;, journal=&#123;arXiv preprint arXiv:2304.10864&#125;, year=&#123;2023&#125;&#125; 主要内容 目前 MIM 在医学领域应用的缺陷： 在某种程度上，MAE 仅将原始像素作为重建目标，主要依赖于局部特征表示，而没有充分利用全局信息。 由于该模型只有最后一个 block 的输出被送到解码器中用于重建任务，缺乏来自其他阶段的监督来提供多尺度信息。 由于高采集成本和患者隐私，通常小规模医学图像数据集的训练样本相对有限，需要行量身定制的设计。 如何解决？ 在自然图像中，详细的纹理信息主要存在于高频分量中，低频分量携带丰富的全局信息，如图 1 所示，该原则同样也可以应用于医学领域。 图1. 将相应的高&#x2F;低通滤波器应用于傅里叶频谱，分别获得傅里叶频谱、高频分量和低频分量的可视化基于此原则，作者提出了一种新的基于 MIM 框架，即FreMAE，用于解决上述问题。 FreMAE 首先屏蔽掉一部分随机选择的图像像素，然后在傅立叶域中预测输入图像的相应缺失频谱。由于同一器官的医学图像本质上对应于相似的特征，因此 FreMAE 使用困难的跨域重建任务，以避免使用捷径进行模型学习。 利用所提出的双边聚合解码器对原始图像依次应用傅立叶变换，并对转换后的傅立叶频谱依次应用低&#x2F;高通滤波器，以获得预期的重建目标。这种多阶段监督方法可以更好地指导模型预训练，从而产生更好的分割表示。 提出了一种有效的前景掩蔽策略作为原始随机掩蔽的替代方案，该策略被证明更适合于医学图像分割的纹理和细节建模。 方法前景掩蔽策略 与自然图像不同，医学图像中前景和背景分布极不平衡。因此，随机掩蔽策略将不可避免地导致生成的掩模主要覆盖医学图像的背景像素，并且保留了太多对象的前景像素。为此，需要提出一种简单而有效的前景掩蔽策略来解决这一不均匀分布问题。 由于医学图像通常包含不同的通道（医学图像通常是灰度图像，这里的意思应该是不同的切片数&#x2F;深度），每个通道拥有不同的前景区域，因此取这些通道的重叠部分作为最终的遮罩区域。总体的前景遮罩策略可以定义为： 对于每个通道，像素值为 0 的表示背景，不为 0 的表示前景； 将所有通道的二值遮罩进行逐像素的逻辑或操作，得到最终的前景遮罩。 这种方法能够在多通道医学图像中有效地识别前景区域，帮助进行自监督预训练。 通用编码器 正如在第一节中强调的，图像的高层次和低层次信息分布在傅立叶谱的不同频率带中。因此，建议分别利用低通和高通傅立叶谱作为监督信号（即重建目标）。最直观的方法是使用相同的高通傅立叶谱直接监督多个低级阶段，反之亦然。然而，这种直观的方式主要存在两个缺点： 不合理性：这种方法违反了模型在不同低级阶段学习的原始意图，因为在不同低级阶段学到的特征表示应该是不同的，而不是相同的。 过于简单：这种监督方法过于直接且简单，并没有充分利用分层结构捕获的多阶段特征之间的相关性来帮助模型更好地执行 MIM 务。 为了克服这些缺点，可以采用以下方法： 对于低级和高级阶段，分别设计不同的监督信号，以便在不同阶段学习不同的特征表示。例如，在低级阶段，可以关注低通傅立叶谱中的结构信息；而在高级阶段，可以关注高通傅立叶谱中的纹理和细节信息。 利用分层结构捕获的多阶段特征之间的相关性来设计更复杂的监督方法。例如，可以通过在不同阶段之间共享信息或融合多阶段特征来帮助模型更好地执行 MIM 任务。这将充分利用嵌套结构在各层之间捕获的信息，有助于提高模型性能。 具体来说，在所提出的双向聚合解码器（bilateral aggregation decoder, BAD）内部，不同阶段的编码特征分别以自底向上和自顶向下的方式汇聚到最低阶段（即具有最大空间分辨率）和最高阶段（即具有最小空间分辨率）。换句话说，双向聚合解码器将不同阶段的特征图分别聚合到最低和最高分辨率。 对于 ViT，第 4、8、12 层的特征图采用类似 UNETR 中的反卷积模块分别上采样 8、4、2 倍，然后输入到 BAD。每个相邻阶段捕获的特征将输入到卷积块中，以实现空间分辨率和通道维度的严格对齐。 接下来，高阶和低阶的聚合特征表示将通过引入的频率映射块（FMB）映射到频域，然后通过低通和高通滤波器得到相应的高通和低通预测频谱，用于计算重建损失。 具体来说，FMB 由二维离散傅立叶变换（2D-DFT）、频域感知器（Frequency Domain Perceptron, FDP）和二维离散傅立叶逆变换（2D-IDFT）组成，可以表示为： 计算高阶和低阶聚合特征表示的 2D-DFT，将其映射到频域。 将映射到频域的特征表示输入到 FDP 中，进一步处理和提取有用信息。FDP 是一个包含多个卷积层和激活函数的神经网络模块，用于在频域中学习特征。 计算 FDP 输出的 2D-IDFT，将其映射回空域。 经过这个过程，得到了低阶和高阶的聚合特征表示的频率谱。 图2. FreMAE 架构频率损失 为了缓解不同频段频谱之间的权重不平衡，便于困难频段的重建，采用聚焦 focal frequency loss 作为损耗函数 \\mathcal L_{freq} ，实现低频映射和高频映射权重的梯度更新，定义为： \\mathcal{L}_{freq}&#x3D;\\frac{1}{HW}\\sum^{H-1}_{u&#x3D;0}\\sum^{W-1}_{v&#x3D;0}w(u,v)\\odot\\gamma(f(u,v),\\hat{f}(u,v))^2 其中， f(u,v) 表示在空间频率坐标 (u, v) 上经过 2D-DFT 后的预测值， \\hat{f}(u,v) 表示对应的金标准， γ(f, \\hat{f}) 计算实际值和预测值之间的欧几里得距离的平方，作为它们的频率距离。 ω 是给定位置的频谱权重矩阵，用于抑制容易频率的权重。具体计算公式如下： ω(u, v) &#x3D; γ(f(u, v), \\hat{f}(u, v))^β γ(f, \\hat{f}) &#x3D; \\sqrt{ (R − \\tilde R)^2 + (I − \\tilde I)^2} β 是缩放因子（默认为 β&#x3D;1 ）。 总损失 \\mathcal L&#x3D;\\mathcal L_{freq}(F_H(P_{low}), F_H(T))+\\alpha\\mathcal L_{freq}(F_L(P_{high}),F_L(T)) 其中 F_H 和 F_L 分别表示高通和低通频率滤波器。 T 表示原始图像。 P_{low} 是通过最高阶段获得的， P_{high} 是相反的。 α 是高级语义信息分支的权重（默认情况下为 α&#x3D;3 ）。 结果图3. 通过 FreMAE 在频域内的重建结果的可视化。表1. 自监督学习框架比较。‘-’代表从头开始的训练。表2. 在 BraTS 2019、ISIC 2018 和 ACDC 2017 数据集的性能比较。表3. 关于重建目标和监督方案的消融研究。表4. 关于掩蔽策略的消融研究。表5. 关于掩蔽比的消融研究。表6. 自监督预训练样本数量的消融研究。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | EMAE：解决 MIM 数据利用率低和预测结果不一致性问题","slug":"Masked_Image_Model/MIM-EMAE解决-MIM-数据利用-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-EMAE解决-MIM-数据利用-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-EMAE%E8%A7%A3%E5%86%B3-MIM-%E6%95%B0%E6%8D%AE%E5%88%A9%E7%94%A8-Glenn/","excerpt":"","text":"题目：Effificient Masked Autoencoders with Self-Consistency单位：中国科学院、中国科学院大学、商汤、鹏城实验室论文网址：https://arxiv.org/abs/2302.14431论文代码：未公开首次发布时间：2023 年 2 月 28 日 主要内容MIM 的高随机掩蔽比将导致两个严重问题： 数据未被有效利用，导致预训练效率低； 例如，在每个 epoch 中 MAE 仅利用整个图像的 25% 训练模型。相比之下，BERT 使用了 85% 的文本语料库。由于 MIM 的数据利用率不足，预训练 epoch 通常高于 MLM（masked language modeling），MAE 的 1600 个 epoch 相当于 MLM 的 40 个 epoch。此外，对于 MAE 需要 1600 个 epoch，而对于有监督学习只需要 300 个epoch。 预训练模型的不确定性和不一致性高，即同一块区域在不同 epoch 的随机掩蔽策略下，产生的预测可能不一致，如图 1 所示。 图1. 不同的随机掩蔽策略对应不同的 MAE 重建结果。(a) 由不同的随机掩蔽策略采样的掩蔽图像。(b) 对应的 MAE 重建结果。对于 (b) 的三个重建，只有第一个代表一只正常的牛，第三个甚至重建了一只狗。可见 MAE 重建的语义是不一致的。为了解决这些问题，作者提出了一种具有自一致性的高效掩码自动编码器（EMAE），主要从两方面进行改进： 1）将图像逐步分成 K 个不重叠的部分，每个部分由掩蔽策略随机生成，具有相同的掩蔽比。然后，在每个 epoch 中，所有部分并行进行 MIM 任务并生成预测。 2）设计自一致性模块，以进一步维护部分之间重叠掩码块的预测一致性。 ImageNet 上的实验表明，与 MAE（1600 个 epoch）相比，EMAE 在 ViT Base 下仅用 300 个预训练epoch 就获得了更高的结果。EMAE 在各种下游任务（如对象检测和语义分割）上也始终获得最先进的传输性能。 方法图2. EMAE 模型结构。整个图像首先被逐渐划分为 K 个不重叠的部分，并且每个部分具有相同数量的可见 patches。然后，每个部分被送入编码器-解码器架构，并行执行 MIM 任务。此外，自一致性模块指导预测部分之间的重叠的 patches 被拉在一起。 有效利用全部数据为了提高数据的利用率，EMAE 将使用全部数据训练模型。 具体来说，就是要将输入图像 x 划分为 K 个具有相同数量且不重叠的可见 patches 的部分，然后所有部分并行执行 MIM 任务。 首先将整个图像分成 N 个图像 patches。然后，设置一个长度为 N 的随机张量 t ，其值在 [0,1] 上均匀分布。张量按值按升序排序，排序后的索引 ids 用如下表示： ids&#x3D;s(t) 其中 s(\\cdot) 表示按值按升序排序。 将长度为 N 的排序索引等分划分为 K 个部分 ids_1，ids_2，…， 和 ids_K ： ids_i&#x3D;ids[(i-1)×\\frac{N}{K}: i×\\frac{N}{K}] 其中 i\\in\\left{1,2，…，K\\right} 。 因此， N 个图像 patches 可以根据索引 ids_i 等分为 K 个部分 x_{1}，x_{2}，…， 和 x_{K} ，且每个部分具有相同数量且不重叠的可见 patches x_{v_i} ： x_{v_i}&#x3D;d(x_i, ids_i) 其中 d(\\cdot) 表示根据索引 ids_i 从输入 x_i 中查找可见 patches。 对应的，掩蔽 patches 为： x_{m_i}&#x3D;d(x_i, ids-ids_i) 此时，任一部分 x_{i} 都有 \\frac{N}{K} 个可见 patches， N-\\frac{N}{K} 个掩蔽 patches x_{m_i}。 则任一部分的掩蔽比都为 (N−\\frac{N}{K})&#x2F;N &#x3D;(K−1)&#x2F;K ，算法伪代码如图 3 所示。 图3. 算法伪代码根据上述描述，当 K 设置为 4 时，各部分的掩蔽比为 75%（与 MAE 的掩蔽比相同）。 该设计保证了迭代中图像的每个 patches 都可以采样一次，使得整个图像都可以应用于模型的训练，从而提高了数据的利用率。 损失函数定义如下： \\mathcal{L}_{whole}(x)&#x3D;\\mathbb E_{i\\in[1,K]}\\mathcal{L}(x_{v_i},x_{m_i})&#x3D;\\mathbb E_{i\\in[1,K]}||g(f(x_{v_i})) − x_{m_i}||^2 其中， g(\\cdot) 表示解码器。 自一致性模块进一步地，需要鼓励预训练模型的预测在来自同一图像的不同输入可见 patches 下是一致的。 根据 2.1 节介绍的内容，每个部分 x_{i} 具有 N-\\frac{N}{K} 个掩蔽 patches x_{m_i} ，对应的，都会生成 N-\\frac{N}{K} 个预测 x_{p_i} 。 显然，任何两个部分的预测之间都存在一定的重叠 patches，比例为 \\frac{K-2}{K-1} 。 重叠位置 s_{ij} 可以通过掩蔽 patches x_{m_i} 和 x_{m_j} 得到，其中 i, j \\in \\left{1,2,3,…,K\\right} 且 i \\neq j ： s_{ij}&#x3D;x_{m_i}\\cap x_{m_j} EMAE 的自一致性模块用来指导每个重叠位置的预测保持一致。如图 2 所示，自一致性模块将任意两个预测 x_{p_i} 和 x_{p_j} 之间的重叠预测拉到一起，这使重叠重建结果之间的平均绝对误差最小化，以提高一致性。 损失函数定义如下： \\mathcal{L}_{consistency}(x)&#x3D;\\mathbb{E}_{i\\in [1,K],j\\in[i+1,K]}\\mathcal{L}_{sc}(x_{v_i}, x_{v_j})&#x3D;\\mathbb{E}_{i\\in [1,K],j\\in[i+1,K]}(||sg[x_{p_i}]-x_{p_j}||+||x_{p_i}-sg[x_{p_j}]||)\\odot s_{ij} 其中 sg[·] 表示停止梯度。对于任一部分的每次预测，将与其他部分的预测进行 K−2 次计算。 总损失为： \\mathcal{L}_{total}(x)&#x3D;\\mathcal{L}_{whole}(x) + \\mathcal{L}_{consistency}(x) 结果由表 1 可以得出： 在线性探测中，相同的预训练 epoch（300 和 800）下，ViT-base 的 EMAE 比 MAE 的精度高 6.0%～6.7%。 ViT-base 的 EMAE epoch 300 的分类结果与 MAE 2400 epoch 的分类结果相当。（GPU-days？） ViT-large 的 EMAE epoch 800 的分类结果与 ViT-huge 的 MAE 1600 epoch 的分类结果相当。 由于基于对比学习的方法具有图像语义一致性的假设，并且该假设与线性探测任务的先验一致，因此 EMAE 的线性结果略低于基于对比学习的算法。 表1. SOTA 自监督学习方法在 ImageNet top-1 精度比较。LP：线性探测；FT：端到端微调。表 2 、3 、4 可以得出，EMAE 的性能优于之前的 SOTA 自监督学习方法。 表2. 使用带 FPNs 的 Mask R-CNN 对 COCO 上的目标检测和实例分割结果进行微调。表3. 使用 ViTDet 对 COCO 上的目标检测和实例分割结果进行微调。表4. 使用 UperNet 对 ADE20K 上的语义分割结果进行微调。表 5、6 反映了 EMAE 中全数据利用、自一致性模块以及 K 值取值的影响。图 4 是自一致性模块有效性的可视化证明。 表5. 消融实验。全数据利用和自一致性模块的影响。表6. 消融实验。K 值取值的影响。图4. 不同的随机掩蔽策略对应不同的 EMAE 重建结果。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | DPPMask：可以保留具有代表性 patches 的掩蔽策略","slug":"Masked_Image_Model/MIM-DPPMask可以保留具有代表-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-DPPMask可以保留具有代表-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-DPPMask%E5%8F%AF%E4%BB%A5%E4%BF%9D%E7%95%99%E5%85%B7%E6%9C%89%E4%BB%A3%E8%A1%A8-Glenn/","excerpt":"","text":"题目：DPPMask: Masked Image Modeling with Determinantal Point Processes单位：中国科学院深圳先进技术研究院、中国科学院大学、之江实验室、香港中文大学论文网址：https://arxiv.org/abs/2303.12736论文代码：未公开首次发布时间：2023 年 3 月 13 日（2023 年 3 月 25 日 v2 版本上传） 123456@article&#123;xu2023dppmask, title=&#123;DPPMask: Masked Image Modeling with Determinantal Point Processes&#125;, author=&#123;Xu, Junde and Lin, Zikai and Zhou, Donghao and Yang, Yaodong and Liao, Xiangyun and Wu, Bian and Chen, Guangyong and Heng, Pheng-ann&#125;, journal=&#123;arXiv preprint arXiv:2303.12736&#125;, year=&#123;2023&#125;&#125; 前言掩蔽策略被证明是 MIM 中较为重要的部分。PixMIM 使用 Simple Resized Crop 代替原始 MAE 使用的 Random Resized Crop，为了保留更多的前景 patch；AutoMAE 更是将生成对抗网络用于优化掩蔽策略。DPPMask 基于行列式点过程 (determinantal point process, DPP)，提出了 DPPMask 的 MIM 掩蔽策略，可以保留更具有代表性 patches。 主要内容在实际情况中，强制模型重建无法恢复的内容是不合理的。 考虑一个简单的情况，如图 1 所示。 图 1 的顶行显示了 MIM 的基本逻辑：成功的重建意味着网络捕捉了正确的语义特征。 而图 1 的底行则显示了一个失败的情况：如果遮蔽过程恰好遮蔽了原始图像的一个重要语义，如图 1 中的书，则会改变原始图像的语义并使网络难以从其余部分中恢复它。 在这种情况下，如果继续强制模型重建原始图像，则模型可能会填充遮挡的图像，干扰学习原始特征的过程。此外，随着遮挡率的增加，原始语义信息被扭曲的概率也会增加。 作者将这种情况称为 misalignment problem，即遮蔽图像的语义与原始图像的语义不匹配。因此，misalignment problem 将导致不恰当样本对的对齐，最终会损害下游任务的性能。 图1. MIM 中的 misalignment problem 的说明。在对比学习中，InfoMin 原则（What Makes for Good Views for Contrastive Learning?）表明：一个图像的两个增强视图应该保留与任务相关的信息，同时最小化不相关的细微干扰。 作者认为 MIM 中也应该拥有如下原则： 所选的可见 patches 应该具有足够的代表性，以覆盖原始图像的整个语义信息。 掩蔽比应设置为较高水平，以最小化同一图像的不同 masks patches 之间的无关信息共享。 对此，作者基于行列式点过程 (determinantal point process, DPP)，提出了 DPPMask 的 MIM 掩蔽策略。 DPP 是一个集合概率模型，它可以从一个底层集合中采样出具有多样性和高质量的子集。在采样过程中，DPP 会计算每个 patch 的距离，并选择与已选子集不相似的 patch。这个过程使得网络集中于具有更多代表性信息的 patch。 misalignment problem 分析图2. (a) 输入图像；(b) 某一个 epoch&#x2F;step 选择的掩蔽图像。MIM 的训练过程中，每个 epoch&#x2F;step 会随机选择不同的掩蔽图像 ，如图 3 Input Space 所示，橙色、绿色、蓝色虚线框分别表示不同 epoch&#x2F;step 选择的掩蔽图像，通过训练网络利用这些掩蔽图像来重建原始图像。 在重建过程中，网络会根据每个 epoch&#x2F;step 选择的掩蔽图像将学习到的信息构建一个语义空间。如图 3 Semantic Space 所示，假设网络可以根据绿色虚线框选择的掩蔽图像学习到原始图像的语义，则重建的目标可以定为使得网络从不同掩蔽图像学习到的信息尽可能的接近原始图像语义，这是通过最小化语义空间中每个数据点之间的距离来实现的。当错误地聚合了语义偏差的数据点（图 3 中的橙色点）时，就会出现未对齐的情况，从而导致语义空间不对齐。 在实际情况中，语义在图像中的分布并不均匀。这些语义很可能因为随机掩蔽策略而被忽略。当 MIM 将不同的掩蔽图像拉到一起时，具有不同语义的两个掩蔽图像会错位。 如果错位的语义是理解图像的重要线索，那么可能会严重影响下游任务性能。为此，需要一种新的掩蔽采样策略，以保留更具有代表性的 patches。 图3. misalignment problem 的说明 方法作者基于行列式点过程 (determinantal point process, DPP)，提出了 DPPMask 的 MIM 掩蔽策略。 DPP 的介绍如下： 对于给定的集合 Z ， Discrete Point Processes 是定义在该集合的所有子集上的一个采样概率。 而 Determinantal Point Process 将 Discrete Point Processes 问题里复杂的概率计算转换成简单的行列式计算，通过核矩阵（kernel matrix） L 的行列式计算每一个子集的采样概率。 即存在一个实数型、PSD（半正定）矩阵 L ，对于集合 Z 的所有子集 Y ， Y 采样的概率与矩阵 L_Y 的行列式成正比： $$ \\mathcal{P}(Y)\\propto det(L_Y) $$ 在本文中，首先计算列表中任意两个 patches 之间的相似度 S_{ij} （先将 patches 表示成特征向量 S_i,S_j ，然后使用相似度度量方法计算相似度），得到相似度矩阵 S ，然后用矩阵 S 的行列式 det(S)&#x3D;S_{ii} × S_{jj} − S_{ij} × S_{ji} 来表示 patches 集合多样性程度（因为矩阵是一组向量的集合，而矩阵的行列式的物理意义为矩阵中的各个向量构成的平行多面体体积的平方。这些向量彼此之间越不相似，相似性度量方法计算出的结果就会越小，构成的平行多面体的体积也就越大，矩阵的行列式也就越大，对应的 patches 集合的多样性也就越高）。 DPPMask 的目的便是从候选 patches 集合 Z 中选择能够最大化后验概率的 patches 子集 Y 来保留： Y_{MAP}&#x3D;\\underset{Y\\in Z}{argmax}, det(L_Y) 该问题是一个 NP-hard 问题。因此，在实际场景中，多使用贪心算法来获取该问题的近似解。 求解过程简单来说就是，每次从候选集中贪心地选择一个能使边际收益最大的 patch 加入到最终的结果子集中，直到满足停止条件为止，即每次选择 patch j 添加到结果集 Y_g 中， Y_g 初始化为空集，patch j 需要满足下面的等式： $$ j&#x3D;\\underset{i\\in Z\\backslash Y_{g}}{argmax}\\ l o g d e t(L_{Y_{g}\\cup{i}})-l o g d e t(L_{Y_{g}}) $$ 其中行列式 det(L_{Y_g}) 直接求解复杂度较高，可以使用其他方法简化计算过程。感兴趣的可以查看博文和原论文。 在本文中，作者将核矩阵 L 设置为 L_{ij} &#x3D; exp(-\\frac{||S_i-S_j||^2}{\\epsilon}) ，其中 \\epsilon 经验值为 1。 另外，如果一味地执行贪心算法，会导致过度保留。如图 4 所示，由于天空的 patches 彼此过于相似，前景比背景更多样，此时贪心的选择将只关注前景。这种情况会使得 MIM 任务过于简单，这对特征学习没有帮助。 图4. 对三种不同清除率进行定性比较，一个合适的清除率可以保持原始图像的语义，同时保持增强输入的多样性。为了解决这一问题，作者设置了一个称为清除率 \\tau\\in (0,1) 的参数作为最大边际增益的阈值。 具体来说，在每次迭代中，都会监控下一个 patch 与所选子集的距离，如果该距离低于清除率，则贪心的选择过程将中止，子集由随机 patches 填充。清除率可以调整 DPPMask 的 “严重” 程度，当 \\tau &#x3D; 0 表示选择过程完全由贪心算法选择，而 \\tau &#x3D; 1 表示完全随机抽样。 DPPMask 的算法伪代码如图 5 所示： 图5. DPPMask 的采样算法伪代码 结果通过观察表 1 、表 2 和表 3，可以得到： 在 ImageNet-100 数据集上，MAE 和 iBOT 的微调任务性能均得到了提升。 在 MAE 中，加入 DPPMask 的精度提高了0.2%； 在 iBOT 中，加入 DPPMask 的精度提高了0.4%。 随着阈值 τ 的减小，预训练损失相应地变小。然而，当 τ 过低时，贪心采样会过度保留，使任务过于简单，网络无法学习有用的特征（？）。 对于线性探测，加入 DPPMask 的 MAE 性能显著下降。这是因为 DPP 使样本空间缩小，以清除未对齐的样本。将更少的样本聚集在一起会使特征空间更连续，并且线性可分离性更低。 相比于其他采样方法，DPPMask 取得了更好的成绩。 表1. MAE+DPPMask 在 ImageNet- 100上 的详细结果。表2. iBOT+DPPMask 在 ImageNet- 100上 的详细结果。表3. 与 ImageNet-1K 上的其他采样方法的比较。观察图 4，可以得到： Mask Ratio 越大，越容易造成 misalignment 问题。作者发现对于微调任务来说，0.75 并不是一个好的 Mask Ratio 值。 加入 DPPMask 的 MAE 在任意的 Mask Ratio 上都取得了更高的性能。 图6. 不同 Mask Ratio 下在 ImageNet-100 上 MAE 的精度。观察表 4 和表 5 ，说明在 COCO 和 CLEVR 数据集的多标签分类任务上，DDPMask 也能取得较高的精度。 表4. 在 COCO 上的多标签分类精度。表5. 在 CLEVR 上的多标签分类精度。2023 年 3 月 25 日上传的 v2 版中增加了附录实验，详细结果请查看原文。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | DeepMIM：将 Deep Supervision 引入掩蔽图像建模","slug":"Masked_Image_Model/MIM-DeepMIM将-Deep-S-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-DeepMIM将-Deep-S-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-DeepMIM%E5%B0%86-Deep-S-Glenn/","excerpt":"","text":"题目：DeepMIM: Deep Supervision for Masked Image Modeling作者单位：微软亚研院、剑桥论文网址：https://arxiv.org/abs/2303.08817论文代码：https://github.com/OliverRensu/DeepMIM首次发布时间：2023 年 3 月 15 日 123456@article&#123;ren2023deepmim, title=&#123;DeepMIM: Deep Supervision for Masked Image Modeling&#125;, author=&#123;Ren, Sucheng and Wei, Fangyun and Albanie, Samuel and Zhang, Zheng and Hu, Han&#125;, journal=&#123;arXiv preprint arXiv:2303.08817&#125;, year=&#123;2023&#125;&#125; 主要内容这篇论文也是在该团队 MIM 宇宙基础上提出的新架构 DeepMIM。主要内容是在 Masked Image Modeling 训练过程中加上 Deep Supervision，可以促进浅层学习更有意义的表示，加快模型收敛速度并扩大注意力的多样性。 Deep Supervision（即在神经网络的中间层引入额外的监督）在早期的深度学习中，Deep Supervision 可以解决深度神经网络训练梯度消失和收敛速度过慢的问题， 而被广泛应用于 CV 领域，随着 normalization 和 residual connection 的出现，Deep Supervision 逐渐被淘汰。 方法DeepMIM 采用编码器-多解码器架构进行 ViT 预训练的掩模和预测任务。具有12个Transformer 块的 ViT-B 作为编码器，4个独立的具有 4 层 Transformer 块的解码器分别置于编码器的第 6、8、10、12 个 Transformer 块后。 在 Masked Image Modeling 上构建这种架构的难度主要在于，如何从原始输入中提取监督信号来指导中间层的学习。浅层 ViT 产生的特征区别较差，这些中间特征可能没有能力重构过于复杂的目标。 对此，DeepMIM 提出可选择的 Hybrid Target Generator 模块，将预训练好的 MAE 产生的模糊重建结果与原始像素按比例混合后作为中间层的监督信号。 t &#x3D; αx + (1 − α)\\hat{x}. t_i 表示解码器 g^i_ξ 的重建目标。对于 g^1_ξ 、 g^2_ξ 和 g^3_ξ ，将 α 分别设置为 0、1&#x2F;3 和 2&#x2F;3。 尽管使用 Hybrid Target 作为中间层监督信号可以提高微调性能，但存在额外的计算开销。所以作者建议，只有在有一个现成的一个预训练过的 MIM 模型时，才使用 Hybrid Target。 不使用 Hybrid Target，所有解码器设置为 α &#x3D; 1 。 总损失是由 N 个额外解码器和 1 个主解码器产生的 N+1 个 \\mathscr{L}_2 重建损失之和： \\mathcal{L} &#x3D; \\sum_{i&#x3D;1}^N||M(t_i)-p_i||^2_2+||M(x)-p||^2_2 其中， M(\\cdot) 表示提取目标 masked patches 的操作，N 为额外解码器的数量。 DeepMIM 也可以应用于一系列具有不同重建目标的 MIM 模型。 图1 DeepMIM 网络框架 这篇论文让我想到了 CVPR23 的 LocalMIM（Masked Image Modeling with Local Multi-Scale Reconstruction）。这两篇应该是同时期论文，DeepMIM 在 arxiv 上提交的时间是 2023 年 3 月 15 ，LocalMIM 提交的时间是 2023 年3 月 9 日，从格式上看，猜测应该是在 CVPR 上撞车了)。 LocalMIM 同样是将 Deep Supervision 引入到了 Masked Image Modeling 中，但他是从多尺度重建的角度讲故事。 DeepMIM 主解码器和中间层解码器的监督信号在不使用 Hybrid Target 情况下是一致的。而 LocalMIM 不同层解码器的监督信号是不同的，使得较低层重构细尺度的监督信号，而较高层重构粗尺度的监督信号。 图2 LocalMIM 网络框架 讨论 DeepMIM 使用训练集和验证集损失曲线，证明在下游任务的微调期间的表现要由于 MAE。根据该团队另一篇论文（On data scaling in masked image modeling）的理论，预训练中的验证损失可以很好的指示模型在下游任务的微调期间的表现。 On data scaling in masked image modeling 解读看这里 图3 MAE 和 DeepMIM-MAE 的训练损失（左）和验证损失（右）的比较。只显示了 DeepMIM-MAE 的最后一层的重建损失 DeepMIM 使用 centered kernel alignment（CKA）来识别最后一层产生的特征和中间层产生的特征之间的对应关系。从第一层到倒数第二层，DeepMIM-MAE 的 CKA 得分总是超过 MAE，说明DeepMIM-MAE 的中间层的特征更具鉴别性。 图4 使用 CKA 评估来自最后一层的特征和来自中间层的特征之间的对应关系。蓝色：MAE；红色：DeepMIM-MAE DeepMIM 使用 CKA 来计算来自 MAE 第 8 层的特征与来自 DeepMIM-MAE 所有层的特征之间的相似性，MAE 的中间层（第8层）特征与 DeepMIM-MAE 的较浅的层特征（第3层和第4层）具有最大的对齐。相比之下，DeepMIM-MAE 的中间（第8层）特征与 MAE 的更深块（第层和第10层）的特征更加紧密。本研究表明，DeepMIM显著增强了特征对浅层的鉴别能力。 图5 由 CKA 评估的交叉特征相似性 DeepMIM 计算不同 attention heads 之间的余弦相似性来探索 head 的多样性。根据该团队另一篇论文（Revealing the dark secrets of masked image modeling）的说法，更多样化的 head 表明有更强的代表能力。研究表明，与MAE相比，DeepMIM 产生的 head 更多样化。 Revealing the dark secrets of masked image modeling 解读看这里： 图6 比较 MAE（左）和 DeepMIM-MAE（右）在不同层上的 head 余弦相似度 为了进一步评估来自浅块的特征的质量，DeepMIM 冻结了浅层的一个子集，并对其余的层进行微调。当可训练块的数量从 1 个（只有最后一个层是可训练的）变化到12个（所有层都是可训练的）时，DeepMIM-MAE 的性能始终显著优于 MAE。 图7 当将可训练层的数量从 1 改变到 12 时，DeepMIM-MAE 的性能始终明显优于 MAE DeepMIM 随机初始化预训练的 ViT-B 的最后 K 个层，然后以端到端的方式在 ImageNet 上对 ViT-B 进行微调。DeepMIM-MAE 在每种情况下都始终优于 MAE，这表明浅层的良好表示有利于更深层的学习，尤其是当它们被随机初始化时。 图8 在 ImageNet-1K 上的端到端微调","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | CCViT：基于 Patches 质心重建的 MIM 框架","slug":"Masked_Image_Model/MIM-CCViT基于-Patches-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-CCViT基于-Patches-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-CCViT%E5%9F%BA%E4%BA%8E-Patches-Glenn/","excerpt":"","text":"题目：Centroid-centered Modeling for Efficient Vision Transformer Pre-training单位：武汉大学、京东、悉尼大学论文网址：https://arxiv.org/abs/2303.04664论文代码：未公开首次发布时间：2023 年 3 月 8 日 前言陶老师挂名的一篇论文。 之前介绍的都是以像素为重建目标的 MIM 模型，另一种主流的重建目标是 tokens，比如 BEiT、iBOT 等。而 CCViT 找出了以 tokens 为重建目标的 MIM 序列模型的缺陷，并提出了一种基于 patches 质心的 tokenizer 方式，可以有效缩短 tokens 生成的时间。 主要内容目前以重建目标为改进方向的 MIM 方法主要有两种类型：一种是基于像素重建的 MIM，另一种基于 token 或高级特征重建的 MIM。 然而，这两种方式都有其缺点： 首先，它们都引入了一个冗余模块来将潜在表示转换为原始像素。基于像素的 MIM（如MAE）需要一个冗余解码器。基于 token 的 MIM，如 BEiT，需要一个 tokenizer 模型来将图像像素转换为离散tokens。 此外，来自 tokenizer 的 tokens 并不能表示对应的图像 patches，因为 tokenizer 是基于整个图像生成 tokens，而不是单个 patch。因此，即使某个 patch 保持不变，如果来自其他 patches 的像素被修改，则该 patch 生成的 token 也可能被改变。 tokenizer 的训练通常需要消耗大量时间。 对此，CCViT ： 1）提出了一种 patch 质心 tokenizer，用于生成局部性 token； 2）提出了一种新的掩蔽策略：质心 patch 替换，可以结合 BEiT 的 blockwise masking； 3）提出了像素与 tokens 结合的混合重建目标。 方法 构建 patches 的质心 codebook使用 k-means 聚类方法，从 N×n 个 patches \\left{x_i^p\\right}^{N×n}_{i&#x3D;1} ，展平为向量形式\\left{x_i^v \\in \\mathbb{R}^{C×P^2}\\right}^{N×n}_{i&#x3D;1} 后，寻找其 K 个质心 \\left{\\mathcal C_k\\in\\mathbb R^{C×P^2}\\right}^K_{k&#x3D;1} ，损失函数为： $$ \\mathbb{E}(\\mathcal C_1,…,\\mathcal C_K)&#x3D;\\frac{1}{N}\\sum_{i&#x3D;1}^{N}||x_i^v-\\mathcal C_{a(i)}||2\\ a(i) &#x3D; argmin{k∈\\left{1，…，K\\right}}||x^v_i−\\mathcal C_k||_2 $$ 其中， N 是训练图像数量， n&#x3D;HW&#x2F;P^2 是单张图像 x\\in\\mathbb R^{C×H×W} 的 patches 数量。 训练完成后，可以得到一个质心 codebook \\left{\\mathcal C_k\\in\\mathbb R^{C×P^2}\\right}^K_{k&#x3D;1} 和对应的 index codebook \\left{k\\right}^K_{k&#x3D;1} 。 给定一个图像 patch 将其展平为向量形式 x_i^v∈\\mathbb R^{C×P^2} ，其 index token t_i 可以通过查找最近质心的索引得到： t_i&#x3D;argmin_{k\\in \\left{1,…,K\\right}}||x_i^v-\\mathcal C_k||_2 同时，将该 patch x_i^v∈\\mathbb R^{C×P^2} 替换为 \\mathcal C_{t_i}\\in\\mathbb R^{C×P^2} 后，可以得到一个质心 patch。 图1. 构建 patches 的质心 codebook 设置掩蔽策略掩蔽策略使用了 BEiT 的 blockwise masking 和质心 patch 替换两种方式。 质心 patch 替换是指在 blockwise masking 后，随机使用对应的质心 patch 替换剩下的未被屏蔽的 patch。 给定掩蔽比 r_m 和替换比 r_{re} ，掩蔽位置 \\mathcal M∈\\left{1，…，n\\right}^{r_m×n} ，替换位置 \\mathcal R∈\\left{1，…，n\\right}^{r_{re}×n} 。被替换位置的前提是该位置未被屏蔽，即 \\mathcal M \\bigcap R&#x3D;∅ 。 给定一个图像 x∈\\mathbb R^{C×H×W} 及其 patches \\left{x_i^p\\right}^n_{i&#x3D;1} ，经过掩蔽策略后转换为： $$ \\tilde{x}&#x3D;\\left{x_i^p \\boldsymbol{E}p \\mid i \\notin \\mathcal{M} \\bigcup \\mathcal{R}\\right}{i&#x3D;1}^n \\bigcup\\left{e_m \\mid i \\in \\mathcal{M}\\right}{i&#x3D;1}^n \\bigcup\\left{C{a(i)} \\boldsymbol{E}p \\mid i \\in \\mathcal{R}\\right}{i&#x3D;1}^n\\ a(i) &#x3D; argmin_{k∈\\left{1，…，K\\right}}||x^v_i−\\mathcal C_k||_2 $$ 其中， e_m 是可学习的 mask token embedding， x \\boldsymbol{E}_p 表示 patch embedding 的计算过程。 图2. 设置掩蔽策略 使用 ViT 作为主干网络编码器分为像素预测模块和质心 index 预测模块。 质心 index 预测模块使用完整的 ViT 网络，使用线性头 lin.1(\\cdot) 将输入向量 \\boldsymbol{H}0&#x3D;\\left[\\boldsymbol{E}_{\\mathrm{CLS}}, \\tilde{x_1} \\boldsymbol{E}_p, \\ldots, \\tilde{x_n} \\boldsymbol{E}_p\\right] 通过 L 层 token block ψ_t^L(\\tilde x) 后产生的表征映射到质心 index 空间中。 由于基于质心的建模具有局部性，需要 CLS token 来显式地聚合全局表示，以补偿由于将原始图像 patch 降采样到质心所造成的信息损失。所以添加了一个像素预测模块用于收集全局信息。 将从第 l 层获取的早期表征定义为 [h^l_1，…, h^l_n] ，最后一层第 L 层获取 CLS token 定义为 h^L_{CLS} 。 像素预测模将表征 \\boldsymbol{H}p&#x3D;\\left[h^L_{CLS},h^l_1，…, h^l_n\\right] 输入两层 pixel ViT block ψ_p^2(H_p) 中，再使用线性头 lin.2(\\cdot) 将输出映射到原始像素空间中。 图3. CCViT 完整框架损失函数如下： $$ \\begin{aligned}\\mathcal{L}_{\\mathrm{CE}} &amp; \\left.&#x3D;-\\sum{\\tilde{x}} \\sum_{i \\in \\mathcal{T}} \\log \\left(p_{\\mathrm{CIM}}\\left(t_i \\mid \\tilde{x}\\right)\\right)\\right) \\ p_{\\mathrm{CIM}}\\left(t_i \\mid \\tilde{x}\\right) &amp;&#x3D; softmax_{t_i} (lin.1◦ψ_t^L(\\tilde x)) \\ \\mathcal{L}_{\\mathrm{MSE}} &amp; &#x3D;-\\sum{\\tilde{x}} \\sum_{i \\in \\mathcal{T}} \\frac{1}{\\lambda\\left(\\tilde{x}^{\\mathcal{T}}\\right)}\\left|x^{p \\mathcal{T}}-\\psi_p^2\\left(\\boldsymbol{H}p\\right)^{\\mathcal{T}}\\right|2 \\\\mathcal{L}_{\\mathrm{CIM}} &amp; &#x3D;\\mathcal{L}{\\mathrm{CE}}+\\mathcal{L}_{\\mathrm{MSE}}\\end{aligned} $$ 结果实验参数设置请查看原文。 1）基线模型对比。从表 1 可以得出： CCViT 比 MAE 和其他基于像素重建的 MIM 方法精度高，说明 CCViT 方法可行且有效。 CCViT 比其他训练周期更长的 MIM 方法相比人有竞争力，这表明质心是比只用 token 和只用 pixel 更有效的重建目标。 CCViT 比 BEiTv2 精度低，说明 CLIP Feature 作为重建目标还是顶的。 表1. ImageNet-1K 上图像分类 top-1 accuracy (%) 和 ADE20K 上语义分割的 mIoU (%) 的微调结果。†：MAE 复现结果。2）消融实验。从表 2 可以得出： 将 tokens 和像素同时作为目标可以获得最好的结果，说明基于质心的建模中同时学习 tokens 和像素是有益的。 加上随机替代策略比仅用 blockwise masking 结果要好，说明使用随机替换策略将鼓励模型学习像素和 tokens 的对齐。 表2. 在 ImageNet-1K 和 ADE20K 上进行的消融实验。“Rep.” 指是否使用随机替代策略3）在 tokenizer 训练过程中，给 patch 随机加噪声，观察对其他 patches tokens 生成的影响。在从表 3 可以得出： BEiT 和 BEiTv2 的 tokenizer 即使没有改变 patches 像素也会改变相应的 tokens（只有 1.41% 和3.97% 的 tokens 保持不变），这意味着它们不能保证图像 patches 和 tokens 之间的局部对应关系。 表3. 不同 tokenizers 的抗噪声能力比较。使用 tokens 不变 patches 的比率作为评价指标。4）从表 4 可以得出： 基于质心的 tokenizer 相比于 BEiT 等基于 dVAE 的 tokenizer 拥有更快的速度和推理时间； 基于质心的 tokenizer 在预测精度上优于 BEiT 和 BEiTv2 tokenizer。这一方面表明，基于质心的 tokenizer 能够通过上下文来预测 token，另一方面，BEiT 和 BEiTv2 没有学习到足够的 patch 间关系，无法通过 patch 间关联进行掩蔽推理。这反映了 BEiTv2 的改进本质上是由离散特征蒸馏所带来的。 表4. 不同 tokenizers 的性能","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]},{"title":"MIM | AutoMAE：将生成对抗网络用于优化掩蔽策略","slug":"Masked_Image_Model/MIM-AutoMAE将生成对抗网络用-Glenn","date":"un55fin55","updated":"un66fin66","comments":true,"path":"2023/05/12/Masked_Image_Model/MIM-AutoMAE将生成对抗网络用-Glenn/","link":"","permalink":"https://chenluda.github.io/2023/05/12/Masked_Image_Model/MIM-AutoMAE%E5%B0%86%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%94%A8-Glenn/","excerpt":"","text":"题目：Improving Masked Autoencoders by Learning Where to Mask作者单位：上海交通大学论文网址：https://arxiv.org/abs/2303.06583论文代码：未公开首次发布时间：2023 年 3 月 12 日 前言为了减少前景 patch 的掩蔽概率，PixMIM 使用 Simple Resized Crop 代替原始 MAE 使用的 Random Resized Crop。然而今天介绍的 AutoMAE 用实验证明，略微增加前景 patch 的掩蔽概率可以提高线性探测结果，但过度提高前景 patch 的掩蔽概率时反而会降低性能。 主要内容MIM 的输入 Patch 通常采用随机掩蔽的方式，这在很大程度上忽略了 Patch 之间的信息密度差异。那么是否存在比随机采样更好的掩蔽策略，我们如何学习它？ 对此，本篇论文提出了 AutoMAE 框架。如图1 所示，相比于独立训练指示器的 SemMAE（Semmae: Semantic-guided masking for learning masked autoencoders） ，AutoMAE 是在一个端到端完全可微的 MAE 框架下优化掩蔽策略的初步研究。 图1. 掩蔽策略的比较。(a) MAE：以均匀的概率随机掩码 75% 的图像补丁。(b) SemMAE 使用一个手动设计的 easy-to-hard 的 masking schedule，由一个独立训练的语义指示器引导。(c) AutoMAE 是一个完全可微的框架，它将一个反向训练的掩码生成器集成到掩码图像建模中。 分析什么应该是一个好的掩蔽策略？一般来说，像素重建的难度与相应的图像 patch 的信息密度有关。 如果我们掩盖了大量的“困难” Patch（如前景对象），模型可能无法感知高语义区域； 然而，如果我们掩盖了大量的“简单” Patch（如背景），自监督训练任务可能过于简单，模型无法学习有效的表示。 在初步的实验中，可以发现这两种情况都导致学习的视觉表征的退化。作者将这种经验现象称为 MAE 中的 patch selection dilemma（patch 选择困境）。 初步实验随机选择 ImageNet 数据集的一个子集（10%），并使用提供的前景边界框（什么方法？）来指示前景位置，如图2 右所示。 图2 左中 β 表示前景 patch 相对于背景 patch 提高的掩蔽概率（例如，总掩蔽率为 75%，当 β&#x3D;15 % 时，前景 patch 掩蔽率为 45%，背景 patch 掩蔽率为 30%）。手动提高边界框内 patch 的掩蔽率，并使用这些样本进行自监督预训练，图2 左显示了线性探测结果。从图中可以得到两个观察结果： 与随机掩蔽相比，略微增加前景 patch 的掩蔽概率，显著提高了线性探测结果。 过度提高前景 patch 的掩蔽概率，可能会影响 pretext task 的有效性，降低其性能。 可以看出，掩蔽策略是一个 trade-off 的问题，前景掩蔽率越高，通过掩蔽图像建模获得的信息越多，但同时解码器的重建难度也越高。一个自然的解决方案是将掩模生成和图像重建集成到一个完全可微的框架中。也就是说，我们希望在获得的具有较高信息密度的区域内找到重建难度较低的 patch 。 图2. 通过 β 提高掩蔽概率对前景边界框内的 patch 的影响。模型在 ImageNet 数据集的一个子集（10%）上进行训练。红色虚线表示使用原始随机掩蔽策略的结果。 方法通过上述实验，现在需要设计一个新的自监督框架，它可以学习选择信息更丰富的前景 patch 来掩蔽，且后续的重建难度较低，其中需要解决两个关键的问题： 如何在没有任何明确监督的情况下指导 ViT 模型挖掘具有较高信息密度的前景 patch？ 如何通过自适应学习的掩蔽策略来控制 pretext task 的难度？ 图3. AutoMAE 的端到端框架。 掩模生成器对于第一个问题，作者提出了一个可微的掩模生成器 G 来生成包含每个图像 patch 重要权重的掩模图像。掩模生成器包含一个预训练的 ViT 编码器（参数冻结）和两个可训练的卷积层。通过预先训练的 ViT 编码器从最后一个 Transformer 块中提取 multi-head self-attention maps，再通过卷积层进行进一步处理以生成掩模。此时获得的掩膜图像中包含了模糊的不同语义区域的指示，但仍需要突出该样本的前景。 因此，作者引入了一种以前景为中心的对抗性训练策略，来指导掩模生成器在可能存在前景的 patch 上产生更高的权重。具体来说，首先随机生成一张伪掩模图像（在全黑图像中随机涂白一个矩形，然后在整张图像上加随机噪声，其中白色矩形区域多加个 \\alpha&#x3D;0.5 ），作为“真实”样本，该样本可以模拟前景和背景补丁之间的差异，并使用对抗性训练策略来最小化生成的掩模和“真实”采样之间的分布偏移。 multi-head self-attention maps A_i 计算公式如下： $$ A_i&#x3D;softmax(q^c_iK_i^T&#x2F;\\sqrt {d’}) $$ 其中， q_i^c \\in \\mathbb{R}^{1×d’} 表示 [CLS] token 的 query embedding， K_i\\in \\mathbb {R}^{\\frac{hw}{p^2}×d’} 表示其他 patch token 的 key embedding。 两个卷积层再加 ReLU 的操作 f_\\theta(\\cdot) 如下： $$ F &#x3D; f_θ(A) $$ 最终掩模图像 M\\in\\mathbb{R}^{1×\\frac{h}{2}×\\frac{w}{2}} 计算公式如下： $$ f_i’&#x3D;log(\\frac{exp(f_i)}{\\sum^{hw&#x2F;p}_{k&#x3D;1}exp(f_k)})+z $$ $$ m_i&#x3D;\\frac{exp(f_i’)}{\\sum^{hw&#x2F;p}_{k&#x3D;1}exp(f_k’)} $$ 其中， z 表示从 Gumbel 分布中采样的随机噪声，这里的 log(softmax(\\cdot)) 用于稳定训练过程。 f_i 是 F 的展平， m_i 是掩膜图像 M 的展平。 在全黑图像 M^b\\in\\mathbb{R}^{1×\\frac{h}{2}×\\frac{w}{2}} 中随机涂白一个矩形 X&#x3D;(20%\\sim80%)M^b ，伪掩模图像 M^p 的计算公式如下： $$ m^p_i&#x3D; \\epsilon + \\begin{cases} \\alpha,i\\in X\\ 0,i\\in X \\end{cases} $$ 其中， \\epsilon\\sim U(0,1) 表示随机噪声。 α 表示矩形内的额外权重，在实验中设置为 0.5。 使用 LSGAN（Least squares generative adversarial networks） 中的损失函数进行对抗性训练： $$ \\mathcal{L}_{adv}&#x3D;-\\mathbb{E}_M[(D(M)-c)^2] $$ $$ \\mathcal{L}{adv}^D&#x3D;\\mathbb{E}{M^p}[(D(M^p)-b)^2]+\\mathbb{E}_M[(D(M)-a)^2] $$ 其中 D 表示鉴别器， a&#x3D;−1 ， b&#x3D;1 ， c&#x3D;0 。 梯度回传对于第二个问题，为了在保证掩模具有较高信息密度的同时，后续的重建难度较低，作者直接将梯度从 MAE 传播回掩模生成器，并同步训练这两个模块。MAE 通过重建损失来约束掩模生成器不生成过于困难的掩模图像，从而鼓励掩模生成器产生容易推断掩蔽信息的重要 Patch，而鉴别器则约束掩模生成器更加关注前景而非背景。 允许来自掩码自动编码器的梯度可以传播回掩码生成器，作者改变了输入 patch token Z&#x3D;\\left{z_i\\right}^n_{i&#x3D;1} 的公式，生成的新输入 patch token Z’&#x3D;\\left{z’_ i \\right}^n_{i&#x3D;1} 的计算公式如下： $$ z’_i &#x3D; z_i\\cdot m_i+z_i\\cdot sg(1 − m_i) $$ 其中， sg(\\cdot) 表示停止梯度操作。 结合对抗损失，掩码生成器的最终损失函数可以写为： $$ L_G &#x3D; L_{recon} + λL_{adv} $$ 其中，作者通过 grid search 将 λ 设置为 0.2。 （中间还有个生成 Final Mask 的操作没说，那个就是利用生成的掩膜经过几个 Top-K 操作，感兴趣的可以去原文 3.3 小节查看） 结果图4. 由掩模生成器产生的 ImageNet 上的高加权掩模图像。具体结果查看原文。 又是一篇观点矛盾的论文，PixMIM 认为较高的前景掩蔽率阻碍了模型有效捕获形状和语义先验的能力，从而限制了表征的质量；而 AutoMAE 认为略微增加前景 patch 的掩蔽概率可以提高模型性能。 说实话，感觉 AutoMAE 的模型有些复杂了。","categories":[{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"}],"tags":[{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"}]}],"categories":[{"name":"推荐","slug":"推荐","permalink":"https://chenluda.github.io/categories/%E6%8E%A8%E8%8D%90/"},{"name":"论文","slug":"论文","permalink":"https://chenluda.github.io/categories/%E8%AE%BA%E6%96%87/"},{"name":"书籍","slug":"书籍","permalink":"https://chenluda.github.io/categories/%E4%B9%A6%E7%B1%8D/"},{"name":"代码","slug":"代码","permalink":"https://chenluda.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"Research","slug":"Research","permalink":"https://chenluda.github.io/tags/Research/"},{"name":"Medical Image Segmentation","slug":"Medical-Image-Segmentation","permalink":"https://chenluda.github.io/tags/Medical-Image-Segmentation/"},{"name":"Masked Image Modeling","slug":"Masked-Image-Modeling","permalink":"https://chenluda.github.io/tags/Masked-Image-Modeling/"},{"name":"Steve Jobs","slug":"Steve-Jobs","permalink":"https://chenluda.github.io/tags/Steve-Jobs/"},{"name":"Competition","slug":"Competition","permalink":"https://chenluda.github.io/tags/Competition/"}]}